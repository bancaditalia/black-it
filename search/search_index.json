{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> <sup>*</sup> </p> Black-box ABM calibration kit <p>This package contains a black-box calibrator, which can be used to calibrate a specified model, using a loss function and a sequence of chosen search algorithms to estimate the wanted parameters. It comes with a set of ready-to-use example models, loss functions and search algorithms. Custom models and functions can be implemented to use with the calibrator.</p>"},{"location":"#why-use-it","title":"Why use it","text":"<p>While this tool can be used as a general optimizer for any built-in or custom model, it has been created with agent-based models in mind, or any situation in which a simple optimization is not enough but you have to combine techniques. That's why the package is easily customizable in terms of model, loss and algorithms, and comes with the ability to combine search algorithms sequentially.</p>"},{"location":"#installation","title":"Installation","text":"<p>This project requires Python v3.9 or later and Poetry.</p> <p>To install the package simply run</p> <pre><code>pip install black-it\n</code></pre>"},{"location":"#how-to-run","title":"How to run","text":"<p>Several calibration examples can be found in the <code>examples</code> folder of the GitHub repo. To run them, you first need to clone the repo</p> <pre><code>git clone https://github.com/bancaditalia/black-it.git\n</code></pre> <p>In the next section we will analyse in detail the <code>main.py</code> script, which you can run by</p> <pre><code>cd black-it/examples\npython main.py\n</code></pre>"},{"location":"#how-to-use","title":"How to use","text":"<p>To write a basic script, it is enough to instantiate a <code>Calibrator</code> object and use the <code>calibrate(n_batches)</code> method. The following example will refer to <code>examples/main.py</code>.</p> <p><pre><code># define a loss\nloss = MethodOfMomentsLoss()\n\n# define the calibration seed\ncalibration_seed = 1\n\n# initialize a Calibrator object\ncal = Calibrator(\n    samplers=[halton_sampler, random_forest_sampler, best_batch_sampler],\n    real_data=real_data,\n    model=model,\n    parameters_bounds=bounds,\n    parameters_precision=bounds_step,\n    ensemble_size=3,\n    loss_function=loss,\n    random_state=calibration_seed,\n)\n\n# calibrate the model\nparams, losses = cal.calibrate(n_batches=15)\n</code></pre> The calibrator constructor accepts as inputs:</p> <ol> <li>the real dataset (<code>real_data</code>),</li> <li>a stochastic model (<code>model</code>),</li> <li>a loss function (<code>loss_function</code>),</li> <li>the parameter space (<code>parameters_bounds</code>)</li> <li>a list of search algorithms (<code>samplers</code>)</li> </ol> <p>The method used to run the calibration (<code>calibrate</code>) accepts as input the number of batches to be executed (<code>n_batches</code>). For more information, check the <code>Code Reference</code> section of the documentation.</p>"},{"location":"#model","title":"Model","text":"<p>The model must be specified as a function and is used by the calibrator to produce simulated data (for more information about this, check how it works). In <code>examples/main.py</code>, the following is used: <pre><code># define a model to be calibrated\nmodel = md.MarkovC_KP\n</code></pre> A list of simple models can be found in the <code>examples/models</code> directory. A custom model may be specified by implementing a custom function.</p> <p>If an external simulator has to be used instead, check simulator_interface page.</p>"},{"location":"#loss-function","title":"Loss function","text":"<p>The loss function must be a concrete class inheriting from the abstract class <code>BaseLoss</code> and is used by the calibrator to evaluate the distance between the real dataset and the simulated one. In <code>examples/main.py</code>, the <code>MinkowskiLoss</code> is used.</p> <p>A list of functions can be found in the <code>loss_functions</code> module or, again, a the <code>BaseLoss</code> class can be extended to implement a specific loss function.</p>"},{"location":"#search-algorithms","title":"Search algorithms","text":"<p>The calibrator accepts a list of search algorithms which are used sequentially to estimate the wanted parameters. The parameter space to be searched is defined through its bounds by specifying <code>parameters_bounds</code>.</p> <p>Each search algorithm must be specified as an object and must be instantiated first. In this example, <pre><code>batch_size = 8\nhalton_sampler = HaltonSampler(batch_size=batch_size)\nrandom_forest_sampler = RandomForestSampler(batch_size=batch_size)\nbest_batch_sampler = BestBatchSampler(batch_size=batch_size)\n</code></pre> Each sampler has its own subclass derived from <code>BaseSampler</code> and a list of ready-to-use samplers is contained in <code>samplers</code>. To specify a custom algorithm, one must extend the <code>BaseSampler</code> superclass and implement its method <code>sample_batch</code> to specify how to sample a batch of parameters.</p> <p>Remark: when instantiated, the sampler accepts a <code>batch_size</code> parameter. While in this example every sampler runs on the same batch size, they can also run on different sizes, if required.</p>"},{"location":"#license","title":"License","text":"<p>Black-it is released under the GNU Affero General Public License v3 or later (AGPLv3+).</p> <p>Copyright 2021-2023 Banca d'Italia.</p>"},{"location":"#original-author","title":"Original Author","text":"<ul> <li>Gennaro Catapano &lt;gennaro.catapano@bancaditalia.it&gt;</li> </ul>"},{"location":"#co-authorsmaintainers","title":"Co-authors/Maintainers","text":"<ul> <li>Marco Benedetti &lt;marco.benedetti@bancaditalia.it&gt;</li> <li>Francesco De Sclavis &lt;francesco.desclavis@bancaditalia.it&gt;</li> <li>Marco Favorito &lt;marco.favorito@bancaditalia.it&gt;</li> <li>Aldo Glielmo &lt;aldo.glielmo@bancaditalia.it&gt;</li> <li>Davide Magnanimi &lt;davide.magnanimi@bancaditalia.it&gt;</li> <li>Antonio Muci &lt;antonio.muci@bancaditalia.it&gt;</li> </ul> <p> * Credits to Sara Corbo for the logo. </p>"},{"location":"benchmarking_samplers/","title":"Benchmarking samplers","text":"<p>In this tutorial we compare the performance of different sampling methods for the calibration of the paradigmatic model for asset pricing by Brock and Hommes (Journal of Economic Dynamics and Control, 1998).</p> <pre><code>import numpy as np\nfrom models.economics.brock_hommes import BH4\nimport matplotlib.pyplot as plt\nfrom black_it.calibrator import Calibrator\nimport pandas as pd\n</code></pre> <pre><code># generate a single realisation of the BH4 using parameters\n# from the literature (see Platt (2020))\n\ntrue_params = [\n    0.0,  # g1\n    0.0,  # b1\n    0.9,  # g2\n    0.2,  # b2\n    0.9,  # g3\n    -0.2,  # b3\n    1.01,  # g4\n    0.01,\n]  # b4\n\nparameter_bounds = [\n    [0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0, 0.0],  # lower bounds\n    [0.1, 0.1, 1.0, 1.0, 1.0, 0.0, 1.1, 1.0],\n]  # upper bounds\n\nprecisions = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n\ntarget_series = BH4(true_params, N=1000, seed=0)\n</code></pre> <pre><code># plot the target time series\n\nplt.figure(figsize=(7, 5))\nplt.plot(target_series)\n\nplt.xlabel(\"t\", fontsize=14)\nplt.ylabel(\"x\", fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12);\n</code></pre> <pre><code># import a series of samplers to benchmark\n\nfrom black_it.samplers.halton import HaltonSampler\nfrom black_it.samplers.random_forest import RandomForestSampler\nfrom black_it.samplers.best_batch import BestBatchSampler\n\nall_samplers = [HaltonSampler, RandomForestSampler, BestBatchSampler]\n</code></pre> <pre><code># initialize a set of sampling methods to test,\n# note that each method uses the same effective batch size\n\n# 3 single samplers, batch size = 6\nbatch_size = 6\nsingle_samplers = [[s(batch_size=batch_size)] for s in all_samplers]\n\n# the 3 combinations of 2 different samplers, batch sizes = 3\ncouple_indices = [[0, 1], [0, 2], [1, 2]]\ncouple_samplers = []\nfor ci in couple_indices:\n    couple = []\n    for i in ci:\n        couple.append(all_samplers[i](batch_size=int(batch_size / 2)))\n    couple_samplers.append(couple)\n\n# a combination of all 3 samplers, batch sizes = 2\ntriplet_indices = [0, 1, 2]\ntriplet_samplers = [[all_samplers[i](batch_size=int(batch_size / 3)) for i in triplet_indices]]\n</code></pre> <pre><code># define a list with all calibration strategies\n\nall_calibration_strategies = single_samplers + couple_samplers + triplet_samplers\n</code></pre> <pre><code># define a method of moments loss\n\nfrom black_it.loss_functions.msm import MethodOfMomentsLoss\n\nloss = MethodOfMomentsLoss()\n</code></pre> <pre><code># sample a few random parameters to provide a starting point to the adaptive samplers\n\nfrom black_it.samplers.random_uniform import RandomUniformSampler\n\nrandom_sampler = RandomUniformSampler(batch_size)\n\ncal = Calibrator(\n    real_data=target_series,\n    samplers=[random_sampler],\n    loss_function=loss,\n    model=BH4,\n    parameters_bounds=parameter_bounds,\n    parameters_precision=precisions,\n    ensemble_size=3,\n    saving_folder=\"initial_state\",\n)\n\n_, _ = cal.calibrate(1)\n</code></pre> <pre>\n<code>\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   1\nPARAMS SAMPLED: 0\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 2.8s\n----&gt;   min loss new params: 4.09\n----&gt;   avg loss new params: 4.419277934778664e+71\n----&gt; avg loss exist params: 4.419277934778664e+71\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 2.8s\nCheckpoint saved in 0.0s\n</code>\n</pre> <pre><code># run a series of experiments of 10 epochs each, and save the results\n\nfor i, samplers in enumerate(all_calibration_strategies):\n    print(\"Sampling strategy \", i, \"of \", len(all_calibration_strategies))\n    sampler_name = \"\"\n    for s in samplers:\n        sampler_name += type(s).__name__\n\n    cal = Calibrator.restore_from_checkpoint(\"initial_state\", model=BH4)\n\n    cal.set_samplers(samplers)\n\n    cal.saving_folder = sampler_name\n\n    cal.calibrate(10)\n</code></pre> <pre>\n<code>Sampling strategy  0 of  7\n\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 6\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 3.66\n----&gt;   avg loss new params: 8.300758412517313e+24\n----&gt; avg loss exist params: 2.2096389673893325e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 12\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 1.8143899404071114e+25\n----&gt;   avg loss new params: 1.9531662506640626e+49\n----&gt; avg loss exist params: 1.4730926449262217e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 18\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 635.52\n----&gt;   avg loss new params: 1.4682776406569115e+65\n----&gt; avg loss exist params: 1.1048198507640762e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 24\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 1.08\n----&gt;   avg loss new params: 1.0259600018245788e+25\n----&gt; avg loss exist params: 8.838558806112611e+70\n----&gt;         curr min loss: 1.0764783501698882\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 30\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 1.1663765177260362e+33\n----&gt;   avg loss new params: 1.2315345916152899e+57\n----&gt; avg loss exist params: 7.365465671760529e+70\n----&gt;         curr min loss: 1.0764783501698882\n====&gt;    total elapsed time: 0.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 36\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 522.0\n----&gt;   avg loss new params: 2.650221569942234e+65\n----&gt; avg loss exist params: 6.313260076111268e+70\n----&gt;         curr min loss: 1.0764783501698882\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 42\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 3.13\n----&gt;   avg loss new params: 6.323596971065026e+31\n----&gt; avg loss exist params: 5.524102566597359e+70\n----&gt;         curr min loss: 1.0764783501698882\n====&gt;    total elapsed time: 0.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 48\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 9.871204642368983e+32\n----&gt;   avg loss new params: 2.0354753999901843e+57\n----&gt; avg loss exist params: 4.910313392531009e+70\n----&gt;         curr min loss: 1.0764783501698882\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 54\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 389.87\n----&gt;   avg loss new params: 1.9594869275792813e+65\n----&gt; avg loss exist params: 4.419284012764835e+70\n----&gt;         curr min loss: 1.0764783501698882\n====&gt;    total elapsed time: 0.9s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   11\nPARAMS SAMPLED: 60\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 2.95\n----&gt;   avg loss new params: 1.9693240640656187e+32\n----&gt; avg loss exist params: 4.017530920695304e+70\n----&gt;         curr min loss: 1.0764783501698882\n====&gt;    total elapsed time: 0.7s\nCheckpoint saved in 0.1s\nSampling strategy  1 of  7\n\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 6\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 1.44\n----&gt;   avg loss new params: 2.9772293266637943e+72\n----&gt; avg loss exist params: 1.7095785600708306e+72\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 2.0s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 12\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 3.09\n----&gt;   avg loss new params: 8.046150675194563e+17\n----&gt; avg loss exist params: 1.1397190400472204e+72\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 2.0s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 18\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.0s\n----&gt;   min loss new params: 158.86\n----&gt;   avg loss new params: 1.1850121207855025e+18\n----&gt; avg loss exist params: 8.547892800354153e+71\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 1.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 24\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 230659.17\n----&gt;   avg loss new params: 4.2845603081718367e+55\n----&gt; avg loss exist params: 6.838314240283323e+71\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 1.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 30\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 7.16\n----&gt;   avg loss new params: 4.952491859756142e+17\n----&gt; avg loss exist params: 5.698595200236102e+71\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 2.8s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 36\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 4.51\n----&gt;   avg loss new params: 28.13\n----&gt; avg loss exist params: 4.884510171630944e+71\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 2.7s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 42\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 4.59\n----&gt;   avg loss new params: 1.4545076704483378e+30\n----&gt; avg loss exist params: 4.2739464001770764e+71\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 2.5s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 48\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.4s\n----&gt;   min loss new params: 2.49\n----&gt;   avg loss new params: 1.4027803946629642e+22\n----&gt; avg loss exist params: 3.799063466824068e+71\n----&gt;         curr min loss: 1.442009376321102\n====&gt;    total elapsed time: 2.4s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 54\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 1.28\n----&gt;   avg loss new params: 4.658999191050945e+16\n----&gt; avg loss exist params: 3.4191571201416613e+71\n----&gt;         curr min loss: 1.2772975970984322\n====&gt;    total elapsed time: 2.4s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   11\nPARAMS SAMPLED: 60\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 1.01\n----&gt;   avg loss new params: 1.327724523081595e+40\n----&gt; avg loss exist params: 3.1083246546742375e+71\n----&gt;         curr min loss: 1.005559758393759\n====&gt;    total elapsed time: 1.8s\nCheckpoint saved in 0.1s\nSampling strategy  2 of  7\n\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 6\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.0s\n----&gt;   min loss new params: 1.2743993839456501e+33\n----&gt;   avg loss new params: 3.134246498833947e+73\n----&gt; avg loss exist params: 1.589219639090867e+73\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 1.0s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 12\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.0s\n----&gt;   min loss new params: 4.69\n----&gt;   avg loss new params: 2.3299915681425326e+48\n----&gt; avg loss exist params: 1.0594797593939112e+73\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 1.0s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 18\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 3.54\n----&gt;   avg loss new params: 5.4794875763647744e+72\n----&gt; avg loss exist params: 9.315970089545528e+72\n----&gt;         curr min loss: 3.537864613089154\n====&gt;    total elapsed time: 1.1s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 24\n\nMETHOD: BestBatchSampler\n</code>\n</pre> <pre>\n<code>----&gt; sim exec elapsed time: 2.0s\n----&gt;   min loss new params: 3.65\n----&gt;   avg loss new params: 5.477355418974891e+54\n----&gt; avg loss exist params: 7.452776071636422e+72\n----&gt;         curr min loss: 3.537864613089154\n====&gt;    total elapsed time: 2.0s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 30\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.7s\n----&gt;   min loss new params: 3.06\n----&gt;   avg loss new params: 3.648147170157569e+32\n----&gt; avg loss exist params: 6.210646726363686e+72\n----&gt;         curr min loss: 3.061494620652621\n====&gt;    total elapsed time: 1.7s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 36\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.0s\n----&gt;   min loss new params: 2.75\n----&gt;   avg loss new params: 3.19\n----&gt; avg loss exist params: 5.323411479740301e+72\n----&gt;         curr min loss: 2.7469033833359866\n====&gt;    total elapsed time: 1.0s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 42\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.5s\n----&gt;   min loss new params: 2.51\n----&gt;   avg loss new params: 3.32\n----&gt; avg loss exist params: 4.657985044772764e+72\n----&gt;         curr min loss: 2.5106289507714665\n====&gt;    total elapsed time: 1.5s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 48\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.4s\n----&gt;   min loss new params: 1.69\n----&gt;   avg loss new params: 2.8230131715153864e+46\n----&gt; avg loss exist params: 4.140431150909123e+72\n----&gt;         curr min loss: 1.6890182521981136\n====&gt;    total elapsed time: 1.4s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 54\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 1.74\n----&gt;   avg loss new params: 2.4\n----&gt; avg loss exist params: 3.726388035818211e+72\n----&gt;         curr min loss: 1.6890182521981136\n====&gt;    total elapsed time: 0.9s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   11\nPARAMS SAMPLED: 60\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 1.71\n----&gt;   avg loss new params: 1.371171529418354e+16\n----&gt; avg loss exist params: 3.3876254871074644e+72\n----&gt;         curr min loss: 1.6890182521981136\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.1s\nSampling strategy  3 of  7\n\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 6\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 3.66\n----&gt;   avg loss new params: 23759.98\n----&gt; avg loss exist params: 2.9461852898524434e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.4s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 266.38\n----&gt;   avg loss new params: 4.929504599938332e+24\n----&gt; avg loss exist params: 2.2096389673893325e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 1.4s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 12\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 259408.3\n----&gt;   avg loss new params: 1.3418466928955868e+25\n----&gt; avg loss exist params: 1.7677111739114658e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.4s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 5.59\n----&gt;   avg loss new params: 9.91\n----&gt; avg loss exist params: 1.4730926449262217e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 1.3s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 18\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 3.218208797625005e+25\n----&gt;   avg loss new params: 4.330494474331893e+33\n----&gt; avg loss exist params: 1.2626508385081898e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.4s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 4.79\n----&gt;   avg loss new params: 4.1039175555312315e+40\n----&gt; avg loss exist params: 1.1048194836946663e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 1.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 24\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 1.0817917199830301e+42\n----&gt;   avg loss new params: 3.9053598974827594e+49\n----&gt; avg loss exist params: 9.820617632841477e+70\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.4s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 3.19\n----&gt;   avg loss new params: 1.9816531622380618e+31\n----&gt; avg loss exist params: 8.838555869557329e+70\n----&gt;         curr min loss: 3.190897002897203\n====&gt;    total elapsed time: 1.7s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 30\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 7.022331859686527e+57\n----&gt;   avg loss new params: 3.5277422231985074e+64\n----&gt; avg loss exist params: 8.035051111210502e+70\n----&gt;         curr min loss: 3.190897002897203\n====&gt;    total elapsed time: 0.7s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 2.38\n----&gt;   avg loss new params: 5475.56\n----&gt; avg loss exist params: 7.365463518609625e+70\n----&gt;         curr min loss: 2.376892792787833\n====&gt;    total elapsed time: 3.5s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 36\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 606.59\n----&gt;   avg loss new params: 3.6625250278652666e+65\n----&gt; avg loss exist params: 6.798892219120445e+70\n----&gt;         curr min loss: 2.376892792787833\n====&gt;    total elapsed time: 0.7s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 4.47\n----&gt;   avg loss new params: 19.76\n----&gt; avg loss exist params: 6.313257060611841e+70\n----&gt;         curr min loss: 2.376892792787833\n====&gt;    total elapsed time: 2.4s\nCheckpoint saved in 0.2s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 42\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 1.11\n----&gt;   avg loss new params: 7.18\n----&gt; avg loss exist params: 5.892373256571052e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 1.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 6.03\n----&gt;   avg loss new params: 27.11\n----&gt; avg loss exist params: 5.524099928035361e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 2.2s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 48\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 1.14089698130206e+17\n----&gt;   avg loss new params: 2.1170674425931233e+25\n----&gt; avg loss exist params: 5.199152873445046e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 0.6s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 1.22\n----&gt;   avg loss new params: 12.59\n----&gt; avg loss exist params: 4.9103110471425434e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 1.7s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 54\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 2.785000441888143e+33\n----&gt;   avg loss new params: 2.1868878143494303e+41\n----&gt; avg loss exist params: 4.651873623608726e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 0.6s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 13.6\n----&gt;   avg loss new params: 1.1235228254061286e+32\n----&gt; avg loss exist params: 4.419279942428289e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 1.3s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   11\nPARAMS SAMPLED: 60\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 1.5s\n----&gt;   min loss new params: 7.165734019763288e+48\n----&gt;   avg loss new params: 2.613140765222106e+57\n----&gt; avg loss exist params: 4.2088380404079067e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 1.5s\n\nMETHOD: RandomForestSampler\n</code>\n</pre> <pre>\n<code>----&gt; sim exec elapsed time: 1.9s\n----&gt;   min loss new params: 6.33\n----&gt;   avg loss new params: 9.43\n----&gt; avg loss exist params: 4.017527220389366e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 4.1s\nCheckpoint saved in 0.2s\nSampling strategy  4 of  7\n\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 6\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 3.66\n----&gt;   avg loss new params: 23759.98\n----&gt; avg loss exist params: 2.9461852898524434e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 3.91\n----&gt;   avg loss new params: 9.57\n----&gt; avg loss exist params: 2.2096389673893325e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.4s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 12\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 259408.3\n----&gt;   avg loss new params: 1.3418466928955868e+25\n----&gt; avg loss exist params: 1.7677111739114658e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 3.38\n----&gt;   avg loss new params: 3.63\n----&gt; avg loss exist params: 1.4730926449262217e+71\n----&gt;         curr min loss: 3.3807587026196213\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 18\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 3.218208797625005e+25\n----&gt;   avg loss new params: 4.330494474331893e+33\n----&gt; avg loss exist params: 1.2626508385081898e+71\n----&gt;         curr min loss: 3.3807587026196213\n====&gt;    total elapsed time: 0.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 2.99\n----&gt;   avg loss new params: 4.57\n----&gt; avg loss exist params: 1.1048194836946663e+71\n----&gt;         curr min loss: 2.986363446469386\n====&gt;    total elapsed time: 0.5s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 24\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 1.0817917199830301e+42\n----&gt;   avg loss new params: 3.9053598974827594e+49\n----&gt; avg loss exist params: 9.820617632841477e+70\n----&gt;         curr min loss: 2.986363446469386\n====&gt;    total elapsed time: 0.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 2.91\n----&gt;   avg loss new params: 4.303297507113467e+39\n----&gt; avg loss exist params: 8.838555869557329e+70\n----&gt;         curr min loss: 2.9081793471084874\n====&gt;    total elapsed time: 0.5s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 30\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 7.022331859686527e+57\n----&gt;   avg loss new params: 3.5277422231985074e+64\n----&gt; avg loss exist params: 8.035051111210502e+70\n----&gt;         curr min loss: 2.9081793471084874\n====&gt;    total elapsed time: 0.4s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 1.61\n----&gt;   avg loss new params: 4.36\n----&gt; avg loss exist params: 7.365463518609625e+70\n----&gt;         curr min loss: 1.6090030240712685\n====&gt;    total elapsed time: 0.4s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 36\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 606.59\n----&gt;   avg loss new params: 3.6625250278652666e+65\n----&gt; avg loss exist params: 6.798892219120445e+70\n----&gt;         curr min loss: 1.6090030240712685\n====&gt;    total elapsed time: 0.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 3.62\n----&gt;   avg loss new params: 123998723929308.95\n----&gt; avg loss exist params: 6.313257060611841e+70\n----&gt;         curr min loss: 1.6090030240712685\n====&gt;    total elapsed time: 0.4s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 42\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 1.11\n----&gt;   avg loss new params: 7.18\n----&gt; avg loss exist params: 5.892373256571052e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 0.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.9s\n----&gt;   min loss new params: 1.9\n----&gt;   avg loss new params: 3.978581939282771e+31\n----&gt; avg loss exist params: 5.524099928035361e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 1.9s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 48\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 1.14089698130206e+17\n----&gt;   avg loss new params: 2.1170674425931233e+25\n----&gt; avg loss exist params: 5.199152873445046e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 0.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 1.13\n----&gt;   avg loss new params: 1.48\n----&gt; avg loss exist params: 4.9103110471425434e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 54\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 2.785000441888143e+33\n----&gt;   avg loss new params: 2.1868878143494303e+41\n----&gt; avg loss exist params: 4.651873623608726e+70\n----&gt;         curr min loss: 1.1066869096045393\n====&gt;    total elapsed time: 0.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 0.94\n----&gt;   avg loss new params: 1.14\n----&gt; avg loss exist params: 4.419279942428289e+70\n----&gt;         curr min loss: 0.942413698087732\n====&gt;    total elapsed time: 1.1s\nCheckpoint saved in 0.2s\n\nBATCH NUMBER:   11\nPARAMS SAMPLED: 60\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 7.165734019763288e+48\n----&gt;   avg loss new params: 2.613140765222106e+57\n----&gt; avg loss exist params: 4.2088380404079067e+70\n----&gt;         curr min loss: 0.942413698087732\n====&gt;    total elapsed time: 0.9s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.0s\n----&gt;   min loss new params: 1.18\n----&gt;   avg loss new params: 1.470380789543466e+48\n----&gt; avg loss exist params: 4.017527220389366e+70\n----&gt;         curr min loss: 0.942413698087732\n====&gt;    total elapsed time: 1.0s\nCheckpoint saved in 0.1s\nSampling strategy  5 of  7\n\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 6\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 12.66\n----&gt;   avg loss new params: 30.73\n----&gt; avg loss exist params: 2.9461852898524434e+71\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 2.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.4s\n----&gt;   min loss new params: 12.8\n----&gt;   avg loss new params: 10431.01\n----&gt; avg loss exist params: 2.2096389673893325e+71\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 1.4s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 12\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 6.81\n----&gt;   avg loss new params: 4.3550320248811904e+17\n----&gt; avg loss exist params: 1.7677111739114658e+71\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 4.8s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 8.52\n----&gt;   avg loss new params: 9.864850943917757e+62\n----&gt; avg loss exist params: 1.4730926465703633e+71\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 0.6s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 18\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 2.2s\n----&gt;   min loss new params: 9.55\n----&gt;   avg loss new params: 3.887305425463723e+32\n----&gt; avg loss exist params: 1.2626508399174543e+71\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 3.9s\n\nMETHOD: BestBatchSampler\n</code>\n</pre> <pre>\n<code>----&gt; sim exec elapsed time: 2.5s\n----&gt;   min loss new params: 6.55\n----&gt;   avg loss new params: 10.8\n----&gt; avg loss exist params: 1.1048194849277725e+71\n----&gt;         curr min loss: 4.093299221041861\n====&gt;    total elapsed time: 2.6s\nCheckpoint saved in 0.2s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 24\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 2.4s\n----&gt;   min loss new params: 3.03\n----&gt;   avg loss new params: 3.9\n----&gt; avg loss exist params: 9.820617643802423e+70\n----&gt;         curr min loss: 3.0284708010510477\n====&gt;    total elapsed time: 4.3s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.9s\n----&gt;   min loss new params: 4.5\n----&gt;   avg loss new params: 1.120218120072616e+40\n----&gt; avg loss exist params: 8.838555879422181e+70\n----&gt;         curr min loss: 3.0284708010510477\n====&gt;    total elapsed time: 1.9s\nCheckpoint saved in 0.2s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 30\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.3s\n----&gt;   min loss new params: 3.96\n----&gt;   avg loss new params: 7.5\n----&gt; avg loss exist params: 8.035050799474711e+70\n----&gt;         curr min loss: 3.0284708010510477\n====&gt;    total elapsed time: 3.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 4.88\n----&gt;   avg loss new params: 7.83\n----&gt; avg loss exist params: 7.365463232851817e+70\n----&gt;         curr min loss: 3.0284708010510477\n====&gt;    total elapsed time: 1.1s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 36\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.3s\n----&gt;   min loss new params: 3.01\n----&gt;   avg loss new params: 6.78\n----&gt; avg loss exist params: 6.798889138017061e+70\n----&gt;         curr min loss: 3.0120008197397503\n====&gt;    total elapsed time: 3.3s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 3.48\n----&gt;   avg loss new params: 4.2366396762870067e+18\n----&gt; avg loss exist params: 6.313254199587272e+70\n----&gt;         curr min loss: 3.0120008197397503\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 42\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.5s\n----&gt;   min loss new params: 2.35\n----&gt;   avg loss new params: 5.86\n----&gt; avg loss exist params: 5.892370586281454e+70\n----&gt;         curr min loss: 2.3490688746866115\n====&gt;    total elapsed time: 3.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 2.1\n----&gt;   avg loss new params: 4.5170950241116807e+24\n----&gt; avg loss exist params: 5.524097424638862e+70\n----&gt;         curr min loss: 2.095820051969746\n====&gt;    total elapsed time: 1.2s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 48\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 143.9\n----&gt;   avg loss new params: 6.3136134841751816e+16\n----&gt; avg loss exist params: 5.1991505173071656e+70\n----&gt;         curr min loss: 2.095820051969746\n====&gt;    total elapsed time: 1.9s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 2.21\n----&gt;   avg loss new params: 2.87\n----&gt; avg loss exist params: 4.910308821901212e+70\n----&gt;         curr min loss: 2.095820051969746\n====&gt;    total elapsed time: 0.8s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 54\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 1.1s\n----&gt;   min loss new params: 3.78\n----&gt;   avg loss new params: 7.144988739148858e+18\n----&gt; avg loss exist params: 4.651871515485358e+70\n----&gt;         curr min loss: 2.095820051969746\n====&gt;    total elapsed time: 2.6s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 2.16\n----&gt;   avg loss new params: 2.52\n----&gt; avg loss exist params: 4.4192779397110904e+70\n----&gt;         curr min loss: 2.095820051969746\n====&gt;    total elapsed time: 0.7s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   11\nPARAMS SAMPLED: 60\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 2.133175205090705e+26\n----&gt;   avg loss new params: 9.445350178094928e+55\n----&gt; avg loss exist params: 4.2088361330581827e+70\n----&gt;         curr min loss: 2.095820051969746\n====&gt;    total elapsed time: 1.6s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 2.81\n----&gt;   avg loss new params: 1.8945860536525166e+39\n----&gt; avg loss exist params: 4.0175253997373555e+70\n----&gt;         curr min loss: 2.095820051969746\n====&gt;    total elapsed time: 0.6s\nCheckpoint saved in 0.1s\nSampling strategy  6 of  7\n\n***\nNumber of free params:       8.\nExplorable param space size: 13988943766831.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 6\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 3.66\n----&gt;   avg loss new params: 7.73\n----&gt; avg loss exist params: 3.3144584510839986e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.4s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 156.11\n----&gt;   avg loss new params: 1.421244832993949e+26\n----&gt; avg loss exist params: 2.6515667608671987e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 1.9s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 4.64\n----&gt;   avg loss new params: 9.528436025701058e+22\n----&gt; avg loss exist params: 2.2096389673893325e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.5s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 12\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 94300.57\n----&gt;   avg loss new params: 149916.69\n----&gt; avg loss exist params: 1.893976257762285e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 0.5s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 59.02\n----&gt;   avg loss new params: 589.01\n----&gt; avg loss exist params: 1.6572292255419993e+71\n----&gt;         curr min loss: 3.6618024049543774\n====&gt;    total elapsed time: 2.2s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 3.49\n----&gt;   avg loss new params: 3.74\n----&gt; avg loss exist params: 1.4730926449262217e+71\n----&gt;         curr min loss: 3.490052814946999\n====&gt;    total elapsed time: 0.6s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 18\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.7s\n----&gt;   min loss new params: 2.2252400654072278e+17\n----&gt;   avg loss new params: 2.0762227039805127e+25\n----&gt; avg loss exist params: 1.3257833804335994e+71\n----&gt;         curr min loss: 3.490052814946999\n====&gt;    total elapsed time: 0.7s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 2.25\n----&gt;   avg loss new params: 387216.91\n----&gt; avg loss exist params: 1.2052576185759994e+71\n----&gt;         curr min loss: 2.2472793592156948\n====&gt;    total elapsed time: 1.9s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 2.85\n----&gt;   avg loss new params: 5.110258403121564e+31\n----&gt; avg loss exist params: 1.1048194836946663e+71\n----&gt;         curr min loss: 2.2472793592156948\n====&gt;    total elapsed time: 0.4s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 24\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 2.0543465401664097e+25\n----&gt;   avg loss new params: 3.6848605655597034e+33\n----&gt; avg loss exist params: 1.0198333695643073e+71\n----&gt;         curr min loss: 2.2472793592156948\n====&gt;    total elapsed time: 0.8s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 3.71\n----&gt;   avg loss new params: 4.03\n----&gt; avg loss exist params: 9.469881288811425e+70\n----&gt;         curr min loss: 2.2472793592156948\n====&gt;    total elapsed time: 1.9s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 2.4\n----&gt;   avg loss new params: 2.58\n----&gt; avg loss exist params: 8.838555869557329e+70\n----&gt;         curr min loss: 2.2472793592156948\n====&gt;    total elapsed time: 0.5s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 30\n\nMETHOD: HaltonSampler\n</code>\n</pre> <pre>\n<code>----&gt; sim exec elapsed time: 0.8s\n----&gt;   min loss new params: 9.534641615636949e+33\n----&gt;   avg loss new params: 5.457569779907695e+41\n----&gt; avg loss exist params: 8.286146127709996e+70\n----&gt;         curr min loss: 2.2472793592156948\n====&gt;    total elapsed time: 0.8s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 3.61\n----&gt;   avg loss new params: 8.29\n----&gt; avg loss exist params: 7.798725767256467e+70\n----&gt;         curr min loss: 2.2472793592156948\n====&gt;    total elapsed time: 1.8s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 2.21\n----&gt;   avg loss new params: 2.49\n----&gt; avg loss exist params: 7.365463224631108e+70\n----&gt;         curr min loss: 2.2063781203583126\n====&gt;    total elapsed time: 0.4s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 36\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.9s\n----&gt;   min loss new params: 1.2930856737877442e+42\n----&gt;   avg loss new params: 5.852402936836168e+49\n----&gt; avg loss exist params: 6.977807265439997e+70\n----&gt;         curr min loss: 2.2063781203583126\n====&gt;    total elapsed time: 0.9s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 6.67\n----&gt;   avg loss new params: 137.43\n----&gt; avg loss exist params: 6.628916902167997e+70\n----&gt;         curr min loss: 2.2063781203583126\n====&gt;    total elapsed time: 2.3s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 1.45\n----&gt;   avg loss new params: 1.0122406761007165e+48\n----&gt; avg loss exist params: 6.313254192540949e+70\n----&gt;         curr min loss: 1.4517270271107627\n====&gt;    total elapsed time: 0.4s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 42\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 4.693912088645729e+57\n----&gt;   avg loss new params: 7.321340783291673e+57\n----&gt; avg loss exist params: 6.02628809288003e+70\n----&gt;         curr min loss: 1.4517270271107627\n====&gt;    total elapsed time: 0.3s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 8.39\n----&gt;   avg loss new params: 34.61\n----&gt; avg loss exist params: 5.764275567102637e+70\n----&gt;         curr min loss: 1.4517270271107627\n====&gt;    total elapsed time: 1.4s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 3.19\n----&gt;   avg loss new params: 1.698842407427921e+47\n----&gt; avg loss exist params: 5.524097418473361e+70\n----&gt;         curr min loss: 1.4517270271107627\n====&gt;    total elapsed time: 0.3s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 48\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 3.288144112758467e+65\n----&gt;   avg loss new params: 7.102222980735781e+65\n----&gt; avg loss exist params: 5.303136362623619e+70\n----&gt;         curr min loss: 1.4517270271107627\n====&gt;    total elapsed time: 0.3s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 2.82\n----&gt;   avg loss new params: 4.49\n----&gt; avg loss exist params: 5.099169579445788e+70\n----&gt;         curr min loss: 1.4517270271107627\n====&gt;    total elapsed time: 1.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 1.33\n----&gt;   avg loss new params: 1.4\n----&gt; avg loss exist params: 4.910311446873721e+70\n----&gt;         curr min loss: 1.3320791175674513\n====&gt;    total elapsed time: 0.3s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 54\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 601.45\n----&gt;   avg loss new params: 1593.66\n----&gt; avg loss exist params: 4.734943180913945e+70\n----&gt;         curr min loss: 1.3320791175674513\n====&gt;    total elapsed time: 0.4s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 6.76\n----&gt;   avg loss new params: 3.645056588348578e+24\n----&gt; avg loss exist params: 4.5716692781238096e+70\n----&gt;         curr min loss: 1.3320791175674513\n====&gt;    total elapsed time: 1.2s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 9.80672370968492e+16\n----&gt;   avg loss new params: 1.0486335887746384e+32\n----&gt; avg loss exist params: 4.419280302186348e+70\n----&gt;         curr min loss: 1.3320791175674513\n====&gt;    total elapsed time: 0.3s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   11\nPARAMS SAMPLED: 60\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.6s\n----&gt;   min loss new params: 0.9\n----&gt;   avg loss new params: 7.53\n----&gt; avg loss exist params: 4.2767228730835636e+70\n----&gt;         curr min loss: 0.9031595091997369\n====&gt;    total elapsed time: 0.6s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.5s\n----&gt;   min loss new params: 10611.75\n----&gt;   avg loss new params: 2.84600288413688e+31\n----&gt; avg loss exist params: 4.143075283299702e+70\n----&gt;         curr min loss: 0.9031595091997369\n====&gt;    total elapsed time: 1.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.4s\n----&gt;   min loss new params: 4.82\n----&gt;   avg loss new params: 8.096356842026602e+24\n----&gt; avg loss exist params: 4.0175275474421353e+70\n----&gt;         curr min loss: 0.9031595091997369\n====&gt;    total elapsed time: 0.4s\nCheckpoint saved in 0.1s\n</code>\n</pre> <pre><code># load the results from the corresponding folders, and plot them!\n\nplt.figure(figsize=(7, 5))\nlss = [\":\"] * 3 + [\"--\"] * 3 + [\"-\"]\n\nfor i, sampler in enumerate(all_calibration_strategies):\n    # name of the corresponding folder\n    sampler_name = \"\"\n    for s in sampler:\n        sampler_name += type(s).__name__\n\n    # get array of minimum losses achieved\n    losses = pd.read_csv(sampler_name + \"/calibration_results.csv\")[\"losses_samp\"].cummin()\n\n    sampler_name = sampler_name.replace(\"Sampler\", \"  \")\n\n    # plot the loss curve\n    plt.plot(losses, label=sampler_name, ls=lss[i], lw=2.5)\n\n\nplt.legend(loc=\"upper center\", bbox_to_anchor=(0.4, -0.14), prop={\"size\": 14})\nplt.xlabel(\"model calls\", fontsize=14)\nplt.ylabel(\"loss\", fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12);\n</code></pre> <pre><code>\n</code></pre>"},{"location":"benchmarking_samplers/#benchmarking-different-samplers-against-a-standard-abm","title":"Benchmarking different samplers against a standard ABM","text":""},{"location":"boltzmann_wealth_model/","title":"Boltzmann wealth model","text":"<p>Here we will explore and calibrate a simple model of agents exchanging wealth. The agents start with a Bernoulli distributed wealth. Then each agent with one unit of money or more gives one unit of wealth to another random agents encountered, unless the ratio of the agents wealths is above a certain threshold. This model is adapted from this source.</p> <p>Note that this notebook requires the installation of the mesa library: you can install it through  <code>pip install mesa</code>.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom models.economics.boltzmann_wealth import BoltzmannWealthModel, compute_gini\n\n\nmodel = BoltzmannWealthModel(num_agents=50, width=5, height=5, generosity_ratio=2.0, mean_init_wealth=10)\nmodel.run_model(1000)\n</code></pre> <pre><code>agent_wealths = [agent.wealth for agent in model.schedule.agents]\n\nplt.hist(agent_wealths)\n\nplt.xlabel(\"wealth\")\nplt.ylabel(\"number of agents\")\n</code></pre> <pre>\n<code>Text(0, 0.5, 'number of agents')</code>\n</pre> <pre><code>compute_gini(model)\n</code></pre> <pre>\n<code>0.18516634050880632</code>\n</pre> <pre><code>plt.figure(figsize=(5, 5))\n\ns_w = sorted(agent_wealths)\n\nX_lorenz = np.cumsum(s_w) / np.sum(s_w)\nX_lorenz = np.insert(X_lorenz, 0, 0)\nX_lorenz[0], X_lorenz[-1]\n\nplt.plot(\n    np.arange(X_lorenz.size) / (X_lorenz.size - 1),\n    X_lorenz,\n)\nplt.plot(\n    np.arange(X_lorenz.size) / (X_lorenz.size - 1),\n    np.linspace(0, 1, len(X_lorenz)),\n    \"k--\",\n)\n\nplt.xlabel(\"fraction of (sorted) population\")\nplt.ylabel(\"fraction of wealth\")\n</code></pre> <pre>\n<code>Text(0, 0.5, 'fraction of wealth')</code>\n</pre> <pre><code>plt.plot(model.datacollector.get_model_vars_dataframe())\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D at 0x7fbd2c5c33d0&gt;]</code>\n</pre> <p>In 2017 the Italian Gini index was 0.36 (World Bank estimate)</p> <pre><code># real GDP of the italian economy\n\nitalian_data = 0.36 * np.ones((100, 1))\n</code></pre> <pre><code># wrapper of the ABM with a specific signature\ndef boltzmann_model(theta, N, seed=None):\n    model = BoltzmannWealthModel(\n        num_agents=100,\n        width=10,\n        height=10,\n        generosity_ratio=theta[0],\n        mean_init_wealth=5,\n    )\n\n    # burn-in phase of 200 steps\n    model.run_model(200 + N)\n\n    data = model.datacollector.get_model_vars_dataframe().to_numpy()\n\n    # discard initialization and burn-in phase of 200 steps\n    return data[201:]\n\n\nboltzmann_model([1], N=5, seed=None)\n</code></pre> <pre>\n<code>array([[0.10088409],\n       [0.1002947 ],\n       [0.10170923],\n       [0.10332024],\n       [0.1046169 ]])</code>\n</pre> <pre><code># Euclidean distance between series\nfrom black_it.loss_functions.minkowski import MinkowskiLoss\n\nloss = MinkowskiLoss(p=2)\n</code></pre> <pre><code># choose samplers\nfrom black_it.samplers.halton import HaltonSampler\nfrom black_it.samplers.random_forest import RandomForestSampler\n\nhalton = HaltonSampler(batch_size=4)\nforest = RandomForestSampler(batch_size=4)\n</code></pre> <pre><code># reasonable range of parameter values\nparameters_bounds = np.array([[0.01, 100.0]]).T\nparameters_precision = [\n    0.01,\n]\n</code></pre> <pre><code># initialize calibrator\nfrom black_it.calibrator import Calibrator\n\ncal = Calibrator(\n    model=boltzmann_model,\n    samplers=[halton, forest],\n    loss_function=loss,\n    parameters_bounds=parameters_bounds,\n    parameters_precision=parameters_precision,\n    real_data=italian_data,\n    ensemble_size=3,\n)\n</code></pre> <pre>\n<code>\n***\nNumber of free params:       1.\nExplorable param space size: 10000.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n</code>\n</pre> <pre><code># calibrate for 5 epochs\nparams, losses = cal.calibrate(5)\n</code></pre> <pre>\n<code>\nBATCH NUMBER:   1\nPARAMS SAMPLED: 0\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 4.0s\n----&gt;   min loss new params: 1.27\n----&gt;   avg loss new params: 1.55\n----&gt; avg loss exist params: 1.55\n----&gt;         curr min loss: 1.2709260887871952\n====&gt;    total elapsed time: 4.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 2.8s\n----&gt;   min loss new params: 1.01\n----&gt;   avg loss new params: 1.37\n----&gt; avg loss exist params: 1.46\n----&gt;         curr min loss: 1.0059668811538809\n====&gt;    total elapsed time: 4.1s\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 8\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 3.3s\n----&gt;   min loss new params: 0.7\n----&gt;   avg loss new params: 1.42\n----&gt; avg loss exist params: 1.45\n----&gt;         curr min loss: 0.6952058780196919\n====&gt;    total elapsed time: 3.3s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 2.7s\n----&gt;   min loss new params: 0.09\n----&gt;   avg loss new params: 0.44\n----&gt; avg loss exist params: 1.2\n----&gt;         curr min loss: 0.09205704142039979\n====&gt;    total elapsed time: 4.1s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 16\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 2.5s\n----&gt;   min loss new params: 1.27\n----&gt;   avg loss new params: 1.65\n----&gt; avg loss exist params: 1.29\n----&gt;         curr min loss: 0.09205704142039979\n====&gt;    total elapsed time: 2.5s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 2.7s\n----&gt;   min loss new params: 0.08\n----&gt;   avg loss new params: 0.16\n----&gt; avg loss exist params: 1.1\n----&gt;         curr min loss: 0.07538664969409653\n====&gt;    total elapsed time: 3.9s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 24\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 2.7s\n----&gt;   min loss new params: 0.37\n----&gt;   avg loss new params: 1.22\n----&gt; avg loss exist params: 1.12\n----&gt;         curr min loss: 0.07538664969409653\n====&gt;    total elapsed time: 2.7s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 2.6s\n----&gt;   min loss new params: 0.08\n----&gt;   avg loss new params: 0.1\n----&gt; avg loss exist params: 0.99\n----&gt;         curr min loss: 0.07538664969409653\n====&gt;    total elapsed time: 3.7s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 32\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 2.9s\n----&gt;   min loss new params: 1.53\n----&gt;   avg loss new params: 1.55\n----&gt; avg loss exist params: 1.05\n----&gt;         curr min loss: 0.07538664969409653\n====&gt;    total elapsed time: 2.9s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 2.9s\n----&gt;   min loss new params: 0.11\n----&gt;   avg loss new params: 0.13\n----&gt; avg loss exist params: 0.96\n----&gt;         curr min loss: 0.07538664969409653\n====&gt;    total elapsed time: 4.1s\n</code>\n</pre> <pre><code># optimal parameter found\nparams[0]\n</code></pre> <pre>\n<code>array([3.7])</code>\n</pre> <pre><code>min_idx = np.argmin(cal.losses_samp)\nprint(min_idx)\nmax_idx = np.argmax(cal.losses_samp)\nprint(max_idx)\n</code></pre> <pre>\n<code>23\n18\n</code>\n</pre> <pre><code>plt.figure()\nfor i, simulated_series in enumerate(cal.series_samp[:, 0, :, 0]):\n    if i in (max_idx,):\n        continue\n    plt.plot(simulated_series, color=\"orange\", alpha=0.2)\n\nplt.plot(\n    cal.series_samp[min_idx, 0, :, 0],\n    color=\"green\",\n    label=\"optimal simulated series\",\n    alpha=0.9,\n    linewidth=3,\n)\nplt.plot(italian_data, \"k--\", label=\"reference value\", alpha=0.9)\n\nplt.xlabel(\"time step\")\nplt.ylabel(\"Gini index\")\nplt.legend()\nplt.ylim(0, 0.6)\n</code></pre> <pre>\n<code>(0.0, 0.6)</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"boltzmann_wealth_model/#a-simple-model-for-wealth-inequality","title":"A simple model for wealth inequality","text":""},{"location":"boltzmann_wealth_model/#calibrate-the-model-on-the-italian-gini-index","title":"Calibrate the model on the Italian Gini Index","text":""},{"location":"calibrator/","title":"Calibrator","text":"<p>The class used to perform a calibration.</p> Source code in <code>black_it/calibrator.py</code> <pre><code>class Calibrator(BaseSeedable):\n    \"\"\"The class used to perform a calibration.\"\"\"\n\n    STATE_VERSION = 0\n\n    def __init__(  # noqa: PLR0913\n        self,\n        loss_function: BaseLoss,\n        real_data: NDArray[np.float64],\n        model: Callable,\n        parameters_bounds: NDArray[np.float64] | list[list[float]],\n        parameters_precision: NDArray[np.float64] | list[float],\n        ensemble_size: int,\n        samplers: Sequence[BaseSampler] | None = None,\n        scheduler: BaseScheduler | None = None,\n        sim_length: int | None = None,\n        convergence_precision: int | None = None,\n        verbose: bool = True,\n        saving_folder: str | None = None,\n        random_state: int | None = None,\n        n_jobs: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Calibrator object.\n\n        It must be initialized with details on the parameters to explore,\n        on the model to calibrate, on the samplers and on the loss function to use.\n\n        Args:\n            loss_function: a loss function which evaluates the similarity between simulated and real datasets\n            real_data: an array containing the real time series\n            model: a model with free parameters to be calibrated\n            parameters_bounds: the bounds of the parameter space\n            parameters_precision: the precisions to be used for the discretization of the parameters\n            ensemble_size: number of repetitions to be run for each set of parameters to decrease statistical\n                fluctuations. For deterministic models this should be set to 1.\n            samplers: list of methods to be used in the calibration procedure\n            scheduler: the scheduler to be used in order to schedule the available samplers\n            sim_length: number of periods to simulate the model for, by default this is equal to the length of the\n                real time series.\n            convergence_precision: number of significant digits to consider in the convergence check. The check is\n                not performed if this is set to 'None'.\n            verbose: whether to print calibration updates\n            saving_folder: the name of the folder where data should be saved and/or retrieved\n            random_state: random state of the calibrator, used for model simulations and to initialise the samplers\n            n_jobs: the maximum number of concurrently running jobs. For more details, see the\n                [joblib.Parallel documentation](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html).\n\n        \"\"\"\n        BaseSeedable.__init__(self, random_state=random_state)\n        self.loss_function = loss_function\n        self.model = model\n        self.random_state = random_state\n        self.real_data = real_data\n        self.ensemble_size = ensemble_size\n\n        if sim_length is None:\n            self.N = self.real_data.shape[0]\n        else:\n            if sim_length != self.real_data.shape[0]:\n                warnings.warn(  # noqa: B028\n                    \"The length of real time series is different from the simulation length, \"\n                    f\"got {self.real_data.shape[0]} and {sim_length}. This may or may not be a problem depending \"\n                    \"on the loss function used.\",\n                    RuntimeWarning,\n                )\n            self.N = sim_length\n        self.D = self.real_data.shape[1]\n        self.verbose = verbose\n        self.convergence_precision = (\n            self._validate_convergence_precision(convergence_precision) if convergence_precision is not None else None\n        )\n        self.saving_folder = saving_folder\n\n        # Initialize search grid\n        self.param_grid = SearchSpace(parameters_bounds, parameters_precision, verbose)\n\n        # initialize arrays\n        self.params_samp = np.zeros((0, self.param_grid.dims))\n        self.losses_samp = np.zeros(0)\n        self.batch_num_samp: NDArray[np.int64] = np.zeros(0, dtype=int)\n        self.method_samp: NDArray[np.int64] = np.zeros(0, dtype=int)\n        self.series_samp: NDArray[np.float64] = np.zeros(\n            (0, self.ensemble_size, self.N, self.D),\n        )\n\n        # initialize variables before calibration\n        self.n_sampled_params = 0\n        self.current_batch_index = 0\n\n        # set number of processes for parallel evaluation of model\n        self.n_jobs = n_jobs if n_jobs is not None else multiprocessing.cpu_count()\n\n        print(\n            f\"Selecting {self.n_jobs} processes for the parallel evaluation of the model\",\n        )\n\n        self.scheduler = self.__validate_samplers_and_scheduler_constructor_args(\n            samplers,\n            scheduler,\n        )\n\n        self.samplers_id_table = self._construct_samplers_id_table(\n            list(self.scheduler.samplers),\n        )\n\n    @classmethod\n    def __validate_samplers_and_scheduler_constructor_args(\n        cls,\n        samplers: Sequence[BaseSampler] | None,\n        scheduler: BaseScheduler | None,\n    ) -&gt; BaseScheduler:\n        \"\"\"Validate the 'samplers' and the 'scheduler' arguments provided to the constructor.\"\"\"\n        both_none = samplers is None and scheduler is None\n        both_not_none = samplers is not None and scheduler is not None\n        if both_none and both_not_none:\n            msg = \"only one between 'samplers' and 'scheduler' must be provided\"\n            raise ValueError(\n                msg,\n            )\n\n        if samplers is not None:\n            return RoundRobinScheduler(samplers)\n\n        return cast(\"BaseScheduler\", scheduler)\n\n    def _set_samplers_seeds(self) -&gt; None:\n        \"\"\"Set the calibration seed.\"\"\"\n        self.scheduler.random_state = self.random_state\n\n        # \"burn\" seeds from the calibrator seed generator for backward compatibility\n        for _ in self.scheduler.samplers:\n            self._get_random_seed()\n\n    @staticmethod\n    def _construct_samplers_id_table(samplers: list[BaseSampler]) -&gt; dict[str, int]:\n        \"\"\"Construct the samplers-by-id table.\n\n        Given the list (built-in or user-defined) of samplers a calibration\n        session is going to use, return a map from the sampler human-readable\n        name to a numeric id (starting from 0).\n\n        Different calibration sessions may result in different conversion\n        tables.\n\n        Args:\n            samplers: the list of samplers of the calibrator\n\n        Returns:\n            A dict that maps from the given sampler names to unique ids.\n        \"\"\"\n        samplers_id_table = {}\n        sampler_id = 0\n\n        for sampler in samplers:\n            sampler_name = type(sampler).__name__\n            if sampler_name in samplers_id_table:\n                continue\n\n            samplers_id_table[sampler_name] = sampler_id\n            sampler_id = sampler_id + 1\n\n        return samplers_id_table\n\n    def set_samplers(self, samplers: list[BaseSampler]) -&gt; None:\n        \"\"\"Set the samplers list of the calibrator.\n\n        This method overwrites the samplers of the calibrator object with a custom list of samplers.\n\n        Args:\n            samplers: a list of samplers\n\n        \"\"\"\n        # overwrite the list of samplers\n        self.scheduler._samplers = tuple(samplers)  # noqa: SLF001\n        self.update_samplers_id_table(samplers)\n\n    def set_scheduler(self, scheduler: BaseScheduler) -&gt; None:\n        \"\"\"Overwrite the scheduler of the calibrator object.\"\"\"\n        self.scheduler = scheduler\n        self.update_samplers_id_table(self.scheduler.samplers)\n\n    def update_samplers_id_table(self, samplers: Sequence[BaseSampler]) -&gt; None:\n        \"\"\"Update the samplers_id_table attribute with possible new samplers.\"\"\"\n        # update the samplers_id_table with the new samplers, only if necessary\n        sampler_id = max(self.samplers_id_table.values()) + 1\n\n        for sampler in samplers:\n            sampler_name = type(sampler).__name__\n            if sampler_name in self.samplers_id_table:\n                continue\n\n            self.samplers_id_table[sampler_name] = sampler_id\n            sampler_id = sampler_id + 1\n\n    @classmethod\n    def restore_from_checkpoint(\n        cls,\n        checkpoint_path: str,\n        model: Callable,\n    ) -&gt; Calibrator:\n        \"\"\"Return an instantiated class from a database file and a model simulator.\n\n        Args:\n            checkpoint_path: the name of the database file to read from\n            model: the model to calibrate. It must be equal to the one already calibrated\n\n        Returns:\n            An initialised Calibrator object.\n        \"\"\"\n        (\n            parameters_bounds,\n            parameters_precision,\n            real_data,\n            ensemble_size,\n            sim_length,\n            _d,\n            convergence_precision,\n            verbose,\n            saving_file,\n            random_state,\n            random_generator_state,\n            model_name,\n            scheduler,\n            loss_function,\n            current_batch_index,\n            n_sampled_params,\n            n_jobs,\n            params_samp,\n            losses_samp,\n            series_samp,\n            batch_num_samp,\n            method_samp,\n        ) = load_calibrator_state(checkpoint_path, cls.STATE_VERSION)\n\n        _assert(\n            model_name == model.__name__,\n            (\"Error: the model provided appears to be different from the one present in the database\"),\n        )\n\n        calibrator = cls(\n            loss_function,\n            real_data,\n            model,\n            parameters_bounds,\n            parameters_precision,\n            ensemble_size,\n            scheduler=scheduler,\n            sim_length=sim_length,\n            convergence_precision=convergence_precision,\n            verbose=verbose,\n            saving_folder=saving_file,\n            random_state=random_state,\n            n_jobs=n_jobs,\n        )\n\n        calibrator.current_batch_index = current_batch_index\n        calibrator.n_sampled_params = n_sampled_params\n        calibrator.params_samp = params_samp\n        calibrator.losses_samp = losses_samp\n        calibrator.series_samp = series_samp\n        calibrator.batch_num_samp = batch_num_samp\n        calibrator.method_samp = method_samp\n\n        # reset the random number generator state\n        calibrator.random_generator.bit_generator.state = random_generator_state\n\n        return calibrator\n\n    def simulate_model(self, params: NDArray) -&gt; NDArray:\n        \"\"\"Simulate the model.\n\n        This method calls the model simulator in parallel on a given set of parameter values, a number of repeated\n        evaluations are performed for each parameter to average out random fluctuations.\n\n        Args:\n            params: the array of parameters for which the model should be evaluated\n        # noqa\n        Returns:\n            simulated_data: an array of dimensions (batch_size, ensemble_size, N, D) containing all\n                simulated time series\n        \"\"\"\n        rep_params = np.repeat(params, self.ensemble_size, axis=0)\n\n        simulated_data_list = Parallel(n_jobs=self.n_jobs)(\n            delayed(self.model)(param, self.N, self._get_random_seed()) for i, param in enumerate(rep_params)\n        )\n\n        simulated_data = np.array(simulated_data_list)\n\n        return np.reshape(\n            simulated_data,\n            (params.shape[0], self.ensemble_size, self.N, self.D),\n        )\n\n    def calibrate(self, n_batches: int) -&gt; tuple[NDArray, NDArray]:\n        \"\"\"Run calibration for n batches.\n\n        Args:\n            n_batches (int): number of 'batches' to be executed. Each batch runs over all methods\n\n        Returns:\n            The sampled parameters and the corresponding sampled losses.\n            Both arrays are sorted by increasing loss values\n        \"\"\"\n        if self.current_batch_index == 0:\n            # we only set the samplers' random state at the start of a calibration\n            self._set_samplers_seeds()\n\n        with self.scheduler.session():\n            for _ in range(n_batches):\n                print()\n                print(f\"BATCH NUMBER:   {self.current_batch_index + 1}\")\n                print(f\"PARAMS SAMPLED: {self.n_sampled_params}\")\n\n                method = self.scheduler.get_next_sampler()\n\n                t_start = time.time()\n                print()\n                print(f\"METHOD: {type(method).__name__}\")\n\n                # get new params from a specific sampler\n                new_params = method.sample(\n                    self.param_grid,\n                    self.params_samp,\n                    self.losses_samp,\n                )\n\n                t_eval = time.time()\n\n                # simulate an ensemble of models for different parameters\n\n                new_simulated_data = self.simulate_model(new_params)\n\n                new_losses = []\n\n                for sim_data_ensemble in new_simulated_data:\n                    new_loss = self.loss_function.compute_loss(\n                        sim_data_ensemble,\n                        self.real_data,\n                    )\n                    new_losses.append(new_loss)\n\n                # update arrays\n                self.params_samp = np.vstack((self.params_samp, new_params))\n                self.losses_samp = np.hstack((self.losses_samp, new_losses))\n                self.series_samp = np.vstack((self.series_samp, new_simulated_data))\n                self.batch_num_samp = np.hstack(\n                    (\n                        self.batch_num_samp,\n                        [self.current_batch_index] * method.batch_size,\n                    ),\n                )\n                self.method_samp = np.hstack(\n                    (\n                        self.method_samp,\n                        [self.samplers_id_table[type(method).__name__]] * method.batch_size,\n                    ),\n                )\n\n                # logging\n                t_end = time.time()\n                if self.verbose:\n                    min_dist_new_points = np.round(np.min(new_losses), 2)\n                    avg_dist_new_points = np.round(np.average(new_losses), 2)\n                    avg_dist_existing_points = np.round(np.average(self.losses_samp), 2)\n\n                    elapsed_tot = np.round(t_end - t_start, 1)\n                    elapsed_eval = np.round(t_end - t_eval, 1)\n                    print(\n                        textwrap.dedent(\n                            f\"\"\"\\\n                        ----&gt; sim exec elapsed time: {elapsed_eval}s\n                        ----&gt;   min loss new params: {min_dist_new_points}\n                        ----&gt;   avg loss new params: {avg_dist_new_points}\n                        ----&gt; avg loss exist params: {avg_dist_existing_points}\n                        ----&gt;         curr min loss: {np.min(self.losses_samp)}\n                        ====&gt;    total elapsed time: {elapsed_tot}s\n                        \"\"\",\n                        ),\n                        end=\"\",\n                    )\n\n                # update count of number of params sampled\n                self.n_sampled_params = self.n_sampled_params + len(new_params)\n\n                self.scheduler.update(\n                    self.current_batch_index,\n                    new_params,\n                    new_losses,  # type: ignore[arg-type]\n                    new_simulated_data,\n                )\n\n                self.current_batch_index += 1\n\n                # check convergence for early termination\n                if self.convergence_precision is not None:\n                    converged = self.check_convergence(\n                        self.losses_samp,\n                        self.n_sampled_params,\n                        self.convergence_precision,\n                    )\n                    if converged and self.verbose:\n                        print(\"\\nCONVERGENCE CHECK:\")\n                        print(\"Achieved convergence loss, stopping search.\")\n                        break\n\n                if self.saving_folder is not None:\n                    self.create_checkpoint(self.saving_folder)\n\n            idx = np.argsort(self.losses_samp)\n\n        return self.params_samp[idx], self.losses_samp[idx]\n\n    @staticmethod\n    def check_convergence(\n        losses_samp: NDArray,\n        n_sampled_params: int,\n        convergence_precision: int,\n    ) -&gt; bool:\n        \"\"\"Check convergence of the calibration.\n\n        Args:\n            losses_samp: the sampled losses\n            n_sampled_params: the number of sampled params\n            convergence_precision: the required convergence precision.\n\n        Returns:\n            True if the calibration converged, False otherwise.\n        \"\"\"\n        return np.round(np.min(losses_samp[:n_sampled_params]), convergence_precision) == 0\n\n    def create_checkpoint(self, file_name: str | os.PathLike) -&gt; None:\n        \"\"\"Save the current state of the object.\n\n        Args:\n            file_name: the name of the folder where the data will be saved\n        \"\"\"\n        checkpoint_path: Path = Path(file_name).resolve()\n\n        t_start = time.time()\n\n        model_name = self.model.__name__\n\n        save_calibrator_state(\n            checkpoint_path,\n            self.param_grid.parameters_bounds,\n            self.param_grid.parameters_precision,\n            self.real_data,\n            self.ensemble_size,\n            self.N,\n            self.D,\n            self.convergence_precision,\n            self.verbose,\n            self.saving_folder,\n            self.random_state,\n            self.random_generator.bit_generator.state,\n            model_name,\n            self.scheduler,\n            self.loss_function,\n            self.current_batch_index,\n            self.n_sampled_params,\n            self.n_jobs,\n            self.params_samp,\n            self.losses_samp,\n            self.series_samp,\n            self.batch_num_samp,\n            self.method_samp,\n        )\n\n        t_end = time.time()\n\n        elapsed = np.round(t_end - t_start, 1)\n        print(f\"Checkpoint saved in {elapsed}s\")\n\n    @staticmethod\n    def _validate_convergence_precision(convergence_precision: int) -&gt; int:\n        \"\"\"Validate convergence precision input.\"\"\"\n        _assert(\n            convergence_precision &gt;= 0,\n            f\"convergence precision must be an integer greater than 0, got {convergence_precision}\",\n            exception_class=ValueError,\n        )\n        return convergence_precision\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.__init__","title":"<code>__init__(self, loss_function, real_data, model, parameters_bounds, parameters_precision, ensemble_size, samplers=None, scheduler=None, sim_length=None, convergence_precision=None, verbose=True, saving_folder=None, random_state=None, n_jobs=None)</code>  <code>special</code>","text":"<p>Initialize the Calibrator object.</p> <p>It must be initialized with details on the parameters to explore, on the model to calibrate, on the samplers and on the loss function to use.</p> <p>Parameters:</p> Name Type Description Default <code>loss_function</code> <code>BaseLoss</code> <p>a loss function which evaluates the similarity between simulated and real datasets</p> required <code>real_data</code> <code>NDArray[np.float64]</code> <p>an array containing the real time series</p> required <code>model</code> <code>Callable</code> <p>a model with free parameters to be calibrated</p> required <code>parameters_bounds</code> <code>NDArray[np.float64] | list[list[float]]</code> <p>the bounds of the parameter space</p> required <code>parameters_precision</code> <code>NDArray[np.float64] | list[float]</code> <p>the precisions to be used for the discretization of the parameters</p> required <code>ensemble_size</code> <code>int</code> <p>number of repetitions to be run for each set of parameters to decrease statistical fluctuations. For deterministic models this should be set to 1.</p> required <code>samplers</code> <code>Sequence[BaseSampler] | None</code> <p>list of methods to be used in the calibration procedure</p> <code>None</code> <code>scheduler</code> <code>BaseScheduler | None</code> <p>the scheduler to be used in order to schedule the available samplers</p> <code>None</code> <code>sim_length</code> <code>int | None</code> <p>number of periods to simulate the model for, by default this is equal to the length of the real time series.</p> <code>None</code> <code>convergence_precision</code> <code>int | None</code> <p>number of significant digits to consider in the convergence check. The check is not performed if this is set to 'None'.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>whether to print calibration updates</p> <code>True</code> <code>saving_folder</code> <code>str | None</code> <p>the name of the folder where data should be saved and/or retrieved</p> <code>None</code> <code>random_state</code> <code>int | None</code> <p>random state of the calibrator, used for model simulations and to initialise the samplers</p> <code>None</code> <code>n_jobs</code> <code>int | None</code> <p>the maximum number of concurrently running jobs. For more details, see the joblib.Parallel documentation.</p> <code>None</code> Source code in <code>black_it/calibrator.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    loss_function: BaseLoss,\n    real_data: NDArray[np.float64],\n    model: Callable,\n    parameters_bounds: NDArray[np.float64] | list[list[float]],\n    parameters_precision: NDArray[np.float64] | list[float],\n    ensemble_size: int,\n    samplers: Sequence[BaseSampler] | None = None,\n    scheduler: BaseScheduler | None = None,\n    sim_length: int | None = None,\n    convergence_precision: int | None = None,\n    verbose: bool = True,\n    saving_folder: str | None = None,\n    random_state: int | None = None,\n    n_jobs: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Calibrator object.\n\n    It must be initialized with details on the parameters to explore,\n    on the model to calibrate, on the samplers and on the loss function to use.\n\n    Args:\n        loss_function: a loss function which evaluates the similarity between simulated and real datasets\n        real_data: an array containing the real time series\n        model: a model with free parameters to be calibrated\n        parameters_bounds: the bounds of the parameter space\n        parameters_precision: the precisions to be used for the discretization of the parameters\n        ensemble_size: number of repetitions to be run for each set of parameters to decrease statistical\n            fluctuations. For deterministic models this should be set to 1.\n        samplers: list of methods to be used in the calibration procedure\n        scheduler: the scheduler to be used in order to schedule the available samplers\n        sim_length: number of periods to simulate the model for, by default this is equal to the length of the\n            real time series.\n        convergence_precision: number of significant digits to consider in the convergence check. The check is\n            not performed if this is set to 'None'.\n        verbose: whether to print calibration updates\n        saving_folder: the name of the folder where data should be saved and/or retrieved\n        random_state: random state of the calibrator, used for model simulations and to initialise the samplers\n        n_jobs: the maximum number of concurrently running jobs. For more details, see the\n            [joblib.Parallel documentation](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html).\n\n    \"\"\"\n    BaseSeedable.__init__(self, random_state=random_state)\n    self.loss_function = loss_function\n    self.model = model\n    self.random_state = random_state\n    self.real_data = real_data\n    self.ensemble_size = ensemble_size\n\n    if sim_length is None:\n        self.N = self.real_data.shape[0]\n    else:\n        if sim_length != self.real_data.shape[0]:\n            warnings.warn(  # noqa: B028\n                \"The length of real time series is different from the simulation length, \"\n                f\"got {self.real_data.shape[0]} and {sim_length}. This may or may not be a problem depending \"\n                \"on the loss function used.\",\n                RuntimeWarning,\n            )\n        self.N = sim_length\n    self.D = self.real_data.shape[1]\n    self.verbose = verbose\n    self.convergence_precision = (\n        self._validate_convergence_precision(convergence_precision) if convergence_precision is not None else None\n    )\n    self.saving_folder = saving_folder\n\n    # Initialize search grid\n    self.param_grid = SearchSpace(parameters_bounds, parameters_precision, verbose)\n\n    # initialize arrays\n    self.params_samp = np.zeros((0, self.param_grid.dims))\n    self.losses_samp = np.zeros(0)\n    self.batch_num_samp: NDArray[np.int64] = np.zeros(0, dtype=int)\n    self.method_samp: NDArray[np.int64] = np.zeros(0, dtype=int)\n    self.series_samp: NDArray[np.float64] = np.zeros(\n        (0, self.ensemble_size, self.N, self.D),\n    )\n\n    # initialize variables before calibration\n    self.n_sampled_params = 0\n    self.current_batch_index = 0\n\n    # set number of processes for parallel evaluation of model\n    self.n_jobs = n_jobs if n_jobs is not None else multiprocessing.cpu_count()\n\n    print(\n        f\"Selecting {self.n_jobs} processes for the parallel evaluation of the model\",\n    )\n\n    self.scheduler = self.__validate_samplers_and_scheduler_constructor_args(\n        samplers,\n        scheduler,\n    )\n\n    self.samplers_id_table = self._construct_samplers_id_table(\n        list(self.scheduler.samplers),\n    )\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.calibrate","title":"<code>calibrate(self, n_batches)</code>","text":"<p>Run calibration for n batches.</p> <p>Parameters:</p> Name Type Description Default <code>n_batches</code> <code>int</code> <p>number of 'batches' to be executed. Each batch runs over all methods</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray]</code> <p>The sampled parameters and the corresponding sampled losses. Both arrays are sorted by increasing loss values</p> Source code in <code>black_it/calibrator.py</code> <pre><code>def calibrate(self, n_batches: int) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"Run calibration for n batches.\n\n    Args:\n        n_batches (int): number of 'batches' to be executed. Each batch runs over all methods\n\n    Returns:\n        The sampled parameters and the corresponding sampled losses.\n        Both arrays are sorted by increasing loss values\n    \"\"\"\n    if self.current_batch_index == 0:\n        # we only set the samplers' random state at the start of a calibration\n        self._set_samplers_seeds()\n\n    with self.scheduler.session():\n        for _ in range(n_batches):\n            print()\n            print(f\"BATCH NUMBER:   {self.current_batch_index + 1}\")\n            print(f\"PARAMS SAMPLED: {self.n_sampled_params}\")\n\n            method = self.scheduler.get_next_sampler()\n\n            t_start = time.time()\n            print()\n            print(f\"METHOD: {type(method).__name__}\")\n\n            # get new params from a specific sampler\n            new_params = method.sample(\n                self.param_grid,\n                self.params_samp,\n                self.losses_samp,\n            )\n\n            t_eval = time.time()\n\n            # simulate an ensemble of models for different parameters\n\n            new_simulated_data = self.simulate_model(new_params)\n\n            new_losses = []\n\n            for sim_data_ensemble in new_simulated_data:\n                new_loss = self.loss_function.compute_loss(\n                    sim_data_ensemble,\n                    self.real_data,\n                )\n                new_losses.append(new_loss)\n\n            # update arrays\n            self.params_samp = np.vstack((self.params_samp, new_params))\n            self.losses_samp = np.hstack((self.losses_samp, new_losses))\n            self.series_samp = np.vstack((self.series_samp, new_simulated_data))\n            self.batch_num_samp = np.hstack(\n                (\n                    self.batch_num_samp,\n                    [self.current_batch_index] * method.batch_size,\n                ),\n            )\n            self.method_samp = np.hstack(\n                (\n                    self.method_samp,\n                    [self.samplers_id_table[type(method).__name__]] * method.batch_size,\n                ),\n            )\n\n            # logging\n            t_end = time.time()\n            if self.verbose:\n                min_dist_new_points = np.round(np.min(new_losses), 2)\n                avg_dist_new_points = np.round(np.average(new_losses), 2)\n                avg_dist_existing_points = np.round(np.average(self.losses_samp), 2)\n\n                elapsed_tot = np.round(t_end - t_start, 1)\n                elapsed_eval = np.round(t_end - t_eval, 1)\n                print(\n                    textwrap.dedent(\n                        f\"\"\"\\\n                    ----&gt; sim exec elapsed time: {elapsed_eval}s\n                    ----&gt;   min loss new params: {min_dist_new_points}\n                    ----&gt;   avg loss new params: {avg_dist_new_points}\n                    ----&gt; avg loss exist params: {avg_dist_existing_points}\n                    ----&gt;         curr min loss: {np.min(self.losses_samp)}\n                    ====&gt;    total elapsed time: {elapsed_tot}s\n                    \"\"\",\n                    ),\n                    end=\"\",\n                )\n\n            # update count of number of params sampled\n            self.n_sampled_params = self.n_sampled_params + len(new_params)\n\n            self.scheduler.update(\n                self.current_batch_index,\n                new_params,\n                new_losses,  # type: ignore[arg-type]\n                new_simulated_data,\n            )\n\n            self.current_batch_index += 1\n\n            # check convergence for early termination\n            if self.convergence_precision is not None:\n                converged = self.check_convergence(\n                    self.losses_samp,\n                    self.n_sampled_params,\n                    self.convergence_precision,\n                )\n                if converged and self.verbose:\n                    print(\"\\nCONVERGENCE CHECK:\")\n                    print(\"Achieved convergence loss, stopping search.\")\n                    break\n\n            if self.saving_folder is not None:\n                self.create_checkpoint(self.saving_folder)\n\n        idx = np.argsort(self.losses_samp)\n\n    return self.params_samp[idx], self.losses_samp[idx]\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.check_convergence","title":"<code>check_convergence(losses_samp, n_sampled_params, convergence_precision)</code>  <code>staticmethod</code>","text":"<p>Check convergence of the calibration.</p> <p>Parameters:</p> Name Type Description Default <code>losses_samp</code> <code>NDArray</code> <p>the sampled losses</p> required <code>n_sampled_params</code> <code>int</code> <p>the number of sampled params</p> required <code>convergence_precision</code> <code>int</code> <p>the required convergence precision.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the calibration converged, False otherwise.</p> Source code in <code>black_it/calibrator.py</code> <pre><code>@staticmethod\ndef check_convergence(\n    losses_samp: NDArray,\n    n_sampled_params: int,\n    convergence_precision: int,\n) -&gt; bool:\n    \"\"\"Check convergence of the calibration.\n\n    Args:\n        losses_samp: the sampled losses\n        n_sampled_params: the number of sampled params\n        convergence_precision: the required convergence precision.\n\n    Returns:\n        True if the calibration converged, False otherwise.\n    \"\"\"\n    return np.round(np.min(losses_samp[:n_sampled_params]), convergence_precision) == 0\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.create_checkpoint","title":"<code>create_checkpoint(self, file_name)</code>","text":"<p>Save the current state of the object.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str | os.PathLike</code> <p>the name of the folder where the data will be saved</p> required Source code in <code>black_it/calibrator.py</code> <pre><code>def create_checkpoint(self, file_name: str | os.PathLike) -&gt; None:\n    \"\"\"Save the current state of the object.\n\n    Args:\n        file_name: the name of the folder where the data will be saved\n    \"\"\"\n    checkpoint_path: Path = Path(file_name).resolve()\n\n    t_start = time.time()\n\n    model_name = self.model.__name__\n\n    save_calibrator_state(\n        checkpoint_path,\n        self.param_grid.parameters_bounds,\n        self.param_grid.parameters_precision,\n        self.real_data,\n        self.ensemble_size,\n        self.N,\n        self.D,\n        self.convergence_precision,\n        self.verbose,\n        self.saving_folder,\n        self.random_state,\n        self.random_generator.bit_generator.state,\n        model_name,\n        self.scheduler,\n        self.loss_function,\n        self.current_batch_index,\n        self.n_sampled_params,\n        self.n_jobs,\n        self.params_samp,\n        self.losses_samp,\n        self.series_samp,\n        self.batch_num_samp,\n        self.method_samp,\n    )\n\n    t_end = time.time()\n\n    elapsed = np.round(t_end - t_start, 1)\n    print(f\"Checkpoint saved in {elapsed}s\")\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.restore_from_checkpoint","title":"<code>restore_from_checkpoint(checkpoint_path, model)</code>  <code>classmethod</code>","text":"<p>Return an instantiated class from a database file and a model simulator.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>the name of the database file to read from</p> required <code>model</code> <code>Callable</code> <p>the model to calibrate. It must be equal to the one already calibrated</p> required <p>Returns:</p> Type Description <code>Calibrator</code> <p>An initialised Calibrator object.</p> Source code in <code>black_it/calibrator.py</code> <pre><code>@classmethod\ndef restore_from_checkpoint(\n    cls,\n    checkpoint_path: str,\n    model: Callable,\n) -&gt; Calibrator:\n    \"\"\"Return an instantiated class from a database file and a model simulator.\n\n    Args:\n        checkpoint_path: the name of the database file to read from\n        model: the model to calibrate. It must be equal to the one already calibrated\n\n    Returns:\n        An initialised Calibrator object.\n    \"\"\"\n    (\n        parameters_bounds,\n        parameters_precision,\n        real_data,\n        ensemble_size,\n        sim_length,\n        _d,\n        convergence_precision,\n        verbose,\n        saving_file,\n        random_state,\n        random_generator_state,\n        model_name,\n        scheduler,\n        loss_function,\n        current_batch_index,\n        n_sampled_params,\n        n_jobs,\n        params_samp,\n        losses_samp,\n        series_samp,\n        batch_num_samp,\n        method_samp,\n    ) = load_calibrator_state(checkpoint_path, cls.STATE_VERSION)\n\n    _assert(\n        model_name == model.__name__,\n        (\"Error: the model provided appears to be different from the one present in the database\"),\n    )\n\n    calibrator = cls(\n        loss_function,\n        real_data,\n        model,\n        parameters_bounds,\n        parameters_precision,\n        ensemble_size,\n        scheduler=scheduler,\n        sim_length=sim_length,\n        convergence_precision=convergence_precision,\n        verbose=verbose,\n        saving_folder=saving_file,\n        random_state=random_state,\n        n_jobs=n_jobs,\n    )\n\n    calibrator.current_batch_index = current_batch_index\n    calibrator.n_sampled_params = n_sampled_params\n    calibrator.params_samp = params_samp\n    calibrator.losses_samp = losses_samp\n    calibrator.series_samp = series_samp\n    calibrator.batch_num_samp = batch_num_samp\n    calibrator.method_samp = method_samp\n\n    # reset the random number generator state\n    calibrator.random_generator.bit_generator.state = random_generator_state\n\n    return calibrator\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.set_samplers","title":"<code>set_samplers(self, samplers)</code>","text":"<p>Set the samplers list of the calibrator.</p> <p>This method overwrites the samplers of the calibrator object with a custom list of samplers.</p> <p>Parameters:</p> Name Type Description Default <code>samplers</code> <code>list[BaseSampler]</code> <p>a list of samplers</p> required Source code in <code>black_it/calibrator.py</code> <pre><code>def set_samplers(self, samplers: list[BaseSampler]) -&gt; None:\n    \"\"\"Set the samplers list of the calibrator.\n\n    This method overwrites the samplers of the calibrator object with a custom list of samplers.\n\n    Args:\n        samplers: a list of samplers\n\n    \"\"\"\n    # overwrite the list of samplers\n    self.scheduler._samplers = tuple(samplers)  # noqa: SLF001\n    self.update_samplers_id_table(samplers)\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.set_scheduler","title":"<code>set_scheduler(self, scheduler)</code>","text":"<p>Overwrite the scheduler of the calibrator object.</p> Source code in <code>black_it/calibrator.py</code> <pre><code>def set_scheduler(self, scheduler: BaseScheduler) -&gt; None:\n    \"\"\"Overwrite the scheduler of the calibrator object.\"\"\"\n    self.scheduler = scheduler\n    self.update_samplers_id_table(self.scheduler.samplers)\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.simulate_model","title":"<code>simulate_model(self, params)</code>","text":"<p>Simulate the model.</p> <p>This method calls the model simulator in parallel on a given set of parameter values, a number of repeated evaluations are performed for each parameter to average out random fluctuations.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>NDArray</code> <p>the array of parameters for which the model should be evaluated</p> required"},{"location":"calibrator/#black_it.calibrator.Calibrator.simulate_model--noqa","title":"noqa","text":"<p>Returns:</p> Type Description <code>simulated_data</code> <p>an array of dimensions (batch_size, ensemble_size, N, D) containing all     simulated time series</p> Source code in <code>black_it/calibrator.py</code> <pre><code>def simulate_model(self, params: NDArray) -&gt; NDArray:\n    \"\"\"Simulate the model.\n\n    This method calls the model simulator in parallel on a given set of parameter values, a number of repeated\n    evaluations are performed for each parameter to average out random fluctuations.\n\n    Args:\n        params: the array of parameters for which the model should be evaluated\n    # noqa\n    Returns:\n        simulated_data: an array of dimensions (batch_size, ensemble_size, N, D) containing all\n            simulated time series\n    \"\"\"\n    rep_params = np.repeat(params, self.ensemble_size, axis=0)\n\n    simulated_data_list = Parallel(n_jobs=self.n_jobs)(\n        delayed(self.model)(param, self.N, self._get_random_seed()) for i, param in enumerate(rep_params)\n    )\n\n    simulated_data = np.array(simulated_data_list)\n\n    return np.reshape(\n        simulated_data,\n        (params.shape[0], self.ensemble_size, self.N, self.D),\n    )\n</code></pre>"},{"location":"calibrator/#black_it.calibrator.Calibrator.update_samplers_id_table","title":"<code>update_samplers_id_table(self, samplers)</code>","text":"<p>Update the samplers_id_table attribute with possible new samplers.</p> Source code in <code>black_it/calibrator.py</code> <pre><code>def update_samplers_id_table(self, samplers: Sequence[BaseSampler]) -&gt; None:\n    \"\"\"Update the samplers_id_table attribute with possible new samplers.\"\"\"\n    # update the samplers_id_table with the new samplers, only if necessary\n    sampler_id = max(self.samplers_id_table.values()) + 1\n\n    for sampler in samplers:\n        sampler_name = type(sampler).__name__\n        if sampler_name in self.samplers_id_table:\n            continue\n\n        self.samplers_id_table[sampler_name] = sampler_id\n        sampler_id = sampler_id + 1\n</code></pre>"},{"location":"checkpointing_parallelisation/","title":"Saving and loading","text":"<p>For each batch execution of the calibration, a checkpoint can be created (save) so that it can be later restored (load).</p> <p>The functions responsible for these features are the method <code>create_checkpoint(file_name)</code> and the class method <code>restore_from_checkpoint(checkpoint_path, model)</code> of the <code>Calibrator</code> class.</p> <p>The checkpoint is created automatically at each batch execution, if a saving folder is provided. To do this, when a <code>Calibrator</code> object is instantiated, the <code>saving_folder</code> field must be specified as a string which contains the path where the checkpoint must be saved.</p> <p>Remark: there's no need to call the save method, once the saving folder is specified.</p> <p>To restore the checkpoint it suffices to call the load class method with the path name and, optionally, the model which was being calibrated. Once restored, the calibration starts from where it was interrupted.</p> <p>An example of save and load is as follows: <pre><code># initialize a Calibrator object\n    cal = Calibrator(\n        samplers=[\n            random_sampler,\n            halton_sampler,\n        ],\n        real_data=real_data,\n        model=model,\n        parameters_bounds=bounds,\n        parameters_precision=bounds_step,\n        ensemble_size=2,\n        loss_function=loss,\n        saving_folder=\"saving_folder\",\n        n_jobs=1,\n    )\n\n    _, _ = cal.calibrate(2)\n\n    cal_restored = Calibrator.restore_from_checkpoint(\"saving_folder\", model=model)\n</code></pre></p> <p>This code has been extracted from the following example: <code>tests/test_calibrator_restore_from_checkpoint.py</code>.</p> <p>Remark: the saving folder where the checkpoint is saved can also be used as a parameter for the plotting functions, as shown in the plotting tutorial, to produce plots quickly.</p>"},{"location":"checkpointing_parallelisation/#parallelisation","title":"Parallelisation","text":"<p>Since calibrating an agent-based model can be very intensive, by default the model simulation is parallelised. The number of parallel processes equals the number of cores in the computer. This number can be changed by specifying the optional parameter <code>n_jobs</code> in the constructor of the calibrator.</p>"},{"location":"citing_black_it/","title":"Citing Black-it","text":"<p>A description of the package is available here.</p> <p>Please consider citing it if you found this package useful for your research</p> <pre><code>@article{black_it, \n  title = {Black-it: A Ready-to-Use and Easy-to-Extend Calibration Kit for Agent-based Models}, \n  journal = {Journal of Open Source Software},\n  publisher = {The Open Journal}, \n  year = {2022}, \n  volume = {7}, \n  number = {79}, \n  pages = {4622}, \n  doi = {10.21105/joss.04622}, \n  url = {https://doi.org/10.21105/joss.04622}, \n  author = {Marco Benedetti and \n            Gennaro Catapano and \n            Francesco {De Sclavis} and \n            Marco Favorito and \n            Aldo Glielmo and \n            Davide Magnanimi and \n            Antonio Muci} \n}\n</code></pre>"},{"location":"contributing/","title":"Extending Black-it","text":"<p>Contributions to the library are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>There are various ways to contribute:</p> <ul> <li> <p>If you need support, want to report a bug or ask for features, you can check the Issues page and raise an issue, if applicable.</p> </li> <li> <p>If you would like to contribute a bug fix of feature then submit a Pull request.</p> </li> </ul> <p>For other kinds of feedback, you can contact one of the authors by email.</p>"},{"location":"contributing/#a-few-simple-rules","title":"A few simple rules","text":"<ul> <li> <p>All pull requests should be opened against the <code>develop</code> branch. Do not open a Pull Request against <code>main</code>.</p> </li> <li> <p>Before working on a feature, reach out to one of the core developers or discuss the feature in an issue. The library caters a diverse audience and new features require upfront coordination.</p> </li> <li> <p>Include unit tests for 100% coverage when you contribute new features, as they help to a) prove that your code works correctly, and b) guard against future breaking changes to lower the maintenance cost.</p> </li> <li> <p>Bug fixes also generally require unit tests, because the presence of bugs usually indicates insufficient test coverage.</p> </li> <li> <p>Whenever possible, keep API compatibility in mind when you change code in the <code>black_it</code> library. Reviewers of your pull request will comment on any API compatibility issues.</p> </li> <li> <p>All files must include a license header.</p> </li> <li> <p>Before committing and opening a PR, run all tests locally. This saves CI hours and ensures you only commit clean code.</p> </li> </ul>"},{"location":"contributing/#contributing-code","title":"Contributing code","text":"<p>If you have improvements, send us your pull requests!</p> <p>A team member will be assigned to review your pull requests. All tests are run as part of CI as well as various other checks (linters, static type checkers, security checkers, etc). If there are any problems, feedback is provided via GitHub. Once the pull request is approved and passes continuous integration checks, you or a team member can merge it.</p> <p>If you want to contribute, start working through the codebase, navigate to the GitHub \"issues\" tab and start looking through interesting issues. If you decide to start on an issue, leave a comment so that other people know that you're working on it. If you want to help out, but not alone, use the issue comment thread to coordinate.</p>"},{"location":"contributing/#development-setup","title":"Development setup","text":"<p>Set up your development environment by following these steps:</p> <ul> <li> <p>Install <code>Poetry</code>, either by running run <code>pip install poetry</code> or as indicated here.</p> </li> <li> <p>Get the latest version of the code by running</p> </li> </ul> <pre><code>git clone https://github.com/bancaditalia/black-it.git\ncd black-it\n</code></pre> <ul> <li>Setup a Poetry environment by running</li> </ul> <pre><code>poetry shell\npoetry install\n</code></pre>"},{"location":"contributing/#further-commands-needed-during-development","title":"Further commands needed during development","text":"<p>We have various commands which are helpful during development.</p> <ul> <li> <p>For linting and static analysis use: <pre><code>make lint-all\nmake static\nmake bandit\n</code></pre></p> </li> <li> <p>To apply <code>black</code> code formatter: <pre><code>make black\n</code></pre></p> </li> <li> <p>whereas, to only check compliance: <pre><code>make black-check\n</code></pre></p> </li> <li> <p>To apply <code>ruff</code> linter: <pre><code>make ruff-check\n</code></pre></p> </li> <li> <p>To run tests: <code>make test</code>.</p> </li> <li> <p>For testing <code>black_it.{SUBMODULE}</code> with <code>tests/test_{TESTMODULE}</code> use: <pre><code>make test-sub dir={SUBMODULE} tdir={TESTMODULE}\n</code></pre></p> </li> </ul> <p>e.g. <pre><code>make test-sub tdir=losses dir=loss_functions\n</code></pre></p>"},{"location":"description/","title":"How it works","text":"Illustration of Black-it calibration. The calibration (black lines) is divided in a maximum of n batches (blue lines), and in each batch S samplers are sequentially deployed (green lines). Each sampler suggests a set of parameters, for which the model is simulated and the loss function is evaluated."},{"location":"description/#how-it-works","title":"How it works","text":"<p>The calibrator is essentially an optimizer which works on simulated data, produced by a specified model,  and searches the parametric space by chaining a set of chosen algorithms.  For a complete example about this, check the Examples section.</p> <p>The calibrator works iteratively and each iteration consists of three steps:</p> <ol> <li>Search algorithms are employed sequentially to sample a set of parameters. In this sense, the parameter space is \"searched\"</li> <li>Data is simulated E times for each sampled parameter set. We will refer to E as <code>ensemble_size</code></li> <li>A loss function is evaluated, measuring the distance between the simulated data and the real data</li> </ol> <p>This is repeated until the loss function is very close to 0 or the maximum number of iterations is reached.  The process is illustrated in the above figure. </p> <p>Remark: the simulator is a distinct subsystem in this process. It can be as simple as a Python function or it can be an external simulator altogether. The calibrator simply runs it each time.</p>"},{"location":"description/#simulation","title":"Simulation","text":"<p>The basic premise behind the calibrator is that it uses simulated data. That is why it adapts well to agent-based models. Simulated data is produced in each iteration according to a given model, using the sampled parameters.</p> <p>A model can be any function which produces data and which depends on one or more parameters. This can be a probability distribution (like the standard Gaussian) or a stochastic process (like an ARMA model or a Markov chain). Some examples of these \"standard\" models are the ones found in <code>examples/models/simple_models.py</code>. The <code>examples/models</code> directory also contains other models, including agent-based ones used in epidemiology and economics. For a description of them, check the Examples section.</p> <p>Remark: if you need to implement your own model, check how to use and the simulator interface.</p>"},{"location":"description/#optimization","title":"Optimization","text":"<p>After the model is defined, the simulated datasets must be compared against the real dataset. The natural choice of distance for this is the mean distance between a simulated dataset and the real dataset, i.e., for each parameter \\theta, we have: $$ \\delta(\\theta) = \\frac{1}{E} \\sum_{e=1}^E L\\Big(\\mathbf{x}, \\mathbf{x}_e(\\theta)\\Big)$$ where:</p> <ul> <li>\\{\\mathbf{x}_e\\}_{e=1}^E are the E simulated datasets which depend upon the parameter \\theta</li> <li>\\mathbf{x} is the real dataset</li> <li>L is the chosen loss function</li> </ul> <p>The loss function L can be anything that suits the problem which is being studied. Already implemented loss functions are:</p> <ul> <li><code>MinkowskiLoss</code>, a generalization of the Euclidean distance (see Minkowski distance)</li> <li><code>MethodOfMomentsLoss</code>, the squared difference between real and simulated moments, based on method of simulated moments</li> <li><code>GslDivLoss</code>, which is the GLS-div information criterion introduced by Lamperti (2018) in An information theoretic criterion for empirical validation of simulation models.</li> <li><code>FourierLoss</code>, an Euclidean loss computed in the frequency domain.</li> </ul> <p>In principle one should minimize the function \\delta over all possible values of \\theta. This is not possible in practice of course and that is why a set of search algorithm is used to sample parameters and get as low a distance \\delta as we can get.</p> <p>The sampling procedure depends on the chosen algorithm(s). The implemented algorithms can be divided in two groups, depending on how they act:</p> <ul> <li><code>RandomUniformSampler</code>, <code>HaltonSampler</code> and <code>RSequenceSampler</code> sample at each iteration independently from previous iterations. This is because they aim at sampling uniformly over the space. In particular, the first one is a simple uniform distribution, while the latter two try to fill the parameter space as uniformly as possible (check low-discrepancy sequences). For this reason, they are suggested to be used as a starting point to be concatenated with other algorithms. For more information on Halton sequences and R sequence, check here and here.</li> <li><code>BestBatchSampler</code>, <code>GaussianProcessSampler</code>, <code>XGBoostSampler</code>, <code>RandomForestSampler</code>, and <code>ParticleSwarmSampler</code> sample using information from previous iterations (i.e. previously calculated distance functions). The first one samples new parameters around parameters with the lowest distance functions, the second, third and fourth one use respectively a Gaussian process, a random forest or an XGBoost classifier to predict the parameters with the lowest distance and sample those, the fifth implements a particle swarm optimization. The use of such algorithms may help against computationally intensive calibrations, which can happen when dealing, for example, with agent-based models.</li> </ul> <p>In this notebook you can find an overview of the different samplers.</p>"},{"location":"examples/","title":"Examples and tutorials","text":"<p>In this section, you can read about some example models, supplied by some Jupyter notebook tutorials, to better understand and explore the package. In the menu, you can find the links for all the notebooks. The tutorials include the models explained in this page, a tutorial on the different samplers and a plotting tutorial.</p>"},{"location":"examples/#toy-model","title":"Toy model","text":"<p>This is a simple Gaussian distribution with known parameters N(1,1). This has been thought for tutorial purposes only, so that estimated parameters can be compared to the true ones. The chosen loss function is the MSM loss and all five built-in samplers are employed. </p> <p>Here you can find the tutorial.</p>"},{"location":"examples/#simple-shock-model","title":"Simple shock model","text":"<p>This is a simple example of a stock price suddenly increasing due to a shock.</p>  P(t) =  \\begin{cases} p_1, \\quad t &lt; \\tau \\\\ p_2, \\quad t \\geq \\tau  \\end{cases}  <p>The model parameters are p_1, p_2 and \\tau.</p> <p>The model is fit on Slack Technologies stock price data, meant to quantify the shock after the spread of rumors about Salesforce wanting to acquire Slack (check here for the news). The loss function used is the quadratic one and the samplers are Halton, random forest and best batch.</p> <p>Here you can find the tutorial.</p>"},{"location":"examples/#sir","title":"SIR","text":"<p>This is an agent-based SIR model, whose parameters \\beta and \\gamma must be estimated. </p> <p>In this model, each agent has a probability of being infected by having a contact with another agent and a probability of recovering. Such probabilities are modeled by the SIR parameters, while the contact network is modeled by a random graph generated by a Watts-Strogatz model.</p> <p>As you will notice, in this tutorial, the model simulation is not implemented directly in Python but an external simulator is called and run on a docker container. For more information on how to use this option, check the simulator interface page.</p> <p>The chosen loss function is the quadratic loss and the chosen samplers are Halton, random forest and best batch samplers.</p> <p>The tutorial is divided in two parts: in the first one we will generate synthetic data feed it to black-it, in order to retrieve the parameters that were used to generate the synthetic time series.</p> <p>In the second part, we will extend the simple SIR model, introducing a structural break in the \\beta parameter. This break is meant to model a lockdown or any event changing the infection rate. The calibration must retrieve the infection rates before (\\beta_1) and after (\\beta_2) the structural break, the recovery rate \\gamma, and the moment t in which the break happened. In this second part the model is fit on Italian lockdown data in 2020, slightly modified for didactic purposes.</p> <p>Here you can find the tutorial.</p>"},{"location":"examples/#boltzmann-wealth-model","title":"Boltzmann wealth model","text":"<p>This is a simple agent-based model where every agent gives one unit of wealth to a random agent if a certain criterion is met (i.e. the ratio of the agents' wealth must be above a chosen threshold).</p> <p>Here you can find the tutorial where the model is calibrated on the Italian Gini index in 2017.</p>"},{"location":"finding_the_parameters_of_a_SIR_model/","title":"SIR model","text":"<pre><code># preparatory imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nfrom black_it.calibrator import Calibrator\nfrom black_it.loss_functions.minkowski import MinkowskiLoss\nfrom black_it.plot.plot_results import (\n    plot_convergence,\n    plot_losses,\n    plot_losses_interact,\n    plot_sampling,\n    plot_sampling_interact,\n)\nfrom black_it.samplers.best_batch import BestBatchSampler\nfrom black_it.samplers.halton import HaltonSampler\nfrom black_it.samplers.random_forest import RandomForestSampler\n\ncmap = matplotlib.colormaps[\"tab10\"].colors\n</code></pre> <pre><code>def plotSeries(title, SIR1, SIR2=None):\n    fig, ax1 = plt.subplots()\n    ax1.plot(SIR1[:, 0], \"-\", label=\"susceptible\", color=cmap[0])\n    ax1.plot(SIR1[:, 1], \"-\", label=\"infected\", color=cmap[1])\n    ax1.plot(SIR1[:, 2], \"-\", label=\"recovered\", color=cmap[2])\n    ax1.tick_params(axis=\"y\", labelcolor=cmap[0])\n    ax1.set_ylabel(\"susceptible per 1000 inhabitants\", color=cmap[0])\n    ax1.set_xlabel(\"weeks\")\n    ax1.legend(loc=5)\n    if SIR2 is not None:\n        ax1.plot(SIR2[:, 0], \"--\", label=\"susceptible\", color=cmap[0])\n        ax1.plot(SIR2[:, 1], \"--\", label=\"infected\", color=cmap[1])\n        ax1.plot(SIR2[:, 2], \"--\", label=\"recovered\", color=cmap[2])\n\n    ax1.title.set_text(title)\n    fig.tight_layout()\n    plt.show()\n\n\ndef printBestParams(params):\n    with np.printoptions(suppress=True, formatter={\"float_kind\": \"{:.3f}   \".format}):\n        print(f\"                  brktime  beta1    beta2    gamma\")\n        print(f\"Best parameters: {params[0]}\")\n</code></pre> <p>Let's start by using black-it to recover the \\beta and \\gamma parameter of a SIR model from synthetic data.</p> <p>To set up a calibration one needs to define the following components first:</p> <ol> <li>a model to be calibrated </li> <li>a loss function to measure the distance between the real time series and the simulated time series</li> <li>a set of samplers that iteratively suggest a set of parameter values to explore</li> <li>the parameter space that should be explored</li> </ol> <p>In order to use the SIR simulator, we need to download its docker image:</p> <ol> <li>Install Docker following the instructions given here</li> <li>Open Docker on your computer</li> <li>Run the following command in your terminal: <code>docker pull bancaditalia/abmsimulator</code></li> </ol> <p>We can then proceed to generate the data we are going to try to reproduce via calibration.</p> <pre><code>from models.sir.sir_docker import SIR\n\ntrue_params = [0.1, 0.1]  # in general not known!\nsynth_data = SIR(true_params, 50, 0)\n\nplotSeries(\"Synthetic data\", synth_data)\n</code></pre> <pre><code># import a quadratic loss, a simple squared difference bewteen the two series\nfrom black_it.loss_functions.minkowski import MinkowskiLoss\n\nloss = MinkowskiLoss()\n</code></pre> <pre><code>from black_it.samplers.best_batch import BestBatchSampler\nfrom black_it.samplers.halton import HaltonSampler\nfrom black_it.samplers.random_forest import RandomForestSampler\n\nbatch_size = 4\n\nhalton_sampler = HaltonSampler(batch_size=batch_size)\nrandom_forest_sampler = RandomForestSampler(batch_size=batch_size)\nbest_batch_sampler = BestBatchSampler(batch_size=batch_size)\n\nsamplers = [halton_sampler, random_forest_sampler, best_batch_sampler]\n</code></pre> <pre><code>bounds = [[0.001, 0.001], [1.00, 1.00]]\n\nprecisions = [0.001, 0.001]\n</code></pre> <pre><code>from black_it.calibrator import Calibrator\n\nsaving_folder = \"sir-test\"\n# initialize a Calibrator object\ncal = Calibrator(\n    samplers=samplers,\n    real_data=synth_data,\n    model=SIR,\n    parameters_bounds=bounds,\n    parameters_precision=precisions,\n    ensemble_size=1,\n    loss_function=loss,\n    saving_folder=saving_folder,\n)\n</code></pre> <pre>\n<code>\n***\nNumber of free params:       2.\nExplorable param space size: 1000000.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n</code>\n</pre> <p>Calibrate the model for five batches</p> <pre><code>params, losses = cal.calibrate(5)\n</code></pre> <pre>\n<code>\nBATCH NUMBER:   1\nPARAMS SAMPLED: 0\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 3.1s\n----&gt;   min loss new params: 1093.27\n----&gt;   avg loss new params: 1387.94\n----&gt; avg loss exist params: 1387.94\n----&gt;         curr min loss: 1093.2679706997856\n====&gt;    total elapsed time: 3.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 2.6s\n----&gt;   min loss new params: 994.25\n----&gt;   avg loss new params: 1401.16\n----&gt; avg loss exist params: 1394.55\n----&gt;         curr min loss: 994.2479459016207\n====&gt;    total elapsed time: 3.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 3.1s\n----&gt;   min loss new params: 1091.99\n----&gt;   avg loss new params: 1218.39\n----&gt; avg loss exist params: 1335.83\n----&gt;         curr min loss: 994.2479459016207\n====&gt;    total elapsed time: 3.1s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 12\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 7.1s\n----&gt;   min loss new params: 1401.9\n----&gt;   avg loss new params: 2014.58\n----&gt; avg loss exist params: 1505.52\n----&gt;         curr min loss: 994.2479459016207\n====&gt;    total elapsed time: 7.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 4.9s\n----&gt;   min loss new params: 1413.63\n----&gt;   avg loss new params: 2200.89\n----&gt; avg loss exist params: 1644.59\n----&gt;         curr min loss: 994.2479459016207\n====&gt;    total elapsed time: 6.2s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 4.7s\n----&gt;   min loss new params: 1084.36\n----&gt;   avg loss new params: 1109.2\n----&gt; avg loss exist params: 1555.36\n----&gt;         curr min loss: 994.2479459016207\n====&gt;    total elapsed time: 4.7s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 24\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 4.4s\n----&gt;   min loss new params: 880.29\n----&gt;   avg loss new params: 1508.88\n----&gt; avg loss exist params: 1548.72\n----&gt;         curr min loss: 880.2915915967067\n====&gt;    total elapsed time: 4.4s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 3.5s\n----&gt;   min loss new params: 850.56\n----&gt;   avg loss new params: 1041.11\n----&gt; avg loss exist params: 1485.27\n----&gt;         curr min loss: 850.5637796224285\n====&gt;    total elapsed time: 4.9s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 6.4s\n----&gt;   min loss new params: 881.79\n----&gt;   avg loss new params: 934.11\n----&gt; avg loss exist params: 1424.03\n----&gt;         curr min loss: 850.5637796224285\n====&gt;    total elapsed time: 6.5s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 36\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 4.0s\n----&gt;   min loss new params: 1670.77\n----&gt;   avg loss new params: 2426.46\n----&gt; avg loss exist params: 1524.27\n----&gt;         curr min loss: 850.5637796224285\n====&gt;    total elapsed time: 4.0s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 3.5s\n----&gt;   min loss new params: 331.47\n----&gt;   avg loss new params: 683.43\n----&gt; avg loss exist params: 1447.83\n----&gt;         curr min loss: 331.4650895275124\n====&gt;    total elapsed time: 4.6s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 3.6s\n----&gt;   min loss new params: 315.78\n----&gt;   avg loss new params: 675.06\n----&gt; avg loss exist params: 1383.44\n----&gt;         curr min loss: 315.7767534283539\n====&gt;    total elapsed time: 3.6s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 48\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 2.9s\n----&gt;   min loss new params: 745.54\n----&gt;   avg loss new params: 1471.68\n----&gt; avg loss exist params: 1390.22\n----&gt;         curr min loss: 315.7767534283539\n====&gt;    total elapsed time: 2.9s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 2.5s\n----&gt;   min loss new params: 943.59\n----&gt;   avg loss new params: 1157.78\n----&gt; avg loss exist params: 1373.62\n----&gt;         curr min loss: 315.7767534283539\n====&gt;    total elapsed time: 3.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 4.7s\n----&gt;   min loss new params: 498.19\n----&gt;   avg loss new params: 574.39\n----&gt; avg loss exist params: 1320.34\n----&gt;         curr min loss: 315.7767534283539\n====&gt;    total elapsed time: 4.7s\nCheckpoint saved in 0.0s\n</code>\n</pre> <pre><code># best parameters obtained so far\nparams[0]\n</code></pre> <pre>\n<code>array([0.088, 0.136])</code>\n</pre> <pre><code># index of the best fitting series\nidxmin = np.argmin(cal.losses_samp)\n</code></pre> <pre><code>plotSeries(\"Synthetic vs calibrated series\", synth_data, cal.series_samp[idxmin, 0])\n</code></pre> <p>In this part of the tutorial we will use black-it to find the parameters of a SIR model fitted on the italian Covid-19 epidemiological data.</p> <p>We will see that a proper modelling of the first wave of the epidemic requires the introduction of a structural break in the SIR simulator i.e., a specific point in time in which an abrupt change in the parameters occurs.</p> <p>This is useful to model the effect of the lockdown over the spreading of the epidemic.</p> <p>For didactic puposes, let's load a previously prepared dataset containing a very rough estimate of the SIR data for the first 20 weeks of the italian Covid-19 epidemic.</p> <p>As the official data underestimates the number of cases, Susceptible and Recovered were rescaled by a constant factor.</p> <pre><code>real_data = np.loadtxt(\"data/italy_20_weeks.txt\")\n</code></pre> <pre><code># plot the real time series we want to reproduce\nplotSeries(\"Real data\", real_data)\n</code></pre> <pre><code>from models.sir.sir_docker import SIR_w_breaks\n</code></pre> <pre><code># we'll use a quadratic loss, a simple squared difference bewteen the two series\nloss = MinkowskiLoss()\n</code></pre> <pre><code>sampler_batch_size = 16\nsamplers = [\n    HaltonSampler(batch_size=sampler_batch_size),\n    RandomForestSampler(batch_size=sampler_batch_size),\n    BestBatchSampler(batch_size=sampler_batch_size),\n]\n</code></pre> <pre><code>#    brktime, beta1, beta2, gamma\nbounds_w_breaks = [\n    [2, 0.1, 0, 0.1],\n    [7, 0.2, 0.1, 0.3],\n]\nprecisions_w_breaks = [1, 0.0005, 0.0005, 0.0005]\n</code></pre> <pre><code>saving_folder = \"output\"\n\n# initialize the Calibrator\ncal = Calibrator(\n    samplers=samplers,\n    real_data=real_data,\n    model=SIR_w_breaks,\n    parameters_bounds=bounds_w_breaks,\n    parameters_precision=precisions_w_breaks,\n    ensemble_size=1,\n    loss_function=loss,\n    saving_folder=saving_folder,\n)\n</code></pre> <pre>\n<code>\n***\nNumber of free params:       4.\nExplorable param space size: 97204806.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n</code>\n</pre> <p>Perform 5 calibration rounds.</p> <p>Note that, with these parameters, you would be able to achieve a much lower loss in 30 epochs.</p> <pre><code>params, losses = cal.calibrate(5)\n</code></pre> <pre>\n<code>\nBATCH NUMBER:   1\nPARAMS SAMPLED: 0\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 10.0s\n----&gt;   min loss new params: 78.97\n----&gt;   avg loss new params: 127.93\n----&gt; avg loss exist params: 127.93\n----&gt;         curr min loss: 78.96522708183643\n====&gt;    total elapsed time: 10.0s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 12.5s\n----&gt;   min loss new params: 69.4\n----&gt;   avg loss new params: 95.8\n----&gt; avg loss exist params: 111.87\n----&gt;         curr min loss: 69.39561568592492\n====&gt;    total elapsed time: 14.0s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 12.9s\n----&gt;   min loss new params: 67.18\n----&gt;   avg loss new params: 90.8\n----&gt; avg loss exist params: 104.85\n----&gt;         curr min loss: 67.1802120840003\n====&gt;    total elapsed time: 12.9s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 48\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 15.2s\n----&gt;   min loss new params: 58.32\n----&gt;   avg loss new params: 122.98\n----&gt; avg loss exist params: 109.38\n----&gt;         curr min loss: 58.32156435411211\n====&gt;    total elapsed time: 15.2s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 10.4s\n----&gt;   min loss new params: 57.7\n----&gt;   avg loss new params: 85.93\n----&gt; avg loss exist params: 104.69\n----&gt;         curr min loss: 57.70356830875495\n====&gt;    total elapsed time: 12.0s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 11.2s\n----&gt;   min loss new params: 58.89\n----&gt;   avg loss new params: 86.32\n----&gt; avg loss exist params: 101.63\n----&gt;         curr min loss: 57.70356830875495\n====&gt;    total elapsed time: 11.2s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 96\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 13.0s\n----&gt;   min loss new params: 64.77\n----&gt;   avg loss new params: 113.03\n----&gt; avg loss exist params: 103.26\n----&gt;         curr min loss: 57.70356830875495\n====&gt;    total elapsed time: 13.0s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 11.7s\n----&gt;   min loss new params: 42.16\n----&gt;   avg loss new params: 69.24\n----&gt; avg loss exist params: 99.01\n----&gt;         curr min loss: 42.162915251802474\n====&gt;    total elapsed time: 14.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 9.3s\n----&gt;   min loss new params: 47.7\n----&gt;   avg loss new params: 80.67\n----&gt; avg loss exist params: 96.97\n----&gt;         curr min loss: 42.162915251802474\n====&gt;    total elapsed time: 9.3s\nCheckpoint saved in 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 144\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 8.6s\n----&gt;   min loss new params: 78.91\n----&gt;   avg loss new params: 131.24\n----&gt; avg loss exist params: 100.4\n----&gt;         curr min loss: 42.162915251802474\n====&gt;    total elapsed time: 8.6s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 8.2s\n----&gt;   min loss new params: 50.61\n----&gt;   avg loss new params: 69.1\n----&gt; avg loss exist params: 97.55\n----&gt;         curr min loss: 42.162915251802474\n====&gt;    total elapsed time: 9.5s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 15.5s\n----&gt;   min loss new params: 42.16\n----&gt;   avg loss new params: 82.51\n----&gt; avg loss exist params: 96.3\n----&gt;         curr min loss: 42.162915251802474\n====&gt;    total elapsed time: 15.5s\nCheckpoint saved in 0.1s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 192\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 10.6s\n----&gt;   min loss new params: 77.07\n----&gt;   avg loss new params: 124.54\n----&gt; avg loss exist params: 98.47\n----&gt;         curr min loss: 42.162915251802474\n====&gt;    total elapsed time: 10.7s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 10.5s\n----&gt;   min loss new params: 53.1\n----&gt;   avg loss new params: 65.98\n----&gt; avg loss exist params: 96.15\n----&gt;         curr min loss: 42.162915251802474\n====&gt;    total elapsed time: 12.4s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 7.7s\n----&gt;   min loss new params: 41.11\n----&gt;   avg loss new params: 72.03\n----&gt; avg loss exist params: 94.54\n----&gt;         curr min loss: 41.11389045820134\n====&gt;    total elapsed time: 7.7s\nCheckpoint saved in 0.1s\n</code>\n</pre> <pre><code># best parameters obtained so far\nprintBestParams(params)\n</code></pre> <pre>\n<code>                  brktime  beta1    beta2    gamma\nBest parameters: [7.000    0.175    0.036    0.234   ]\n</code>\n</pre> <pre><code>idxmin = np.argmin(cal.losses_samp)\nplotSeries(\"real vs calibrated\", real_data, cal.series_samp[idxmin, 0])\n</code></pre> <pre><code>plot_sampling(saving_folder)\n</code></pre> <pre><code>plot_sampling_interact(saving_folder)\n</code></pre> <pre>\n<code>interactive(children=(Dropdown(description='batch_nums', options={'from 0 to 1': [0, 1], 'from 2 to 3': [2, 3]\u2026</code>\n</pre> <pre><code>plot_losses(saving_folder)\n</code></pre> <pre><code>plot_losses_interact(saving_folder)\n</code></pre> <pre>\n<code>interactive(children=(Dropdown(description='method_num', options={'HaltonSampler': 0, 'RandomForestSampler': 1\u2026</code>\n</pre> <pre><code>plot_convergence(saving_folder)\n</code></pre>"},{"location":"finding_the_parameters_of_a_SIR_model/#finding-the-parameters-of-a-sir-model","title":"Finding the parameters of a SIR model","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#basic-elements-of-a-sir-model","title":"Basic elements of a SIR model","text":"In our model, each agent transitions among 3 states: Susceptible, Infectious and Recovered.At each epoch, an Infectious agent has a probability \u03b2 of infecting its Susceptible neighbours, and a probability \u03b3 to transition to the Recovered state.From that moment on, it will no longer participate in the spreading of the disease. The connectivity between agents is modeled as a Watts-Strogatz small world random graph, a regular ring lattice of mean degree K where each node has a probability r of being randomly rewired.In our model, these parameters will be input-calibrated (i.e., fixed)."},{"location":"finding_the_parameters_of_a_SIR_model/#simple-calibration-over-synthetic-data","title":"Simple calibration over synthetic data","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#initialize-a-calibrator-object","title":"Initialize a calibrator object","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#1-model-simulator","title":"1. Model simulator","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#2-loss-function","title":"2. Loss function","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#3-samplers","title":"3. Samplers","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#4-parameter-space-bounds-and-precision","title":"4. Parameter space (bounds and precision)","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#finally-initialize-the-calibrator","title":"Finally, initialize the Calibrator","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#calibration","title":"Calibration","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#compare-the-synthetic-and-calibrated-series","title":"Compare the synthetic and calibrated series","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#calibration-of-a-more-advanced-sir-model-against-realistic-data","title":"Calibration of a more advanced SIR model against realistic data","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#load-reference-data","title":"Load reference data","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#initialize-a-calibrator-object_1","title":"Initialize a calibrator object","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#1-model-simulator_1","title":"1. Model simulator","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#2-loss-function_1","title":"2. Loss function","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#3-samplers_1","title":"3. Samplers","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#4-parameter-space-bounds-and-precision_1","title":"4. Parameter space (bounds and precision)","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#initialize-the-calibrator","title":"Initialize the Calibrator","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#calibration_1","title":"Calibration","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#compare-the-original-and-the-calibrated-time-series","title":"Compare the original and the calibrated time series","text":""},{"location":"finding_the_parameters_of_a_SIR_model/#plots","title":"Plots","text":""},{"location":"losses/","title":"Loss functions","text":"<p>BaseLoss interface.</p> Source code in <code>black_it/loss_functions/base.py</code> <pre><code>class BaseLoss(ABC):\n    \"\"\"BaseLoss interface.\"\"\"\n\n    def __init__(\n        self,\n        coordinate_weights: NDArray | None = None,\n        coordinate_filters: Sequence[Callable | None] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the loss function.\n\n        Args:\n            coordinate_weights: the weights of the loss coordinates.\n            coordinate_filters: filters/transformations to be applied to each simulated series before\n                the loss computation.\n        \"\"\"\n        self.coordinate_weights = coordinate_weights\n        self.coordinate_filters = coordinate_filters\n\n    def compute_loss(\n        self,\n        sim_data_ensemble: NDArray[np.float64],\n        real_data: NDArray[np.float64],\n    ) -&gt; float:\n        \"\"\"Compute the loss between simulated and real data.\n\n        Args:\n            sim_data_ensemble: an ensemble of simulated data, of shape (ensemble_size, N, D)\n            real_data: the real data, of shape (N, D)\n\n        Returns:\n            The loss value.\n        \"\"\"\n        num_coords = real_data.shape[1]\n\n        weights = self._check_coordinate_weights(num_coords)\n        filters = self._check_coordinate_filters(num_coords)\n\n        filtered_data = self._filter_data(filters, sim_data_ensemble)\n\n        loss = 0\n        for i in range(num_coords):\n            loss += self.compute_loss_1d(filtered_data[i], real_data[:, i]) * weights[i]\n\n        return loss\n\n    @staticmethod\n    def _filter_data(\n        filters: Sequence[Callable | None],\n        sim_data_ensemble: NDArray[np.float64],\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Filter the simulated time series.\"\"\"\n        filtered_data = []\n\n        for i, filter_ in enumerate(filters):\n            if filter_ is None:\n                filtered_data_1d = sim_data_ensemble[:, :, i]\n            else:\n                filtered_data_1d = np.array(\n                    [filter_(sim_data_ensemble[j, :, i]) for j in range(sim_data_ensemble.shape[0])],\n                )\n            filtered_data.append(filtered_data_1d)\n\n        return np.array(filtered_data)\n\n    def _check_coordinate_weights(self, num_coords: int) -&gt; NDArray[np.float64]:\n        \"\"\"Check self.coordinate_weights and return usable weights.\"\"\"\n        weights: NDArray[np.float64]\n\n        if self.coordinate_weights is None:\n            weights = np.ones(num_coords) / num_coords\n        else:\n            nb_coordinate_weights = len(self.coordinate_weights)\n            _assert(\n                nb_coordinate_weights == num_coords,\n                (\n                    \"the length of coordinate_weights should be equal \"\n                    f\"to the number of coordinates, got {nb_coordinate_weights} and {num_coords}\"\n                ),\n                exception_class=ValueError,\n            )\n            weights = self.coordinate_weights\n\n        return weights\n\n    def _check_coordinate_filters(self, num_coords: int) -&gt; Sequence[Callable | None]:\n        \"\"\"Check self.coordinate_filters and return usable filters.\"\"\"\n        filters: Sequence[Callable | None]\n\n        if self.coordinate_filters is None:\n            # a list of identity functions\n            filters = [None] * num_coords\n        else:\n            nb_coordinate_filters = len(self.coordinate_filters)\n            _assert(\n                nb_coordinate_filters == num_coords,\n                (\n                    \"the length of coordinate_filters should be equal \"\n                    f\"to the number of coordinates, got {nb_coordinate_filters} and {num_coords}\"\n                ),\n                exception_class=ValueError,\n            )\n            filters = self.coordinate_filters\n\n        return filters\n\n    @abstractmethod\n    def compute_loss_1d(\n        self,\n        sim_data_ensemble: NDArray[np.float64],\n        real_data: NDArray[np.float64],\n    ) -&gt; float:\n        \"\"\"Return the loss between a specific coordinate of two time series.\n\n        Concrete classes have to override this method in order to implement new\n        loss functions.\n\n        Args:\n            sim_data_ensemble: an ensemble of simulated 1D series, shape (ensemble_size, N, 1)\n            real_data: the real data, shape (N, 1)\n\n        Returns:\n            the computed loss over the specific coordinate\n        \"\"\"\n</code></pre> <p>This module contains the implementation of the Fast Fourier Transform loss.</p> <p>Class for the Minkowski loss.</p> Source code in <code>black_it/loss_functions/minkowski.py</code> <pre><code>class MinkowskiLoss(BaseLoss):\n    \"\"\"Class for the Minkowski loss.\"\"\"\n\n    def __init__(\n        self,\n        p: int = 2,\n        coordinate_weights: NDArray | None = None,\n        coordinate_filters: list[Callable | None] | None = None,  # noqa: ARG002\n    ) -&gt; None:\n        \"\"\"Loss computed using a Minkowski distance.\n\n        The [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance)\n        is a generalization of both the Manhattan distance (p=1) and the Euclidean distance (p=2).\n\n        This function computes the Minkowski distance between two series.\n\n        Note: this class is a wrapper of scipy.spatial.distance.minkowski\n\n        Args:\n            p: The order of the norm used to compute the distance between real and simulated series\n            coordinate_weights: The order of the norm used to compute the distance between real and simulated series\n            coordinate_filters: filters/transformations to be applied to each simulated series before\n                the loss computation.\n        \"\"\"\n        self.p = p\n        super().__init__(coordinate_weights)\n\n    def compute_loss_1d(\n        self,\n        sim_data_ensemble: NDArray[np.float64],\n        real_data: NDArray[np.float64],\n    ) -&gt; float:\n        \"\"\"Call scipy.spatial.distance.minkowski() on its arguments.\n\n        Args:\n            sim_data_ensemble: the first operand\n            real_data: the second operand\n\n        Returns:\n            The computed loss over the specific coordinate.\n        \"\"\"\n        # average simulated time series\n        sim_data_ensemble = sim_data_ensemble.mean(axis=0)\n\n        return minkowski(sim_data_ensemble, real_data, p=self.p)\n</code></pre> <p>Class for the 'method of moments' loss.</p> Source code in <code>black_it/loss_functions/msm.py</code> <pre><code>class MethodOfMomentsLoss(BaseLoss):\n    \"\"\"Class for the 'method of moments' loss.\"\"\"\n\n    def __init__(\n        self,\n        covariance_mat: str | NDArray[np.float64] = \"identity\",\n        coordinate_weights: NDArray[np.float64] | None = None,\n        moment_calculator: MomentCalculator = get_mom_ts_1d,\n        coordinate_filters: list[Callable | None] | None = None,\n        standardise_moments: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the loss function based on the 'method of moments'.\n\n        Returns the MSM objective function, i.e. the square difference between\n        the moments of the two time series.\n        By default the loss computes the moments using\n        black_it.utils.time_series.get_mom_ts_1d(), which computes an\n        18-dimensional vector of statistics.\n\n        You can alter the behaviour passing a custom function to\n        moment_calculator. Please note that there is a constraint between the\n        moment calculator and the size of the covariance matrix.\n\n        Args:\n            covariance_mat: covariance matrix between the moments.\n                'identity' (the default) gives the identity matrix, 'inverse_variance' gives the diagonal\n                matrix of the estimated inverse variances. Alternatively, one can specify a numerical symmetric matrix\n                whose size must be equal to the number of elements that the moment calculator returns.\n            coordinate_weights: importance of each coordinate. By default all\n                coordinates are treated equally.\n            moment_calculator: a function that takes a 1D time series and\n                returns a series of moments. The default is\n                black_it.utils.time_series.get_mom_ts_1d()\n            coordinate_filters: filters/transformations to be applied to each simulated series before\n                the loss computation.\n            standardise_moments: if True all moments are divided by the absolute value of the real moments,\n                default is False.\n        \"\"\"\n        MethodOfMomentsLoss._validate_covariance_and_calculator(\n            moment_calculator,\n            covariance_mat,\n        )\n\n        super().__init__(coordinate_weights, coordinate_filters)\n        self._covariance_mat = covariance_mat\n        self._moment_calculator = moment_calculator\n        self._standardise_moments = standardise_moments\n\n    @staticmethod\n    def _validate_covariance_and_calculator(\n        moment_calculator: MomentCalculator,\n        covariance_mat: NDArray[np.float64] | str,\n    ) -&gt; None:\n        \"\"\"Validate the covariance matrix.\n\n        Args:\n            moment_calculator: the moment calculator\n            covariance_mat: the covariance matrix, either as a string or as a numpy array\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: if the covariance matrix is not valid.\n                If it is a string, ot can be invalid if it is not one of the possible options.\n                If it is a numpy array, it can be invalid if it is not symmetric or if moment_calculator\n                is the default get_mom_ts_1d and the covariance matrix's shape\n                is not 18. Other possible errors won't be caught by this\n                function, and can only be detected at runtime.\n        \"\"\"\n        if isinstance(covariance_mat, str):\n            try:\n                _CovarianceMatrixType(covariance_mat)\n            except ValueError as exc:\n                msg = f\"expected one of {[x.value for x in _CovarianceMatrixType]}, got {covariance_mat})\"\n                raise ValueError(\n                    msg,\n                ) from exc\n            return\n\n        if isinstance(covariance_mat, np.ndarray):\n            # a non null covariance_mat was given\n            if not is_symmetric(covariance_mat):\n                msg = \"the provided covariance matrix is not valid as it is not a symmetric matrix\"\n                raise ValueError(\n                    msg,\n                )\n            if (moment_calculator is get_mom_ts_1d) and (covariance_mat.shape[0] != _NB_MOMENTS):\n                msg = (\n                    \"the provided covariance matrix is not valid as it has a wrong shape: \"\n                    f\"expected {_NB_MOMENTS}, got {covariance_mat.shape[0]}\"\n                )\n                raise ValueError(\n                    msg,\n                )\n            return\n\n        msg = \"please specify a valid covariance matrix, either as a string or directly as a numpy array\"\n        raise ValueError(\n            msg,\n        )\n\n    def compute_loss_1d(\n        self,\n        sim_data_ensemble: NDArray[np.float64],\n        real_data: NDArray[np.float64],\n    ) -&gt; float:\n        \"\"\"Compute the loss based on the 'method of moments'.\n\n        Returns the MSM objective function, i.e. the square difference between the moments of the two time series.\n\n        Args:\n            sim_data_ensemble: the first operand\n            real_data: the second operand\n\n        Returns:\n            the MSM loss over a specific coordinate.\n        \"\"\"\n        # compute the moments for the simulated ensemble\n        ensemble_sim_mom_1d = np.array(\n            [self._moment_calculator(s) for s in sim_data_ensemble],\n        )\n\n        # compute moments of the real time series\n        real_mom_1d = self._moment_calculator(real_data)\n\n        # write moments in relative terms is needed\n        if self._standardise_moments:\n            ensemble_sim_mom_1d = ensemble_sim_mom_1d / (abs(real_mom_1d)[None, :])\n            real_mom_1d = real_mom_1d / abs(real_mom_1d)\n\n        # mean simulated moments\n        sim_mom_1d = np.mean(ensemble_sim_mom_1d, axis=0)\n\n        g = real_mom_1d - sim_mom_1d\n\n        if isinstance(self._covariance_mat, str) and self._covariance_mat == _CovarianceMatrixType.IDENTITY.value:\n            return g.dot(g)\n        if (\n            isinstance(self._covariance_mat, str)\n            and self._covariance_mat == _CovarianceMatrixType.INVERSE_VARIANCE.value\n        ):\n            W = np.diag(  # noqa: N806\n                1.0 / np.mean((real_mom_1d[None, :] - ensemble_sim_mom_1d) ** 2, axis=0),\n            )\n        else:\n            self._covariance_mat = cast(\"NDArray[np.float64]\", self._covariance_mat)\n            W = self._covariance_mat  # noqa: N806\n        try:\n            loss_1d = g.dot(W).dot(g)\n        except ValueError as e:\n            covariance_size = W.shape[0]\n            moments_size = g.shape[0]\n\n            if covariance_size == moments_size:\n                # this value error is not due to a mismatch between the\n                # covariance matrix size and the number moments. Let's raise the\n                # original error.\n                raise\n\n            msg = (\n                f\"The size of the covariance matrix ({covariance_size}) \"\n                f\"and the number of moments ({moments_size}) should be identical\"\n            )\n            raise ValueError(\n                msg,\n            ) from e\n\n        return loss_1d\n</code></pre> <p>Class for the Gsl-div loss.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expected_loss = 0.39737637181336855\n&gt;&gt;&gt; np.random.seed(11)\n&gt;&gt;&gt; series1 = np.random.normal(0, 1, (100, 3))\n&gt;&gt;&gt; series2 = np.random.normal(0, 1, (100, 3))\n&gt;&gt;&gt; loss_func = GslDivLoss()\n&gt;&gt;&gt; loss = loss_func.compute_loss(series1[None, :, :], series2)\n&gt;&gt;&gt; assert np.isclose(expected_loss, loss)\n</code></pre> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>class GslDivLoss(BaseLoss):\n    \"\"\"Class for the Gsl-div loss.\n\n    Example:\n        &gt;&gt;&gt; expected_loss = 0.39737637181336855\n        &gt;&gt;&gt; np.random.seed(11)\n        &gt;&gt;&gt; series1 = np.random.normal(0, 1, (100, 3))\n        &gt;&gt;&gt; series2 = np.random.normal(0, 1, (100, 3))\n        &gt;&gt;&gt; loss_func = GslDivLoss()\n        &gt;&gt;&gt; loss = loss_func.compute_loss(series1[None, :, :], series2)\n        &gt;&gt;&gt; assert np.isclose(expected_loss, loss)\n    \"\"\"\n\n    def __init__(\n        self,\n        nb_values: int | None = None,\n        nb_word_lengths: int | None = None,\n        coordinate_weights: NDArray | None = None,\n        coordinate_filters: list[Callable | None] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the GSL-div loss object.\n\n        Args:\n            nb_values: number of values the digitised series can take\n            nb_word_lengths: the number of word length to consider\n            coordinate_weights: the weights of the loss coordinates\n            coordinate_filters: filters/transformations to be applied to each simulated series before\n                the loss computation.\n        \"\"\"\n        super().__init__(coordinate_weights, coordinate_filters)\n        self.nb_values = nb_values\n        self.nb_word_lengths = nb_word_lengths\n\n    def compute_loss_1d(\n        self,\n        sim_data_ensemble: NDArray[np.float64],\n        real_data: NDArray[np.float64],\n    ) -&gt; float:\n        \"\"\"Return the GSL-div measure.\n\n        From (Lamperti, 2017):\n\n        &gt; The information loss about the behaviour of the stochastic process\n          due to the symbolization becomes smaller and smaller as b increases.\n          On the other side, low values of b would likely wash away\n          processes' noise the modeller might not be interested in.\n\n        Args:\n            sim_data_ensemble: the ensemble of simulated data\n            real_data: the real data\n\n        Returns:\n            the GSL loss\n        \"\"\"\n        ts_length = len(real_data)\n        ensemble_size = sim_data_ensemble.shape[0]\n\n        nb_values = int((ts_length - 1) / 2.0) if self.nb_values is None else self.nb_values\n\n        nb_word_lengths = int((ts_length - 1) / 2.0) if self.nb_word_lengths is None else self.nb_word_lengths\n\n        # discretize real time series\n        obs_xd = self.discretize(\n            real_data,\n            nb_values,\n            np.min(real_data),\n            np.max(real_data),\n        )\n\n        gsl_loss = 0.0\n\n        # average loss over the ensemble\n        for sim_data in sim_data_ensemble:\n            # discretize simulated series\n            sim_xd = self.discretize(\n                sim_data,\n                nb_values,\n                np.min(sim_data),\n                np.max(sim_data),\n            )\n\n            loss = self.gsl_div_1d_1_sample(\n                sim_xd,\n                obs_xd,\n                nb_word_lengths,\n                nb_values,\n                ts_length,\n            )\n\n            gsl_loss += loss\n\n        return gsl_loss / ensemble_size\n\n    def gsl_div_1d_1_sample(\n        self,\n        sim_xd: NDArray,\n        obs_xd: NDArray,\n        nb_word_lengths: int,\n        nb_values: int,\n        ts_length: int,\n    ) -&gt; float:\n        \"\"\"Compute the GSL-div for a single realisation of the simulated data.\n\n        Args:\n            sim_xd: discretised simulated series\n            obs_xd: discretised real series\n            nb_word_lengths: the number of word length to consider\n            nb_values: number of values the digitised series can take\n            ts_length: the length of real and simulated series\n\n        Returns:\n            the computed loss\n        \"\"\"\n        # outcome measure\n        gsl_div = 0.0\n\n        # weight\n        weight = 0.0\n\n        # for any word len:\n        for word_length in range(1, nb_word_lengths + 1):\n            sim_xw = self.get_words(sim_xd, word_length)\n            obs_xw = self.get_words(obs_xd, word_length)\n            m_xw = np.concatenate((sim_xw, obs_xw))\n            sim_xp = self.get_words_est_prob(sim_xw)\n            m_xp = self.get_words_est_prob(m_xw)\n            base = float(nb_values**word_length)\n            sim_entr = self.get_sh_entr(sim_xp, base)\n            m_entr = self.get_sh_entr(m_xp, base)\n\n            # update weight\n            weight = weight + 2 / (nb_word_lengths * (nb_word_lengths + 1))\n\n            # correction\n            corr = ((len(m_xp) - 1) - (len(sim_xp) - 1)) / (2 * ts_length)\n\n            # add to measure\n            gsl_divl = 2 * m_entr - sim_entr + corr\n            gsl_div = gsl_div + weight * gsl_divl\n\n        # end of cycle, return\n        return gsl_div\n\n    @staticmethod\n    def discretize(\n        time_series: NDArray[np.float64],\n        nb_values: int,\n        start_index: np.float64 | float,\n        stop_index: np.float64 | float,\n    ) -&gt; NDArray[np.int64]:\n        \"\"\"Discretize the TS in 'nb_values' finite states.\n\n        &gt;&gt;&gt; GslDivLoss.discretize(\n        ...     [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        ...     nb_values=3,\n        ...     start_index=1,\n        ...     stop_index=10\n        ... ).tolist()\n        [1, 1, 1, 2, 2, 2, 2, 3, 3, 3]\n\n        Args:\n            time_series: any univariate time series\n            nb_values: int, number of values the digitised series can take.\n                It must be greater than 0.\n            start_index: the starting point\n            stop_index: the stopping point\n\n        Returns:\n            the discretised time series\n        \"\"\"\n        start_index = np.float64(start_index)\n        stop_index = np.float64(stop_index)\n        linspace = np.linspace(start_index - EPS, stop_index + EPS, nb_values + 1)\n\n        return np.searchsorted(linspace, time_series, side=\"left\")\n\n    @staticmethod\n    def get_words(time_series: NDArray[np.float64], length: int) -&gt; NDArray:\n        \"\"\"Return an overlapping array of words (int32) of 'length' given a discretised vector.\n\n        &gt;&gt;&gt; GslDivLoss.get_words(np.asarray([1, 2, 2, 2]), 2)\n        array([12, 22, 22])\n\n        Args:\n            time_series: any univariate discretised time series\n            length: int, len of words to be returned. Must be such that\n                (len(time_series) + 1 - length) is positive.\n\n        Returns:\n            the time series of overlapping words\n\n        \"\"\"\n        tswlen = len(time_series) + 1 - length\n        _assert(\n            tswlen &gt;= 0,\n            \"the chosen word length is too high\",\n            exception_class=ValueError,\n        )\n        tsw: NDArray[np.float64] = np.zeros(shape=(tswlen,), dtype=np.int32)\n\n        for i in range(length):\n            k = 10 ** (length - i - 1)\n            tsw = tsw + time_series[i : tswlen + i] * k\n\n        return tsw\n\n    @staticmethod\n    def get_words_est_prob(time_series: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n        \"\"\"Return an array of estimated probabilities given an array of words (int32).\n\n        Args:\n            time_series: any univariate array of words\n\n        Returns:\n            estimate of probabilities\n        \"\"\"\n        _, count = np.unique(time_series, return_counts=True)\n        return np.divide(count, np.sum(count))\n\n    @staticmethod\n    def get_sh_entr(probs: NDArray[np.float64], log_base: float) -&gt; float:\n        \"\"\"Return the Shannon entropy given an array of probabilities.\n\n        Args:\n            probs: an array of probabilities describing the discrete probability distribution\n            log_base: the entropy logarithm base.\n\n        Returns:\n            the entropy of the discrete probability distribution\n        \"\"\"\n        log = np.log(probs) / np.log(log_base)\n        return -np.sum(np.multiply(probs, log))\n</code></pre>"},{"location":"losses/#black_it.loss_functions.base.BaseLoss.__init__","title":"<code>__init__(self, coordinate_weights=None, coordinate_filters=None)</code>  <code>special</code>","text":"<p>Initialize the loss function.</p> <p>Parameters:</p> Name Type Description Default <code>coordinate_weights</code> <code>NDArray | None</code> <p>the weights of the loss coordinates.</p> <code>None</code> <code>coordinate_filters</code> <code>Sequence[Callable | None] | None</code> <p>filters/transformations to be applied to each simulated series before the loss computation.</p> <code>None</code> Source code in <code>black_it/loss_functions/base.py</code> <pre><code>def __init__(\n    self,\n    coordinate_weights: NDArray | None = None,\n    coordinate_filters: Sequence[Callable | None] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the loss function.\n\n    Args:\n        coordinate_weights: the weights of the loss coordinates.\n        coordinate_filters: filters/transformations to be applied to each simulated series before\n            the loss computation.\n    \"\"\"\n    self.coordinate_weights = coordinate_weights\n    self.coordinate_filters = coordinate_filters\n</code></pre>"},{"location":"losses/#black_it.loss_functions.base.BaseLoss.compute_loss","title":"<code>compute_loss(self, sim_data_ensemble, real_data)</code>","text":"<p>Compute the loss between simulated and real data.</p> <p>Parameters:</p> Name Type Description Default <code>sim_data_ensemble</code> <code>NDArray[np.float64]</code> <p>an ensemble of simulated data, of shape (ensemble_size, N, D)</p> required <code>real_data</code> <code>NDArray[np.float64]</code> <p>the real data, of shape (N, D)</p> required <p>Returns:</p> Type Description <code>float</code> <p>The loss value.</p> Source code in <code>black_it/loss_functions/base.py</code> <pre><code>def compute_loss(\n    self,\n    sim_data_ensemble: NDArray[np.float64],\n    real_data: NDArray[np.float64],\n) -&gt; float:\n    \"\"\"Compute the loss between simulated and real data.\n\n    Args:\n        sim_data_ensemble: an ensemble of simulated data, of shape (ensemble_size, N, D)\n        real_data: the real data, of shape (N, D)\n\n    Returns:\n        The loss value.\n    \"\"\"\n    num_coords = real_data.shape[1]\n\n    weights = self._check_coordinate_weights(num_coords)\n    filters = self._check_coordinate_filters(num_coords)\n\n    filtered_data = self._filter_data(filters, sim_data_ensemble)\n\n    loss = 0\n    for i in range(num_coords):\n        loss += self.compute_loss_1d(filtered_data[i], real_data[:, i]) * weights[i]\n\n    return loss\n</code></pre>"},{"location":"losses/#black_it.loss_functions.base.BaseLoss.compute_loss_1d","title":"<code>compute_loss_1d(self, sim_data_ensemble, real_data)</code>","text":"<p>Return the loss between a specific coordinate of two time series.</p> <p>Concrete classes have to override this method in order to implement new loss functions.</p> <p>Parameters:</p> Name Type Description Default <code>sim_data_ensemble</code> <code>NDArray[np.float64]</code> <p>an ensemble of simulated 1D series, shape (ensemble_size, N, 1)</p> required <code>real_data</code> <code>NDArray[np.float64]</code> <p>the real data, shape (N, 1)</p> required <p>Returns:</p> Type Description <code>float</code> <p>the computed loss over the specific coordinate</p> Source code in <code>black_it/loss_functions/base.py</code> <pre><code>@abstractmethod\ndef compute_loss_1d(\n    self,\n    sim_data_ensemble: NDArray[np.float64],\n    real_data: NDArray[np.float64],\n) -&gt; float:\n    \"\"\"Return the loss between a specific coordinate of two time series.\n\n    Concrete classes have to override this method in order to implement new\n    loss functions.\n\n    Args:\n        sim_data_ensemble: an ensemble of simulated 1D series, shape (ensemble_size, N, 1)\n        real_data: the real data, shape (N, 1)\n\n    Returns:\n        the computed loss over the specific coordinate\n    \"\"\"\n</code></pre>"},{"location":"losses/#black_it.loss_functions.fourier.FrequencyFilter","title":"<code>FrequencyFilter</code>","text":"<p>A filter that receives the signal in the frequency domain and returns its filtered version. Used by FourierLoss constructor.</p> <p>In this version, the filter supports a single parameter: no multi-band filtering is supported yet.</p>"},{"location":"losses/#black_it.loss_functions.fourier.FourierLoss","title":"<code> FourierLoss            (BaseLoss)         </code>","text":"<p>Class for the Fourier loss.</p> Source code in <code>black_it/loss_functions/fourier.py</code> <pre><code>class FourierLoss(BaseLoss):\n    \"\"\"Class for the Fourier loss.\"\"\"\n\n    def __init__(\n        self,\n        frequency_filter: FrequencyFilter = gaussian_low_pass_filter,\n        f: float = 0.8,\n        coordinate_weights: NDArray | None = None,\n        coordinate_filters: list[Callable | None] | None = None,\n    ) -&gt; None:\n        \"\"\"Loss computed using a distance in the Fourier space of the time series.\n\n        This loss is equivalent to the Euclidean loss computed on the time\n        series after a Fourier-filter.\n        The parameter f controls the fraction of frequencies that are kept in\n        the Fourier series.\n\n        Args:\n            frequency_filter: the function used to filter the fourier\n                frequencies before the distance is computed.\n            f: fraction of fourier components to keep when computing the\n                distance between the time series. This parameter will be passed\n                to frequency_filter.\n            coordinate_weights: relative weights of the losses computed over\n                different time series coordinates.\n            coordinate_filters: filters/transformations to be applied to each simulated series before\n                the loss computation.\n        \"\"\"\n        _assert(\n            0 &lt; f &lt;= 1,\n            \"'f' must be in the interval (0.0, 1.0]\",\n        )\n        self.frequency_filter = frequency_filter\n        self.f = f\n        super().__init__(coordinate_weights, coordinate_filters)\n\n    def compute_loss_1d(\n        self,\n        sim_data_ensemble: NDArray[np.float64],\n        real_data: NDArray[np.float64],\n    ) -&gt; float:\n        \"\"\"Compute Euclidean distance between the Fourier transform of the two time series.\n\n        Args:\n            sim_data_ensemble: the first operand\n            real_data: the second operand\n\n        Returns:\n            The computed loss over the specific coordinate.\n        \"\"\"\n        f_real_data = np.fft.rfft(real_data, axis=0)\n        ts_length = f_real_data.shape[0]\n        f_real_data = self.frequency_filter(f_real_data, self.f)\n        # computer mean fft transform of simulated ensemble\n        f_sim_data = []\n        for s in sim_data_ensemble:\n            f_sim_data_ = np.fft.rfft(s, axis=0)\n            f_sim_data_ = self.frequency_filter(f_sim_data_, self.f)\n            f_sim_data.append(f_sim_data_)\n\n        f_sim_data = np.array(f_sim_data).mean(0)\n\n        return np.sqrt(np.sum((abs(f_sim_data - f_real_data)) ** 2) / ts_length)\n</code></pre>"},{"location":"losses/#black_it.loss_functions.fourier.FourierLoss.__init__","title":"<code>__init__(self, frequency_filter=&lt;function gaussian_low_pass_filter at 0x7f646b8e1280&gt;, f=0.8, coordinate_weights=None, coordinate_filters=None)</code>  <code>special</code>","text":"<p>Loss computed using a distance in the Fourier space of the time series.</p> <p>This loss is equivalent to the Euclidean loss computed on the time series after a Fourier-filter. The parameter f controls the fraction of frequencies that are kept in the Fourier series.</p> <p>Parameters:</p> Name Type Description Default <code>frequency_filter</code> <code>FrequencyFilter</code> <p>the function used to filter the fourier frequencies before the distance is computed.</p> <code>&lt;function gaussian_low_pass_filter at 0x7f646b8e1280&gt;</code> <code>f</code> <code>float</code> <p>fraction of fourier components to keep when computing the distance between the time series. This parameter will be passed to frequency_filter.</p> <code>0.8</code> <code>coordinate_weights</code> <code>NDArray | None</code> <p>relative weights of the losses computed over different time series coordinates.</p> <code>None</code> <code>coordinate_filters</code> <code>list[Callable | None] | None</code> <p>filters/transformations to be applied to each simulated series before the loss computation.</p> <code>None</code> Source code in <code>black_it/loss_functions/fourier.py</code> <pre><code>def __init__(\n    self,\n    frequency_filter: FrequencyFilter = gaussian_low_pass_filter,\n    f: float = 0.8,\n    coordinate_weights: NDArray | None = None,\n    coordinate_filters: list[Callable | None] | None = None,\n) -&gt; None:\n    \"\"\"Loss computed using a distance in the Fourier space of the time series.\n\n    This loss is equivalent to the Euclidean loss computed on the time\n    series after a Fourier-filter.\n    The parameter f controls the fraction of frequencies that are kept in\n    the Fourier series.\n\n    Args:\n        frequency_filter: the function used to filter the fourier\n            frequencies before the distance is computed.\n        f: fraction of fourier components to keep when computing the\n            distance between the time series. This parameter will be passed\n            to frequency_filter.\n        coordinate_weights: relative weights of the losses computed over\n            different time series coordinates.\n        coordinate_filters: filters/transformations to be applied to each simulated series before\n            the loss computation.\n    \"\"\"\n    _assert(\n        0 &lt; f &lt;= 1,\n        \"'f' must be in the interval (0.0, 1.0]\",\n    )\n    self.frequency_filter = frequency_filter\n    self.f = f\n    super().__init__(coordinate_weights, coordinate_filters)\n</code></pre>"},{"location":"losses/#black_it.loss_functions.fourier.FourierLoss.compute_loss_1d","title":"<code>compute_loss_1d(self, sim_data_ensemble, real_data)</code>","text":"<p>Compute Euclidean distance between the Fourier transform of the two time series.</p> <p>Parameters:</p> Name Type Description Default <code>sim_data_ensemble</code> <code>NDArray[np.float64]</code> <p>the first operand</p> required <code>real_data</code> <code>NDArray[np.float64]</code> <p>the second operand</p> required <p>Returns:</p> Type Description <code>float</code> <p>The computed loss over the specific coordinate.</p> Source code in <code>black_it/loss_functions/fourier.py</code> <pre><code>def compute_loss_1d(\n    self,\n    sim_data_ensemble: NDArray[np.float64],\n    real_data: NDArray[np.float64],\n) -&gt; float:\n    \"\"\"Compute Euclidean distance between the Fourier transform of the two time series.\n\n    Args:\n        sim_data_ensemble: the first operand\n        real_data: the second operand\n\n    Returns:\n        The computed loss over the specific coordinate.\n    \"\"\"\n    f_real_data = np.fft.rfft(real_data, axis=0)\n    ts_length = f_real_data.shape[0]\n    f_real_data = self.frequency_filter(f_real_data, self.f)\n    # computer mean fft transform of simulated ensemble\n    f_sim_data = []\n    for s in sim_data_ensemble:\n        f_sim_data_ = np.fft.rfft(s, axis=0)\n        f_sim_data_ = self.frequency_filter(f_sim_data_, self.f)\n        f_sim_data.append(f_sim_data_)\n\n    f_sim_data = np.array(f_sim_data).mean(0)\n\n    return np.sqrt(np.sum((abs(f_sim_data - f_real_data)) ** 2) / ts_length)\n</code></pre>"},{"location":"losses/#black_it.loss_functions.fourier.gaussian_low_pass_filter","title":"<code>gaussian_low_pass_filter(signal_frequencies, f)</code>","text":"<p>Gaussian low-pass filter.</p> <p>Parameters:</p> Name Type Description Default <code>signal_frequencies</code> <code>NDArray[np.complex128]</code> <p>the input signal transformed in the frequency domain.</p> required <code>f</code> <code>float</code> <p>the fraction of frequencies to keep, this will determine the length scale of the Gaussian filter</p> required <p>Returns:</p> Type Description <code>NDArray[np.complex128]</code> <p>the filtered frequencies</p> Source code in <code>black_it/loss_functions/fourier.py</code> <pre><code>def gaussian_low_pass_filter(\n    signal_frequencies: NDArray[np.complex128],\n    f: float,\n) -&gt; NDArray[np.complex128]:\n    \"\"\"Gaussian low-pass filter.\n\n    Args:\n        signal_frequencies: the input signal transformed in the frequency\n            domain.\n        f: the fraction of frequencies to keep, this will determine the length\n            scale of the Gaussian filter\n\n    Returns:\n        the filtered frequencies\n    \"\"\"\n    # number of low-frequency component to keep\n    sigma = np.round(f * signal_frequencies.shape[0])\n\n    # gaussian low-pass filter\n    mask = np.exp(-(np.arange(signal_frequencies.shape[0]) ** 2) / (2 * sigma**2))\n    return signal_frequencies * mask\n</code></pre>"},{"location":"losses/#black_it.loss_functions.fourier.ideal_low_pass_filter","title":"<code>ideal_low_pass_filter(signal_frequencies, f)</code>","text":"<p>Ideal low-pass filter.</p> <p>Parameters:</p> Name Type Description Default <code>signal_frequencies</code> <code>NDArray[np.complex128]</code> <p>the input signal transformed in the frequency domain.</p> required <code>f</code> <code>float</code> <p>the fraction of frequencies to keep unchanged, for f=1 the filter is just the identity.</p> required <p>Returns:</p> Type Description <code>NDArray[np.complex128]</code> <p>the filtered frequencies</p> Source code in <code>black_it/loss_functions/fourier.py</code> <pre><code>def ideal_low_pass_filter(\n    signal_frequencies: NDArray[np.complex128],\n    f: float,\n) -&gt; NDArray[np.complex128]:\n    \"\"\"Ideal low-pass filter.\n\n    Args:\n        signal_frequencies: the input signal transformed in the frequency\n            domain.\n        f: the fraction of frequencies to keep unchanged, for f=1 the filter is\n            just the identity.\n\n    Returns:\n        the filtered frequencies\n    \"\"\"\n    # number of low-frequency component to keep\n    n = int(np.round(f * signal_frequencies.shape[0]))\n\n    # ideal low-pass filter\n    mask = np.zeros(signal_frequencies.shape[0])\n    mask[:n] = 1.0\n    return signal_frequencies * mask\n</code></pre>"},{"location":"losses/#black_it.loss_functions.minkowski.MinkowskiLoss.__init__","title":"<code>__init__(self, p=2, coordinate_weights=None, coordinate_filters=None)</code>  <code>special</code>","text":"<p>Loss computed using a Minkowski distance.</p> <p>The Minkowski distance is a generalization of both the Manhattan distance (p=1) and the Euclidean distance (p=2).</p> <p>This function computes the Minkowski distance between two series.</p> <p>Note: this class is a wrapper of scipy.spatial.distance.minkowski</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>int</code> <p>The order of the norm used to compute the distance between real and simulated series</p> <code>2</code> <code>coordinate_weights</code> <code>NDArray | None</code> <p>The order of the norm used to compute the distance between real and simulated series</p> <code>None</code> <code>coordinate_filters</code> <code>list[Callable | None] | None</code> <p>filters/transformations to be applied to each simulated series before the loss computation.</p> <code>None</code> Source code in <code>black_it/loss_functions/minkowski.py</code> <pre><code>def __init__(\n    self,\n    p: int = 2,\n    coordinate_weights: NDArray | None = None,\n    coordinate_filters: list[Callable | None] | None = None,  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Loss computed using a Minkowski distance.\n\n    The [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance)\n    is a generalization of both the Manhattan distance (p=1) and the Euclidean distance (p=2).\n\n    This function computes the Minkowski distance between two series.\n\n    Note: this class is a wrapper of scipy.spatial.distance.minkowski\n\n    Args:\n        p: The order of the norm used to compute the distance between real and simulated series\n        coordinate_weights: The order of the norm used to compute the distance between real and simulated series\n        coordinate_filters: filters/transformations to be applied to each simulated series before\n            the loss computation.\n    \"\"\"\n    self.p = p\n    super().__init__(coordinate_weights)\n</code></pre>"},{"location":"losses/#black_it.loss_functions.minkowski.MinkowskiLoss.compute_loss_1d","title":"<code>compute_loss_1d(self, sim_data_ensemble, real_data)</code>","text":"<p>Call scipy.spatial.distance.minkowski() on its arguments.</p> <p>Parameters:</p> Name Type Description Default <code>sim_data_ensemble</code> <code>NDArray[np.float64]</code> <p>the first operand</p> required <code>real_data</code> <code>NDArray[np.float64]</code> <p>the second operand</p> required <p>Returns:</p> Type Description <code>float</code> <p>The computed loss over the specific coordinate.</p> Source code in <code>black_it/loss_functions/minkowski.py</code> <pre><code>def compute_loss_1d(\n    self,\n    sim_data_ensemble: NDArray[np.float64],\n    real_data: NDArray[np.float64],\n) -&gt; float:\n    \"\"\"Call scipy.spatial.distance.minkowski() on its arguments.\n\n    Args:\n        sim_data_ensemble: the first operand\n        real_data: the second operand\n\n    Returns:\n        The computed loss over the specific coordinate.\n    \"\"\"\n    # average simulated time series\n    sim_data_ensemble = sim_data_ensemble.mean(axis=0)\n\n    return minkowski(sim_data_ensemble, real_data, p=self.p)\n</code></pre>"},{"location":"losses/#black_it.loss_functions.msm.MethodOfMomentsLoss.__init__","title":"<code>__init__(self, covariance_mat='identity', coordinate_weights=None, moment_calculator=&lt;function get_mom_ts_1d at 0x7f6437766820&gt;, coordinate_filters=None, standardise_moments=False)</code>  <code>special</code>","text":"<p>Initialize the loss function based on the 'method of moments'.</p> <p>Returns the MSM objective function, i.e. the square difference between the moments of the two time series. By default the loss computes the moments using black_it.utils.time_series.get_mom_ts_1d(), which computes an 18-dimensional vector of statistics.</p> <p>You can alter the behaviour passing a custom function to moment_calculator. Please note that there is a constraint between the moment calculator and the size of the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>covariance_mat</code> <code>str | NDArray[np.float64]</code> <p>covariance matrix between the moments. 'identity' (the default) gives the identity matrix, 'inverse_variance' gives the diagonal matrix of the estimated inverse variances. Alternatively, one can specify a numerical symmetric matrix whose size must be equal to the number of elements that the moment calculator returns.</p> <code>'identity'</code> <code>coordinate_weights</code> <code>NDArray[np.float64] | None</code> <p>importance of each coordinate. By default all coordinates are treated equally.</p> <code>None</code> <code>moment_calculator</code> <code>MomentCalculator</code> <p>a function that takes a 1D time series and returns a series of moments. The default is black_it.utils.time_series.get_mom_ts_1d()</p> <code>&lt;function get_mom_ts_1d at 0x7f6437766820&gt;</code> <code>coordinate_filters</code> <code>list[Callable | None] | None</code> <p>filters/transformations to be applied to each simulated series before the loss computation.</p> <code>None</code> <code>standardise_moments</code> <code>bool</code> <p>if True all moments are divided by the absolute value of the real moments, default is False.</p> <code>False</code> Source code in <code>black_it/loss_functions/msm.py</code> <pre><code>def __init__(\n    self,\n    covariance_mat: str | NDArray[np.float64] = \"identity\",\n    coordinate_weights: NDArray[np.float64] | None = None,\n    moment_calculator: MomentCalculator = get_mom_ts_1d,\n    coordinate_filters: list[Callable | None] | None = None,\n    standardise_moments: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the loss function based on the 'method of moments'.\n\n    Returns the MSM objective function, i.e. the square difference between\n    the moments of the two time series.\n    By default the loss computes the moments using\n    black_it.utils.time_series.get_mom_ts_1d(), which computes an\n    18-dimensional vector of statistics.\n\n    You can alter the behaviour passing a custom function to\n    moment_calculator. Please note that there is a constraint between the\n    moment calculator and the size of the covariance matrix.\n\n    Args:\n        covariance_mat: covariance matrix between the moments.\n            'identity' (the default) gives the identity matrix, 'inverse_variance' gives the diagonal\n            matrix of the estimated inverse variances. Alternatively, one can specify a numerical symmetric matrix\n            whose size must be equal to the number of elements that the moment calculator returns.\n        coordinate_weights: importance of each coordinate. By default all\n            coordinates are treated equally.\n        moment_calculator: a function that takes a 1D time series and\n            returns a series of moments. The default is\n            black_it.utils.time_series.get_mom_ts_1d()\n        coordinate_filters: filters/transformations to be applied to each simulated series before\n            the loss computation.\n        standardise_moments: if True all moments are divided by the absolute value of the real moments,\n            default is False.\n    \"\"\"\n    MethodOfMomentsLoss._validate_covariance_and_calculator(\n        moment_calculator,\n        covariance_mat,\n    )\n\n    super().__init__(coordinate_weights, coordinate_filters)\n    self._covariance_mat = covariance_mat\n    self._moment_calculator = moment_calculator\n    self._standardise_moments = standardise_moments\n</code></pre>"},{"location":"losses/#black_it.loss_functions.msm.MethodOfMomentsLoss.compute_loss_1d","title":"<code>compute_loss_1d(self, sim_data_ensemble, real_data)</code>","text":"<p>Compute the loss based on the 'method of moments'.</p> <p>Returns the MSM objective function, i.e. the square difference between the moments of the two time series.</p> <p>Parameters:</p> Name Type Description Default <code>sim_data_ensemble</code> <code>NDArray[np.float64]</code> <p>the first operand</p> required <code>real_data</code> <code>NDArray[np.float64]</code> <p>the second operand</p> required <p>Returns:</p> Type Description <code>float</code> <p>the MSM loss over a specific coordinate.</p> Source code in <code>black_it/loss_functions/msm.py</code> <pre><code>def compute_loss_1d(\n    self,\n    sim_data_ensemble: NDArray[np.float64],\n    real_data: NDArray[np.float64],\n) -&gt; float:\n    \"\"\"Compute the loss based on the 'method of moments'.\n\n    Returns the MSM objective function, i.e. the square difference between the moments of the two time series.\n\n    Args:\n        sim_data_ensemble: the first operand\n        real_data: the second operand\n\n    Returns:\n        the MSM loss over a specific coordinate.\n    \"\"\"\n    # compute the moments for the simulated ensemble\n    ensemble_sim_mom_1d = np.array(\n        [self._moment_calculator(s) for s in sim_data_ensemble],\n    )\n\n    # compute moments of the real time series\n    real_mom_1d = self._moment_calculator(real_data)\n\n    # write moments in relative terms is needed\n    if self._standardise_moments:\n        ensemble_sim_mom_1d = ensemble_sim_mom_1d / (abs(real_mom_1d)[None, :])\n        real_mom_1d = real_mom_1d / abs(real_mom_1d)\n\n    # mean simulated moments\n    sim_mom_1d = np.mean(ensemble_sim_mom_1d, axis=0)\n\n    g = real_mom_1d - sim_mom_1d\n\n    if isinstance(self._covariance_mat, str) and self._covariance_mat == _CovarianceMatrixType.IDENTITY.value:\n        return g.dot(g)\n    if (\n        isinstance(self._covariance_mat, str)\n        and self._covariance_mat == _CovarianceMatrixType.INVERSE_VARIANCE.value\n    ):\n        W = np.diag(  # noqa: N806\n            1.0 / np.mean((real_mom_1d[None, :] - ensemble_sim_mom_1d) ** 2, axis=0),\n        )\n    else:\n        self._covariance_mat = cast(\"NDArray[np.float64]\", self._covariance_mat)\n        W = self._covariance_mat  # noqa: N806\n    try:\n        loss_1d = g.dot(W).dot(g)\n    except ValueError as e:\n        covariance_size = W.shape[0]\n        moments_size = g.shape[0]\n\n        if covariance_size == moments_size:\n            # this value error is not due to a mismatch between the\n            # covariance matrix size and the number moments. Let's raise the\n            # original error.\n            raise\n\n        msg = (\n            f\"The size of the covariance matrix ({covariance_size}) \"\n            f\"and the number of moments ({moments_size}) should be identical\"\n        )\n        raise ValueError(\n            msg,\n        ) from e\n\n    return loss_1d\n</code></pre>"},{"location":"losses/#black_it.loss_functions.gsl_div.GslDivLoss.__init__","title":"<code>__init__(self, nb_values=None, nb_word_lengths=None, coordinate_weights=None, coordinate_filters=None)</code>  <code>special</code>","text":"<p>Initialize the GSL-div loss object.</p> <p>Parameters:</p> Name Type Description Default <code>nb_values</code> <code>int | None</code> <p>number of values the digitised series can take</p> <code>None</code> <code>nb_word_lengths</code> <code>int | None</code> <p>the number of word length to consider</p> <code>None</code> <code>coordinate_weights</code> <code>NDArray | None</code> <p>the weights of the loss coordinates</p> <code>None</code> <code>coordinate_filters</code> <code>list[Callable | None] | None</code> <p>filters/transformations to be applied to each simulated series before the loss computation.</p> <code>None</code> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>def __init__(\n    self,\n    nb_values: int | None = None,\n    nb_word_lengths: int | None = None,\n    coordinate_weights: NDArray | None = None,\n    coordinate_filters: list[Callable | None] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the GSL-div loss object.\n\n    Args:\n        nb_values: number of values the digitised series can take\n        nb_word_lengths: the number of word length to consider\n        coordinate_weights: the weights of the loss coordinates\n        coordinate_filters: filters/transformations to be applied to each simulated series before\n            the loss computation.\n    \"\"\"\n    super().__init__(coordinate_weights, coordinate_filters)\n    self.nb_values = nb_values\n    self.nb_word_lengths = nb_word_lengths\n</code></pre>"},{"location":"losses/#black_it.loss_functions.gsl_div.GslDivLoss.compute_loss_1d","title":"<code>compute_loss_1d(self, sim_data_ensemble, real_data)</code>","text":"<p>Return the GSL-div measure.</p> <p>From (Lamperti, 2017):</p> <p>The information loss about the behaviour of the stochastic process   due to the symbolization becomes smaller and smaller as b increases.   On the other side, low values of b would likely wash away   processes' noise the modeller might not be interested in.</p> <p>Parameters:</p> Name Type Description Default <code>sim_data_ensemble</code> <code>NDArray[np.float64]</code> <p>the ensemble of simulated data</p> required <code>real_data</code> <code>NDArray[np.float64]</code> <p>the real data</p> required <p>Returns:</p> Type Description <code>float</code> <p>the GSL loss</p> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>def compute_loss_1d(\n    self,\n    sim_data_ensemble: NDArray[np.float64],\n    real_data: NDArray[np.float64],\n) -&gt; float:\n    \"\"\"Return the GSL-div measure.\n\n    From (Lamperti, 2017):\n\n    &gt; The information loss about the behaviour of the stochastic process\n      due to the symbolization becomes smaller and smaller as b increases.\n      On the other side, low values of b would likely wash away\n      processes' noise the modeller might not be interested in.\n\n    Args:\n        sim_data_ensemble: the ensemble of simulated data\n        real_data: the real data\n\n    Returns:\n        the GSL loss\n    \"\"\"\n    ts_length = len(real_data)\n    ensemble_size = sim_data_ensemble.shape[0]\n\n    nb_values = int((ts_length - 1) / 2.0) if self.nb_values is None else self.nb_values\n\n    nb_word_lengths = int((ts_length - 1) / 2.0) if self.nb_word_lengths is None else self.nb_word_lengths\n\n    # discretize real time series\n    obs_xd = self.discretize(\n        real_data,\n        nb_values,\n        np.min(real_data),\n        np.max(real_data),\n    )\n\n    gsl_loss = 0.0\n\n    # average loss over the ensemble\n    for sim_data in sim_data_ensemble:\n        # discretize simulated series\n        sim_xd = self.discretize(\n            sim_data,\n            nb_values,\n            np.min(sim_data),\n            np.max(sim_data),\n        )\n\n        loss = self.gsl_div_1d_1_sample(\n            sim_xd,\n            obs_xd,\n            nb_word_lengths,\n            nb_values,\n            ts_length,\n        )\n\n        gsl_loss += loss\n\n    return gsl_loss / ensemble_size\n</code></pre>"},{"location":"losses/#black_it.loss_functions.gsl_div.GslDivLoss.discretize","title":"<code>discretize(time_series, nb_values, start_index, stop_index)</code>  <code>staticmethod</code>","text":"<p>Discretize the TS in 'nb_values' finite states.</p> <p>GslDivLoss.discretize( ...     [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ...     nb_values=3, ...     start_index=1, ...     stop_index=10 ... ).tolist() [1, 1, 1, 2, 2, 2, 2, 3, 3, 3]</p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>NDArray[np.float64]</code> <p>any univariate time series</p> required <code>nb_values</code> <code>int</code> <p>int, number of values the digitised series can take. It must be greater than 0.</p> required <code>start_index</code> <code>np.float64 | float</code> <p>the starting point</p> required <code>stop_index</code> <code>np.float64 | float</code> <p>the stopping point</p> required <p>Returns:</p> Type Description <code>NDArray[np.int64]</code> <p>the discretised time series</p> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>@staticmethod\ndef discretize(\n    time_series: NDArray[np.float64],\n    nb_values: int,\n    start_index: np.float64 | float,\n    stop_index: np.float64 | float,\n) -&gt; NDArray[np.int64]:\n    \"\"\"Discretize the TS in 'nb_values' finite states.\n\n    &gt;&gt;&gt; GslDivLoss.discretize(\n    ...     [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    ...     nb_values=3,\n    ...     start_index=1,\n    ...     stop_index=10\n    ... ).tolist()\n    [1, 1, 1, 2, 2, 2, 2, 3, 3, 3]\n\n    Args:\n        time_series: any univariate time series\n        nb_values: int, number of values the digitised series can take.\n            It must be greater than 0.\n        start_index: the starting point\n        stop_index: the stopping point\n\n    Returns:\n        the discretised time series\n    \"\"\"\n    start_index = np.float64(start_index)\n    stop_index = np.float64(stop_index)\n    linspace = np.linspace(start_index - EPS, stop_index + EPS, nb_values + 1)\n\n    return np.searchsorted(linspace, time_series, side=\"left\")\n</code></pre>"},{"location":"losses/#black_it.loss_functions.gsl_div.GslDivLoss.get_sh_entr","title":"<code>get_sh_entr(probs, log_base)</code>  <code>staticmethod</code>","text":"<p>Return the Shannon entropy given an array of probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>NDArray[np.float64]</code> <p>an array of probabilities describing the discrete probability distribution</p> required <code>log_base</code> <code>float</code> <p>the entropy logarithm base.</p> required <p>Returns:</p> Type Description <code>float</code> <p>the entropy of the discrete probability distribution</p> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>@staticmethod\ndef get_sh_entr(probs: NDArray[np.float64], log_base: float) -&gt; float:\n    \"\"\"Return the Shannon entropy given an array of probabilities.\n\n    Args:\n        probs: an array of probabilities describing the discrete probability distribution\n        log_base: the entropy logarithm base.\n\n    Returns:\n        the entropy of the discrete probability distribution\n    \"\"\"\n    log = np.log(probs) / np.log(log_base)\n    return -np.sum(np.multiply(probs, log))\n</code></pre>"},{"location":"losses/#black_it.loss_functions.gsl_div.GslDivLoss.get_words","title":"<code>get_words(time_series, length)</code>  <code>staticmethod</code>","text":"<p>Return an overlapping array of words (int32) of 'length' given a discretised vector.</p> <p>GslDivLoss.get_words(np.asarray([1, 2, 2, 2]), 2) array([12, 22, 22])</p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>NDArray[np.float64]</code> <p>any univariate discretised time series</p> required <code>length</code> <code>int</code> <p>int, len of words to be returned. Must be such that (len(time_series) + 1 - length) is positive.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>the time series of overlapping words</p> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>@staticmethod\ndef get_words(time_series: NDArray[np.float64], length: int) -&gt; NDArray:\n    \"\"\"Return an overlapping array of words (int32) of 'length' given a discretised vector.\n\n    &gt;&gt;&gt; GslDivLoss.get_words(np.asarray([1, 2, 2, 2]), 2)\n    array([12, 22, 22])\n\n    Args:\n        time_series: any univariate discretised time series\n        length: int, len of words to be returned. Must be such that\n            (len(time_series) + 1 - length) is positive.\n\n    Returns:\n        the time series of overlapping words\n\n    \"\"\"\n    tswlen = len(time_series) + 1 - length\n    _assert(\n        tswlen &gt;= 0,\n        \"the chosen word length is too high\",\n        exception_class=ValueError,\n    )\n    tsw: NDArray[np.float64] = np.zeros(shape=(tswlen,), dtype=np.int32)\n\n    for i in range(length):\n        k = 10 ** (length - i - 1)\n        tsw = tsw + time_series[i : tswlen + i] * k\n\n    return tsw\n</code></pre>"},{"location":"losses/#black_it.loss_functions.gsl_div.GslDivLoss.get_words_est_prob","title":"<code>get_words_est_prob(time_series)</code>  <code>staticmethod</code>","text":"<p>Return an array of estimated probabilities given an array of words (int32).</p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>NDArray[np.float64]</code> <p>any univariate array of words</p> required <p>Returns:</p> Type Description <code>NDArray[np.float64]</code> <p>estimate of probabilities</p> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>@staticmethod\ndef get_words_est_prob(time_series: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n    \"\"\"Return an array of estimated probabilities given an array of words (int32).\n\n    Args:\n        time_series: any univariate array of words\n\n    Returns:\n        estimate of probabilities\n    \"\"\"\n    _, count = np.unique(time_series, return_counts=True)\n    return np.divide(count, np.sum(count))\n</code></pre>"},{"location":"losses/#black_it.loss_functions.gsl_div.GslDivLoss.gsl_div_1d_1_sample","title":"<code>gsl_div_1d_1_sample(self, sim_xd, obs_xd, nb_word_lengths, nb_values, ts_length)</code>","text":"<p>Compute the GSL-div for a single realisation of the simulated data.</p> <p>Parameters:</p> Name Type Description Default <code>sim_xd</code> <code>NDArray</code> <p>discretised simulated series</p> required <code>obs_xd</code> <code>NDArray</code> <p>discretised real series</p> required <code>nb_word_lengths</code> <code>int</code> <p>the number of word length to consider</p> required <code>nb_values</code> <code>int</code> <p>number of values the digitised series can take</p> required <code>ts_length</code> <code>int</code> <p>the length of real and simulated series</p> required <p>Returns:</p> Type Description <code>float</code> <p>the computed loss</p> Source code in <code>black_it/loss_functions/gsl_div.py</code> <pre><code>def gsl_div_1d_1_sample(\n    self,\n    sim_xd: NDArray,\n    obs_xd: NDArray,\n    nb_word_lengths: int,\n    nb_values: int,\n    ts_length: int,\n) -&gt; float:\n    \"\"\"Compute the GSL-div for a single realisation of the simulated data.\n\n    Args:\n        sim_xd: discretised simulated series\n        obs_xd: discretised real series\n        nb_word_lengths: the number of word length to consider\n        nb_values: number of values the digitised series can take\n        ts_length: the length of real and simulated series\n\n    Returns:\n        the computed loss\n    \"\"\"\n    # outcome measure\n    gsl_div = 0.0\n\n    # weight\n    weight = 0.0\n\n    # for any word len:\n    for word_length in range(1, nb_word_lengths + 1):\n        sim_xw = self.get_words(sim_xd, word_length)\n        obs_xw = self.get_words(obs_xd, word_length)\n        m_xw = np.concatenate((sim_xw, obs_xw))\n        sim_xp = self.get_words_est_prob(sim_xw)\n        m_xp = self.get_words_est_prob(m_xw)\n        base = float(nb_values**word_length)\n        sim_entr = self.get_sh_entr(sim_xp, base)\n        m_entr = self.get_sh_entr(m_xp, base)\n\n        # update weight\n        weight = weight + 2 / (nb_word_lengths * (nb_word_lengths + 1))\n\n        # correction\n        corr = ((len(m_xp) - 1) - (len(sim_xp) - 1)) / (2 * ts_length)\n\n        # add to measure\n        gsl_divl = 2 * m_entr - sim_entr + corr\n        gsl_div = gsl_div + weight * gsl_divl\n\n    # end of cycle, return\n    return gsl_div\n</code></pre>"},{"location":"overview_of_plotting_functions/","title":"Plotting tutorial","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <p>Let us assume we have performed a calibration and we have stored our results in a given folder. In this tutotial we will go through the black-it tools to quickly analyse the calibration results.</p> <pre><code>saving_folder = \"saving_folder\"\n</code></pre> <pre><code>from black_it.plot.plot_results import plot_sampling\n</code></pre> <pre><code>plot_sampling(saving_folder)\n</code></pre> <pre><code>from black_it.plot.plot_results import plot_sampling_interact\n</code></pre> <pre><code>plot_sampling_interact(saving_folder)\n</code></pre> <pre>\n<code>interactive(children=(Dropdown(description='batch_nums', options={'from 0 to 3': [0, 1, 2, 3], 'from 4 to 6': \u2026</code>\n</pre> <pre><code>from black_it.plot.plot_results import plot_losses\n</code></pre> <pre><code>plot_losses(saving_folder)\n</code></pre> <pre><code>from black_it.plot.plot_results import plot_losses_interact\n</code></pre> <pre><code>plot_losses_interact(saving_folder)\n</code></pre> <pre>\n<code>interactive(children=(Dropdown(description='method_num', options={'RandomUniformSampler': 0, 'RSequenceSampler\u2026</code>\n</pre> <pre><code>from black_it.plot.plot_results import plot_convergence\n</code></pre> <pre><code>plot_convergence(saving_folder)\n</code></pre>"},{"location":"overview_of_plotting_functions/#overview-of-the-plotting-tools-of-black-it","title":"Overview of the plotting tools of black-it","text":""},{"location":"overview_of_plotting_functions/#analyse-the-action-of-the-different-samplers","title":"Analyse the action of the different samplers","text":""},{"location":"overview_of_plotting_functions/#analyse-the-loss-landscape-explored","title":"Analyse the loss landscape explored","text":""},{"location":"overview_of_plotting_functions/#analyse-the-calibration-convergence","title":"Analyse the calibration convergence","text":""},{"location":"overview_of_the_different_samplers/","title":"Different samplers","text":"<pre><code>import numpy as np\n\nfrom black_it.search_space import SearchSpace\nfrom black_it.samplers.random_uniform import RandomUniformSampler\nfrom black_it.samplers.halton import HaltonSampler\nfrom black_it.samplers.r_sequence import RSequenceSampler\nfrom black_it.samplers.random_forest import RandomForestSampler\nfrom black_it.samplers.gaussian_process import GaussianProcessSampler\nfrom black_it.samplers.xgboost import XGBoostSampler\nfrom black_it.samplers.best_batch import BestBatchSampler\nfrom black_it.samplers.particle_swarm import ParticleSwarmSampler\n\nimport matplotlib.pyplot as plt\n\n%load_ext autoreload\n%autoreload 2\n</code></pre> <p>In this notebook we will illustrate the inner working of the samplers of the package. For ease of visualization we will focus exclusively on a simple two-dimensional parameter space. </p> <pre><code># define a 2-dimensional grid of possible parameter values between 0 and 1.\n\nparam_grid = SearchSpace(\n    parameters_bounds=np.array([[0, 1], [0, 1]]).T,\n    parameters_precision=[0.01, 0.01],\n    verbose=False,\n)\n</code></pre> <p>The simplest sampler is a sampler that proposes completely random parameters.</p> <pre><code>sampler = RandomUniformSampler(batch_size=50, random_state=0)\nnew_params = sampler.sample(param_grid, np.zeros((0, 2)), np.zeros((0, 2)))\n</code></pre> <pre><code>plt.figure(figsize=(5, 5))\nfor i in range(5):\n    plt.scatter(\n        new_params[i * 10 : (i + 1) * 10, 0],\n        new_params[i * 10 : (i + 1) * 10, 1],\n        label=\"iter. \" + str(i * 10) + \" - \" + str((i + 1) * 10),\n    )\n\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend();\n</code></pre> <p>As you can see from the graph above, random uniform sampling is not very effective at covering the parameter space uniformly (in spite of the name). To address this problem one can use a series of samplers designed to provide low discrepancy. These are discussed in the folling.</p> <pre><code>sampler = HaltonSampler(batch_size=50, random_state=0)\nnew_params = sampler.sample(param_grid, np.zeros((0, 2)), np.zeros((0, 2)))\n</code></pre> <pre><code>plt.figure(figsize=(5, 5))\nfor i in range(5):\n    plt.scatter(\n        new_params[i * 10 : (i + 1) * 10, 0],\n        new_params[i * 10 : (i + 1) * 10, 1],\n        label=\"iter. \" + str(i * 10) + \" - \" + str((i + 1) * 10),\n    )\n\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend();\n</code></pre> <pre><code>sampler = RSequenceSampler(batch_size=50, random_state=0)\nnew_params = sampler.sample(param_grid, np.zeros((0, 2)), np.zeros((0, 2)))\n</code></pre> <pre><code>plt.figure(figsize=(5, 5))\nfor i in range(5):\n    plt.scatter(\n        new_params[i * 10 : (i + 1) * 10, 0],\n        new_params[i * 10 : (i + 1) * 10, 1],\n        label=\"iter. \" + str(i * 10) + \" - \" + str((i + 1) * 10),\n    )\n\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend();\n</code></pre> <p>Certain samplers exploit the information collected during previous calibration steps to adaptively propose optimal sampling regions to explore. These adaptive samplers are discussed in the following.</p> <pre><code># In the folliwing we will assume that the value of the loss function\n# is known on a grid of points in parameter space\n\n\ndef target_loss(x, y):\n    return 5 * (x**2 + y**2)\n\n\nxs = np.linspace(0, 1, 6)\nys = np.linspace(0, 1, 6)\nxys = []\nlosses = []\n\nfor x in xs:\n    for y in ys:\n        # a small noise is needed to avoid\n        # unrealistically simmetric sampling\n        x = x + np.random.normal(0, 1e-2)\n        y = y + np.random.normal(0, 1e-2)\n        xys.append([x, y])\n        losses.append(target_loss(x, y))\n\nxys = np.array(xys)\nlosses = np.array(losses)\n</code></pre> <pre><code>plt.title(\"loss function values\")\nplt.scatter(xys[:, 0], xys[:, 1], c=losses)\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.colorbar();\n</code></pre> <pre><code>sampler = RandomForestSampler(batch_size=16, random_state=0)\nnew_params = sampler.sample(param_grid, xys, losses)\n</code></pre> <pre><code>plt.figure(figsize=(5, 5))\n\nplt.scatter(xys[:, 0], xys[:, 1], c=\"0.8\", label=\"previously sampled\")\n\nplt.scatter(new_params[:, 0], new_params[:, 1], label=\"random forest sampling\")\n\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend();\n</code></pre> <pre><code>sampler = XGBoostSampler(\n    batch_size=16, random_state=0, colsample_bytree=1.0, learning_rate=0.1, max_depth=5, alpha=1.0, n_estimators=10\n)\nnew_params = sampler.sample(param_grid, xys, losses)\n</code></pre> <pre><code>plt.figure(figsize=(5, 5))\n\nplt.scatter(xys[:, 0], xys[:, 1], c=\"0.8\", label=\"previously sampled\")\n\nplt.scatter(new_params[:, 0], new_params[:, 1], label=\"xgboost sampling\")\n\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend();\n</code></pre> <pre><code>sampler = GaussianProcessSampler(batch_size=8, random_state=0, acquisition=\"mean\")\nnew_params_mean_acquisition = sampler.sample(param_grid, xys, losses)\n</code></pre> <pre><code>sampler = GaussianProcessSampler(batch_size=8, random_state=0, acquisition=\"expected_improvement\", jitter=8.0)\nnew_params_EI_acquisition = sampler.sample(param_grid, xys, losses)\n</code></pre> <pre><code># note that the expected_improvement acquisition function automatically\n# tries to strike a balance between \"exploration\" and \"exploitation\"\n\nplt.figure(figsize=(5, 5))\n\nplt.scatter(xys[:, 0], xys[:, 1], c=\"0.8\", label=\"previously sampled\")\n\nplt.scatter(\n    new_params_mean_acquisition[:, 0],\n    new_params_mean_acquisition[:, 1],\n    label=\"mean acq. func.\",\n)\n\nplt.scatter(\n    new_params_EI_acquisition[:, 0],\n    new_params_EI_acquisition[:, 1],\n    label=\"EI acq. func.\",\n)\n\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x16b385520&gt;</code>\n</pre> <pre><code>sampler = BestBatchSampler(batch_size=16, random_state=0)\nnew_params = sampler.sample(param_grid, xys, losses)\n</code></pre> <pre><code>plt.figure(figsize=(5, 5))\nplt.scatter(xys[:, 0], xys[:, 1], c=\"0.8\", label=\"previously sampled\")\n\n\nplt.scatter(new_params[:, 0], new_params[:, 1], label=\"best batch sampling\")\n\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x16b422e50&gt;</code>\n</pre> <pre><code>from collections import defaultdict\n\n# short PS dynamics with 4 particles for 10 timesteps\nbatch_size = 4\nnb_iterations = 10\nsampler = ParticleSwarmSampler(batch_size=batch_size, random_state=0, global_minimum_across_samplers=False)\n\n# here we start with no evaluated losses\npoints_ = np.zeros((0, 2))\nlosses_ = np.array([])\n\n\nparticle_trajectory = defaultdict(lambda: [])\n\nfor i in range(nb_iterations):\n    current_positions = sampler.sample(param_grid, points_, losses_)\n    current_losses = []\n    for particle_id, point in enumerate(current_positions):\n        outputs = []\n        loss = target_loss(point[0], point[1])\n        current_losses.append(loss)\n        particle_trajectory[particle_id].append(point)\n\n    points_ = np.concatenate([points_, current_positions])\n    losses_ = np.concatenate([losses_, current_losses])\n</code></pre> <pre><code>import matplotlib.patches as mpatches\n\nfig = plt.figure()\naxes = plt.axes()\n\n\ndef plot_trajectory(trajectory, axes, marker=\"o\", label=None, color=None):\n    trajectory = np.array(trajectory)\n    path_obj = axes.scatter(\n        trajectory[:, 0], trajectory[:, 1], label=label, edgecolors=\"black\", marker=marker, color=color\n    )\n    color = path_obj.get_facecolor()[0] if color is None else color\n    for step_id in range(0, len(trajectory) - 1):\n        p1, p2 = trajectory[step_id], trajectory[step_id + 1]\n        arrow = mpatches.FancyArrowPatch(p1, p2, facecolor=color, mutation_scale=10, edgecolor=\"black\")\n        axes.add_patch(arrow)\n\n\n# plot trajectories\nfor particle_id, trajectory_i in particle_trajectory.items():\n    plot_trajectory(trajectory_i, axes, label=f\"particle {particle_id}\")\n\n\nplt.xlim(-0.1, 1.1)\nplt.ylim(-0.1, 1.1)\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend();\n</code></pre> <pre><code>\n</code></pre>"},{"location":"overview_of_the_different_samplers/#overvieview-of-the-different-samplers-of-the-package","title":"Overvieview of the different samplers of the package","text":""},{"location":"overview_of_the_different_samplers/#random-uniform-trivial-sampler","title":"Random Uniform (Trivial) Sampler","text":""},{"location":"overview_of_the_different_samplers/#low-discrepancy-samplers","title":"Low discrepancy samplers","text":""},{"location":"overview_of_the_different_samplers/#halton-sampler","title":"Halton Sampler","text":""},{"location":"overview_of_the_different_samplers/#r-sequence-sampler","title":"R-Sequence Sampler","text":""},{"location":"overview_of_the_different_samplers/#adaptive-samplers","title":"Adaptive samplers","text":""},{"location":"overview_of_the_different_samplers/#random-forest-sampler","title":"Random Forest Sampler","text":""},{"location":"overview_of_the_different_samplers/#xgboost-sampler","title":"XGBoost Sampler","text":""},{"location":"overview_of_the_different_samplers/#gaussian-process-sampler-bayesian-optimisation","title":"Gaussian Process Sampler (\"Bayesian Optimisation\")","text":""},{"location":"overview_of_the_different_samplers/#best-batch-genetic-sampler","title":"Best Batch (\"Genetic\") Sampler","text":""},{"location":"overview_of_the_different_samplers/#particle-swarm-sampler","title":"Particle Swarm Sampler","text":""},{"location":"particle_swarm_sampler/","title":"Particle swarm sampler","text":"<p>In this notebook we will illustrate the functioning of the Particle Swarm sampler by using it to recover the mean and variance of a Normal distribution from a time series.</p> <pre><code>import numpy as np\nfrom black_it.loss_functions.msm import MethodOfMomentsLoss\n\nfrom black_it.search_space import SearchSpace\n\nfrom black_it.samplers.particle_swarm import ParticleSwarmSampler\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom numpy.typing import NDArray\nfrom typing import Sequence\nfrom numpy.random import default_rng\n</code></pre> <pre><code># target time series\ndef NormalMV(theta: Sequence[float], N: int, seed: int) -&amp;gt; NDArray[np.float64]:\n    \"\"\"Normal samples with adjustable mean and variance.\"\"\"\n    rng = default_rng(seed)\n    y = rng.normal(theta[0], theta[1], N)\n    return np.atleast_2d(y).T\n\n\ntrue_params = [0.5, 0.5]  # in general these are not known!\nreal_data = NormalMV(true_params, 1000, 0)\nplt.plot(real_data[:, 0])\nplt.xlabel(\"time step\")\nplt.ylabel(\"value\")\n</code></pre> <pre>\n<code>Text(0, 0.5, 'value')</code>\n</pre> <pre><code># define a 2-dimensional grid of possible parameter values between 0 and 1.\nparam_grid = SearchSpace(\n    parameters_bounds=np.array([[0, 1], [0, 1]]).T,\n    parameters_precision=[0.01, 0.01],\n    verbose=False,\n)\n\nloss_function = MethodOfMomentsLoss()\n</code></pre> <pre><code># run 200 iterations of the particle swarm sampler with 5 particles\nfrom collections import defaultdict\n\nseed = 0\nnb_iterations = 200\nnb_ensemble = 1\nN = 1000\nbatch_size = 5\nsampler = ParticleSwarmSampler(batch_size=batch_size, random_state=seed, c1=0.2, c2=0.5, inertia=0.8)\n\npoints = np.zeros((0, 2))\nlosses = np.array([])\n\nhistory_of_mins = []\nparticle_trajectory = defaultdict(lambda: [])\n\nfor i in range(nb_iterations):\n    if i % 10 == 0:\n        print(\"Iteration \", i)\n    current_positions = sampler.sample(param_grid, points, losses)\n    current_losses = []\n    for particle_id, point in enumerate(current_positions):\n        outputs = []\n        for ensemble in range(nb_ensemble):\n            outputs.append(NormalMV(point, N, seed))\n            seed += 1\n        outputs = np.array(outputs).reshape((nb_ensemble, -1, 1))\n        loss = loss_function.compute_loss(outputs, real_data)\n        current_losses.append(loss)\n        particle_trajectory[particle_id].append(point)\n    points = np.concatenate([points, current_positions])\n    losses = np.concatenate([losses, current_losses])\n\ncurrent_global_min_particle_id = None\ncurrent_global_min_loss = np.inf\nfor batch_id in range(nb_iterations):\n    batch_points = points[batch_id * batch_size : (batch_id + 1) * batch_size]\n    batch_losses = losses[batch_id * batch_size : (batch_id + 1) * batch_size]\n    new_current_global_min_loss = np.min(batch_losses)\n    new_current_global_min_particle_id = np.argmin(batch_losses)\n    if new_current_global_min_loss &amp;lt; current_global_min_loss:\n        current_global_min_particle_id = new_current_global_min_particle_id\n        current_global_min_loss = new_current_global_min_loss\n        history_of_mins.append(batch_points[current_global_min_particle_id])\n</code></pre> <pre>\n<code>Iteration  0\n</code>\n</pre> <pre>\n<code>/Users/aldoglielmo/DRTA_code/black-it/black_it/utils/time_series.py:54: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n  s, k = skew(time_series), kurtosis(time_series)\n/Users/aldoglielmo/miniconda3/envs/py39_2/lib/python3.9/site-packages/statsmodels/tsa/stattools.py:681: RuntimeWarning: invalid value encountered in divide\n  acf = avf[: nlags + 1] / avf[0]\n</code>\n</pre> <pre>\n<code>Iteration  10\nIteration  20\nIteration  30\nIteration  40\nIteration  50\nIteration  60\nIteration  70\nIteration  80\nIteration  90\nIteration  100\nIteration  110\nIteration  120\nIteration  130\nIteration  140\nIteration  150\nIteration  160\nIteration  170\nIteration  180\nIteration  190\n</code>\n</pre> <pre><code># plot the trajectories of the 5 particles, check that they gradually move towards the correct target value\nfig = plt.figure(figsize=(15, 15))\naxes = plt.axes()\n\n\ndef plot_trajectory(trajectory, axes, marker=\"o\", label=None, color=None):\n    trajectory = np.array(trajectory)\n    path_obj = axes.scatter(\n        trajectory[:, 0], trajectory[:, 1], label=label, edgecolors=\"black\", marker=marker, color=color\n    )\n    color = path_obj.get_facecolor()[0] if color is None else color\n    for step_id in range(0, len(trajectory) - 1):\n        p1, p2 = trajectory[step_id], trajectory[step_id + 1]\n        arrow = mpatches.FancyArrowPatch(p1, p2, facecolor=color, mutation_scale=10, edgecolor=\"black\")\n        axes.add_patch(arrow)\n\n\n# plot trajectories\nfor particle_id, trajectory_i in particle_trajectory.items():\n    plot_trajectory(trajectory_i, axes, label=f\"particle {particle_id}\")\n\n# plot the history of global minimum\nplot_trajectory(history_of_mins, axes, marker=\"X\", color=\"y\", label=\"found global minima\")\n\n# plot true parameters\nplt.scatter(\n    true_params[0], true_params[1], marker=\"x\", s=500, facecolor=\"red\", edgecolors=\"black\", label=\"true global minima\"\n)\n\n\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\nplt.legend()\n</code></pre> <pre>\n<code>/var/folders/t5/yx84b1sd52lblk0kjl01g2y00000gn/T/ipykernel_57476/3355392541.py:29: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  plt.scatter(true_params[0], true_params[1], marker=\"x\", s=500, facecolor=\"red\", edgecolors='black', label=\"true global minima\")\n</code>\n</pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x7fa24a167d30&gt;</code>\n</pre> <pre>\n<code>/Users/aldoglielmo/miniconda3/envs/py39_2/lib/python3.9/site-packages/IPython/core/events.py:89: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  func(*args, **kwargs)\n/Users/aldoglielmo/miniconda3/envs/py39_2/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n</code>\n</pre> <pre><code># only plot global minimums\nfig = plt.figure(figsize=(15, 15))\naxes = plt.axes()\n\n# plot true parameters\nplt.scatter(true_params[0], true_params[1], marker=\"X\", s=500, facecolor=\"red\", edgecolors=\"black\")\n\n# plot the history of global minimum\nplot_trajectory(history_of_mins, axes, marker=\"X\", color=\"y\", label=\"global minima\")\n\n\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.xlabel(\"param. 1\")\nplt.ylabel(\"param. 2\")\n</code></pre> <pre>\n<code>Text(0, 0.5, 'param. 2')</code>\n</pre> <pre><code>print(\"Best parameter found: \", history_of_mins[-1])\n</code></pre> <pre>\n<code>Best parameter found:  [0.55 0.52]\n</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"particle_swarm_sampler/#illustrating-the-functioning-of-the-particle-swarm-sampler-on-a-2d-parameter-space","title":"Illustrating the functioning of the Particle Swarm Sampler on a 2D parameter space","text":""},{"location":"samplers/","title":"Samplers","text":"<p>BaseSampler interface.</p> <p>This is the base class for all samplers.</p> Source code in <code>black_it/samplers/base.py</code> <pre><code>class BaseSampler(BaseSeedable, ABC):\n    \"\"\"BaseSampler interface.\n\n    This is the base class for all samplers.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        max_deduplication_passes: int = 5,\n    ) -&gt; None:\n        \"\"\"Initialize the sampler.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the internal state of the sampler, fixing this numbers the sampler behaves deterministically\n            max_deduplication_passes: maximum number of duplication passes done to avoid sampling repeated parameters\n        \"\"\"\n        BaseSeedable.__init__(self, random_state=random_state)\n        self.batch_size: int = batch_size\n        self.max_deduplication_passes = max_deduplication_passes\n\n    @abstractmethod\n    def sample_batch(\n        self,\n        batch_size: int,\n        search_space: SearchSpace,\n        existing_points: NDArray[np.float64],\n        existing_losses: NDArray[np.float64],\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Sample a number of new parameters fixed by the 'batch_size' attribute.\n\n        Args:\n            batch_size: number of samples to collect\n            search_space: an object containing the details of the parameter search space\n            existing_points: the parameters already sampled\n            existing_losses: the loss corresponding to the sampled parameters\n\n        Returns:\n            the new parameters\n        \"\"\"\n\n    def sample(\n        self,\n        search_space: SearchSpace,\n        existing_points: NDArray[np.float64],\n        existing_losses: NDArray[np.float64],\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Sample from the search space.\n\n        Args:\n            search_space: an object containing the details of the parameter search space\n            existing_points: the parameters already sampled\n            existing_losses: the loss corresponding to the sampled parameters\n\n        Returns:\n            the sampled parameters\n        \"\"\"\n        samples = self.sample_batch(\n            self.batch_size,\n            search_space,\n            existing_points,\n            existing_losses,\n        )\n\n        for n in range(self.max_deduplication_passes):\n            duplicates = self.find_and_get_duplicates(samples, existing_points)\n\n            num_duplicates = len(duplicates)\n\n            if num_duplicates == 0:\n                break\n\n            new_samples = self.sample_batch(\n                num_duplicates,\n                search_space,\n                existing_points,\n                existing_losses,\n            )\n            samples[duplicates] = new_samples\n\n            if n == self.max_deduplication_passes - 1:\n                print(\n                    f\"Warning: Repeated samples still found after {self.max_deduplication_passes} duplication passes.\"\n                    \" This is probably due to a small search space.\",\n                )\n\n        return samples\n\n    @staticmethod\n    def find_and_get_duplicates(\n        new_points: NDArray[np.float64],\n        existing_points: NDArray[np.float64],\n    ) -&gt; list:\n        \"\"\"Find the points in 'new_points' that are already present in 'existing_points'.\n\n        Args:\n            new_points: candidates points for the sampler\n            existing_points: previously sampled points\n\n        Returns:\n            the location of the duplicates in 'new_points'\n        \"\"\"\n        all_points = np.concatenate((existing_points, new_points))\n        unq, count = np.unique(all_points, axis=0, return_counts=True)\n        repeated_groups = unq[count &gt; 1]\n\n        repeated_pos = []\n        if len(repeated_groups) &gt; 0:\n            for repeated_group in repeated_groups:\n                repeated_idx = np.argwhere(np.all(new_points == repeated_group, axis=1))\n                for index in repeated_idx:\n                    repeated_pos.append(index[0])  # noqa: PERF401\n\n        return repeated_pos\n</code></pre> <p>Random uniform sampling.</p> Source code in <code>black_it/samplers/random_uniform.py</code> <pre><code>class RandomUniformSampler(BaseSampler):\n    \"\"\"Random uniform sampling.\"\"\"\n\n    def sample_batch(\n        self,\n        batch_size: int,\n        search_space: SearchSpace,\n        existing_points: NDArray[np.float64],  # noqa: ARG002\n        existing_losses: NDArray[np.float64],  # noqa: ARG002\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Sample uniformly from the search space.\n\n        Args:\n            batch_size: the number of points to sample\n            search_space: an object containing the details of the parameter search space\n            existing_points: the parameters already sampled\n            existing_losses: the loss corresponding to the sampled parameters\n\n        Returns:\n            the sampled parameters (an array of shape `(self.batch_size, search_space.dims)`)\n        \"\"\"\n        candidates = np.zeros((batch_size, search_space.dims))\n        for i, params in enumerate(search_space.param_grid):\n            candidates[:, i] = self.random_generator.choice(params, size=(batch_size,))\n        return candidates\n</code></pre> <p>Halton low discrepancy sequence.</p> <p>This snippet implements the Halton sequence following the generalization of a sequence of Van der Corput in n-dimensions.</p> Source code in <code>black_it/samplers/halton.py</code> <pre><code>class HaltonSampler(BaseSampler):\n    \"\"\"Halton low discrepancy sequence.\n\n    This snippet implements the Halton sequence following the generalization of\n    a sequence of *Van der Corput* in n-dimensions.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        max_deduplication_passes: int = 5,\n    ) -&gt; None:\n        \"\"\"Initialize the sampler.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n            max_deduplication_passes: the maximum number of sample deduplication passes.\n        \"\"\"\n        super().__init__(batch_size, random_state, max_deduplication_passes)\n        self._prime_number_generator = _CachedPrimesCalculator()\n\n        # drop first N entries to avoid linear correlation\n        self._reset_sequence_index()\n\n    def _set_random_state(self, random_state: int | None) -&gt; None:\n        \"\"\"Set the random state (private use).\n\n        For the Halton sampler, it also resets the sequence index.\n\n        Args:\n            random_state: the random seed\n        \"\"\"\n        super()._set_random_state(random_state)\n        self._reset_sequence_index()\n\n    def _reset_sequence_index(self) -&gt; None:\n        \"\"\"Reset the sequence index pointer.\"\"\"\n        self._sequence_index = self.random_generator.integers(\n            _MIN_SEQUENCE_START_INDEX,\n            _MAX_SEQUENCE_START_INDEX,\n        )\n\n    def sample_batch(\n        self,\n        batch_size: int,\n        search_space: SearchSpace,\n        existing_points: NDArray[np.float64],  # noqa: ARG002\n        existing_losses: NDArray[np.float64],  # noqa: ARG002\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Sample points using Halton sequence.\n\n        Args:\n            batch_size: the number of samples\n            search_space: an object containing the details of the parameter search space\n            existing_points: the parameters already sampled (not used)\n            existing_losses: the loss corresponding to the sampled parameters (not used)\n\n        Returns:\n            the parameter sampled\n        \"\"\"\n        unit_cube_points: NDArray[np.float64] = self._halton(\n            batch_size,\n            search_space.dims,\n        )\n        p_bounds: NDArray[np.float64] = search_space.parameters_bounds\n        sampled_points = p_bounds[0] + unit_cube_points * (p_bounds[1] - p_bounds[0])\n        return digitize_data(sampled_points, search_space.param_grid)\n\n    def _halton(self, nb_samples: int, dims: int) -&gt; NDArray[np.float64]:\n        \"\"\"Get a Halton sequence.\n\n        It uses a simple prime number generator, which takes the first `dims` primes.\n\n        Args:\n            nb_samples: number of samples\n            dims: the number of dimensions of the space to sample from the unitary cube\n\n        Returns:\n            sequence of Halton.\n        \"\"\"\n        bases: NDArray[np.int64] = self._prime_number_generator.get_n_primes(dims)\n        # Generate a sample using a Halton sequence.\n        sample: NDArray[np.float64] = halton(\n            sample_size=nb_samples,\n            bases=bases,\n            n_start=self._sequence_index,\n        )\n\n        # increment sequence start index for the next sampling\n        self._sequence_index += nb_samples\n        return sample\n</code></pre> <p>The R-sequence sampler.</p> Source code in <code>black_it/samplers/r_sequence.py</code> <pre><code>class RSequenceSampler(BaseSampler):\n    \"\"\"The R-sequence sampler.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        max_deduplication_passes: int = 5,\n    ) -&gt; None:\n        \"\"\"Initialize the sampler.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n            max_deduplication_passes: (non-negative integer) the maximum number of deduplication passes that are made\n                after every batch sampling. Default: 0, i.e. no deduplication happens.\n        \"\"\"\n        super().__init__(batch_size, random_state, max_deduplication_passes)\n\n        self._sequence_index: int\n        self._sequence_start: float\n        self._reset()\n\n    @classmethod\n    def compute_phi(cls, nb_dims: int) -&gt; float:\n        \"\"\"Get an approximation of phi^nb_dims.\n\n        Args:\n            nb_dims: the number of dimensions.\n\n        Returns:\n            phi^nb_dims\n        \"\"\"\n        check_arg(nb_dims &gt;= 1, f\"nb_dims should be greater than 0, got {nb_dims}\")\n        phi: float = 2.0\n        old_phi = None\n        while old_phi != phi:\n            old_phi = phi\n            phi = pow(1 + phi, 1.0 / (nb_dims + 1))\n        return phi\n\n    def _set_random_state(self, random_state: int | None) -&gt; None:\n        \"\"\"Set the random state (private use).\n\n        For the RSequence sampler, it also resets the sequence index and the sequence start.\n\n        Args:\n            random_state: the random seed\n        \"\"\"\n        super()._set_random_state(random_state)\n        self._reset()\n\n    def _reset(self) -&gt; None:\n        \"\"\"Reset the index of the sequence.\"\"\"\n        self._sequence_index = self.random_generator.integers(\n            _MIN_SEQUENCE_START_INDEX,\n            _MAX_SEQUENCE_START_INDEX,\n        )\n        self._sequence_start = self.random_generator.random()\n\n    def sample_batch(\n        self,\n        batch_size: int,\n        search_space: SearchSpace,\n        existing_points: NDArray[np.float64],  # noqa: ARG002\n        existing_losses: NDArray[np.float64],  # noqa: ARG002\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Sample points using the R-sequence.\n\n        Args:\n            batch_size: the number of samples\n            search_space: an object containing the details of the parameter search space\n            existing_points: the parameters already sampled (not used)\n            existing_losses: the loss corresponding to the sampled parameters (not used)\n\n        Returns:\n            the parameter sampled\n        \"\"\"\n        unit_cube_points: NDArray[np.float64] = self._r_sequence(\n            batch_size,\n            search_space.dims,\n        )\n        p_bounds: NDArray[np.float64] = search_space.parameters_bounds\n        sampled_points = p_bounds[0] + unit_cube_points * (p_bounds[1] - p_bounds[0])\n        return digitize_data(sampled_points, search_space.param_grid)\n\n    def _r_sequence(self, nb_samples: int, dims: int) -&gt; NDArray[np.float64]:\n        \"\"\"Compute the R-sequence (http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/).\n\n        Args:\n            nb_samples: number of points to sample\n            dims: the number of dimensions\n\n        Returns:\n            Set of params uniformly placed in d-dimensional unit cube.\n        \"\"\"\n        phi = self.compute_phi(dims)\n        alpha: NDArray[np.float64] = np.power(1 / phi, np.arange(1, dims + 1)).reshape(\n            (1, -1),\n        )\n        end_index = self._sequence_index + nb_samples\n        indexes = np.arange(self._sequence_index, end_index).reshape((-1, 1))\n        points: NDArray[np.float64] = (self._sequence_start + indexes.dot(alpha)) % 1\n        self._sequence_index = end_index\n        return points\n</code></pre> <p>This class implements the best-batch sampler.</p> <p>The sampler is a very essential type of genetic algorithm that takes the parameters corresponding   to the current lowest loss values and perturbs them slightly in a purely random fashion. The sampler first chooses the total number of coordinates to perturb via a beta-binomial distribution   BetaBin(dims, a, b) --where dims is the total number of dimensions in the search space --, it then selects   that many coordinate randomly, and perturbs them uniformly within the range specified by 'perturbation_range'.</p> Source code in <code>black_it/samplers/best_batch.py</code> <pre><code>class BestBatchSampler(BaseSampler):\n    \"\"\"This class implements the best-batch sampler.\n\n    The sampler is a very essential type of genetic algorithm that takes the parameters corresponding\n      to the current lowest loss values and perturbs them slightly in a purely random fashion.\n    The sampler first chooses the total number of coordinates to perturb via a beta-binomial distribution\n      BetaBin(dims, a, b) --where dims is the total number of dimensions in the search space --, it then selects\n      that many coordinate randomly, and perturbs them uniformly within the range specified by 'perturbation_range'.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        max_deduplication_passes: int = 5,\n        a: float = 3.0,\n        b: float = 1.0,\n        perturbation_range: int = 6,\n    ) -&gt; None:\n        \"\"\"Initialize the sampler.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n            max_deduplication_passes: the maximum number of deduplication passes that are made\n            a: the a parameter of the beta-binomial distribution\n            b: the b parameter of the beta-binomial distribution\n            perturbation_range: the range of the perturbation applied. The actual perturbation will be in the range\n                plus/minus the perturbation_range times the precision of the specific parameter coordinate\n        \"\"\"\n        _assert(\n            a &gt; 0,\n            \"'a' should be greater than zero\",\n        )\n        _assert(\n            b &gt; 0,\n            \"'b' should be greater than zero\",\n        )\n        _assert(\n            perturbation_range &gt; 1,\n            \"'perturbation_range' should be greater than one\",\n        )\n\n        super().__init__(batch_size, random_state, max_deduplication_passes)\n        self.a = a\n        self.b = b\n        self.perturbation_range = perturbation_range\n\n    def sample_batch(\n        self,\n        batch_size: int,\n        search_space: SearchSpace,\n        existing_points: NDArray[np.float64],\n        existing_losses: NDArray[np.float64],\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Sample from the search space using a genetic algorithm.\n\n        Args:\n            batch_size: the number of points to sample\n            search_space: an object containing the details of the parameter search space\n            existing_points: the parameters already sampled\n            existing_losses: the loss corresponding to the sampled parameters\n\n        Returns:\n            the sampled parameters (an array of shape `(self.batch_size, search_space.dims)`)\n        \"\"\"\n        if len(existing_points) &lt; batch_size:\n            msg = (\n                \"best-batch sampler requires a number of existing points \"\n                f\"which is at least the batch size {batch_size}, got {len(existing_points)}\"\n            )\n            raise ValueError(\n                msg,\n            )\n\n        # sort existing params\n        candidate_points: NDArray[np.float64] = existing_points[np.argsort(existing_losses)][:batch_size, :]\n\n        candidate_point_indexes: NDArray[np.int64] = self.random_generator.integers(\n            0,\n            batch_size,\n            size=batch_size,\n        )\n        sampled_points: NDArray[np.float64] = np.copy(\n            candidate_points[candidate_point_indexes],\n        )\n\n        beta_binom_rv = betabinom(n=search_space.dims - 1, a=self.a, b=self.b)\n        beta_binom_rv.random_state = self.random_generator\n\n        for sampled_point in sampled_points:\n            num_shocks: NDArray[np.int64] = beta_binom_rv.rvs(size=1) + 1\n            params_shocked: NDArray[np.int64] = self.random_generator.choice(\n                search_space.dims,\n                tuple(num_shocks),\n                replace=False,\n            )\n            for index in params_shocked:\n                shock_size: int = self.random_generator.integers(\n                    1,\n                    self.perturbation_range,\n                )\n                shock_sign: int = (self.random_generator.integers(0, 2) * 2) - 1\n\n                delta: float = search_space.parameters_precision[index]\n                shift: float = delta * shock_sign * shock_size\n                sampled_point[index] += shift\n\n                sampled_point[index] = np.clip(\n                    sampled_point[index],\n                    search_space.parameters_bounds[0][index],\n                    search_space.parameters_bounds[1][index],\n                )\n\n        return sampled_points\n</code></pre> <p>This class implements the Gaussian process-based sampler.</p> <p>In particular, the sampling is based on a Gaussian Process interpolation of the loss function.</p> <p>Note: this class is a wrapper of the GaussianProcessRegressor model of the scikit-learn package.</p> Source code in <code>black_it/samplers/gaussian_process.py</code> <pre><code>class GaussianProcessSampler(MLSurrogateSampler):\n    \"\"\"This class implements the Gaussian process-based sampler.\n\n    In particular, the sampling is based on a Gaussian Process interpolation of the loss function.\n\n    Note: this class is a wrapper of the GaussianProcessRegressor model of the scikit-learn package.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        max_deduplication_passes: int = 5,\n        candidate_pool_size: int | None = None,\n        optimize_restarts: int = 5,\n        acquisition: str = \"expected_improvement\",\n        jitter: float = 0.1,\n    ) -&gt; None:\n        \"\"\"Initialize the sampler.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n            max_deduplication_passes: the maximum number of deduplication passes that are made\n            candidate_pool_size: number of randomly sampled points on which the random forest predictions are evaluated\n            optimize_restarts: number of independent random trials of the optimization of the GP hyperparameters\n            acquisition: type of acquisition function, it can be 'expected_improvement' of simply 'mean'\n            jitter: positive value to make the \"expected_improvement\" acquisition more explorative.\n        \"\"\"\n        self._validate_acquisition(acquisition)\n\n        super().__init__(\n            batch_size,\n            random_state,\n            max_deduplication_passes,\n            candidate_pool_size,\n        )\n        self.optimize_restarts = optimize_restarts\n        self.acquisition = acquisition\n        self.jitter = jitter\n        self._gpmodel: GaussianProcessRegressor | None = None\n        self._fmin: np.double | float | None = None\n\n    @staticmethod\n    def _validate_acquisition(acquisition: str) -&gt; None:\n        \"\"\"Check that the required acquisition is among the supported ones.\n\n        Args:\n            acquisition: the acquisition provided as input of the constructor.\n\n        Raises:\n            ValueError: if the provided acquisition type is not among the allowed ones.\n        \"\"\"\n        try:\n            _AcquisitionTypes(acquisition)\n        except ValueError as e:\n            msg = (\n                \"expected one of the following acquisition types: \"\n                f\"[{' '.join(map(str, _AcquisitionTypes))}], got {acquisition}\"\n            )\n            raise ValueError(\n                msg,\n            ) from e\n\n    def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; None:  # noqa: N803\n        \"\"\"Fit a gaussian process surrogate model.\"\"\"\n        y = np.atleast_2d(y).T\n\n        if X.shape[0] &gt; _BIG_DATASET_SIZE_WARNING_THRESHOLD:\n            warnings.warn(  # noqa: B028\n                \"Standard GP evaluations can be expensive for large datasets, consider implementing a sparse GP\",\n                RuntimeWarning,\n            )\n\n        # initialize GP class from scikit-learn with a Matern kernel\n        kernel = kernels.Matern(length_scale=1, length_scale_bounds=(1e-5, 1e5), nu=2.5)\n\n        noise_var = y.var() * 0.01\n\n        self._gpmodel = GaussianProcessRegressor(\n            kernel=kernel,\n            alpha=noise_var,\n            n_restarts_optimizer=self.optimize_restarts,\n            optimizer=\"fmin_l_bfgs_b\",\n            random_state=self._get_random_seed(),\n        )\n\n        self._gpmodel.fit(X, y)\n\n        # store minimum\n        if self.acquisition == \"expected_improvement\":\n            m, _ = self._predict_mean_std(X)\n            self._fmin = np.min(m)\n\n    def predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:  # noqa: N803\n        \"\"\"Predict using a gaussian process surrogate model.\"\"\"\n        # predict mean or expected improvement on the full sample set\n        if self.acquisition == _AcquisitionTypes.EI.value:\n            # minus sign needed for subsequent sorting\n            candidates_score = -self._predict_EI(X, self.jitter)\n        else:  # acquisition is \"mean\"\n            candidates_score = self._predict_mean_std(X)[0]\n\n        return candidates_score\n\n    def _predict_mean_std(\n        self,\n        X: NDArray[np.float64],  # noqa: N803\n    ) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]:\n        \"\"\"Predict mean and standard deviation of a fitted GP.\n\n        Args:\n            X: the points on which the predictions should be performed\n\n        Returns:\n            The pair (mean, std).\n        \"\"\"\n        gpmodel = cast(\"GaussianProcessRegressor\", self._gpmodel)\n        X = X[None, :] if X.ndim == 1 else X  # noqa: N806\n        m, s = gpmodel.predict(X, return_std=True, return_cov=False)\n        s = np.clip(s, 1e-5, np.inf)\n        return m, s\n\n    def _predict_EI(  # noqa: N802\n        self,\n        X: NDArray[np.float64],  # noqa: N803\n        jitter: float = 0.1,\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Compute the Expected Improvement per unit of cost.\n\n        Args:\n            X:  the points on which the predictions should be performed\n            jitter: positive value to make the acquisition more explorative.\n\n        Returns:\n            the expected improvement.\n        \"\"\"\n        m, s = self._predict_mean_std(X)\n\n        fmin = cast(\"float\", self._fmin)\n\n        phi, Phi, u = self.get_quantiles(jitter, fmin, m, s)  # noqa: N806\n\n        return s * (u * Phi + phi)\n\n    @staticmethod\n    def get_quantiles(\n        acquisition_par: float,\n        fmin: float,\n        m: NDArray[np.float64],\n        s: NDArray[np.float64],\n    ) -&gt; tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:\n        \"\"\"Quantiles of the Gaussian distribution useful to determine the acquisition function values.\n\n        Args:\n            acquisition_par: parameter of the acquisition function\n            fmin: current minimum.\n            m: vector of means.\n            s: vector of standard deviations.\n\n        Returns:\n            the quantiles.\n        \"\"\"\n        # remove values of variance that are too small\n        s[s &lt; _SMALL_VARIANCE_VALUES] = _SMALL_VARIANCE_VALUES\n\n        u: NDArray[np.float64] = (fmin - m - acquisition_par) / s\n        phi: NDArray[np.float64] = np.exp(-0.5 * u**2) / np.sqrt(2 * np.pi)\n        Phi: NDArray[np.float64] = 0.5 * erfc(-u / np.sqrt(2))  # noqa: N806\n\n        return phi, Phi, u\n</code></pre> <p>This class implements random forest sampling.</p> Source code in <code>black_it/samplers/random_forest.py</code> <pre><code>class RandomForestSampler(MLSurrogateSampler):\n    \"\"\"This class implements random forest sampling.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        max_deduplication_passes: int = 5,\n        candidate_pool_size: int | None = None,\n        n_estimators: int = 500,\n        criterion: str = \"gini\",\n        n_classes: int = 10,\n    ) -&gt; None:\n        \"\"\"Random forest sampling.\n\n        Note: this class makes use of sklearn.ensemble.RandomForestClassifier.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n            max_deduplication_passes: the maximum number of deduplication passes\n            candidate_pool_size: number of randomly sampled points on which the random forest predictions are evaluated\n            n_estimators: number of trees in the forest\n            criterion: the function to measure the quality of a split.\n            n_classes: the number of classes used in the random forest. The classes are selected as the quantiles\n                of the distribution of loss values.\n        \"\"\"\n        _assert(\n            n_classes &gt; _MIN_RF_CLASSES,\n            \"'n_classes' should be at least 2 to provide meaningful results\",\n        )\n\n        super().__init__(\n            batch_size,\n            random_state,\n            max_deduplication_passes,\n            candidate_pool_size,\n        )\n\n        self._n_estimators = n_estimators\n        self._criterion = criterion\n        self._n_classes = n_classes\n        self._classifier: RandomForestClassifier | None = None\n\n    @property\n    def n_estimators(self) -&gt; int:\n        \"\"\"Get the number of estimators.\"\"\"\n        return self._n_estimators\n\n    @property\n    def criterion(self) -&gt; str:\n        \"\"\"Get the criterion.\"\"\"\n        return self._criterion\n\n    @property\n    def n_classes(self) -&gt; int:\n        \"\"\"Get the number of classes.\"\"\"\n        return self._n_classes\n\n    def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; None:  # noqa: N803\n        \"\"\"Fit a random forest surrogate model.\"\"\"\n        # Train surrogate\n\n        (\n            X,  # noqa: N806\n            y_cat,\n            _existing_points_quantiles,\n        ) = self.prepare_data_for_classifier(X, y, self.n_classes)\n\n        self._classifier = RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            criterion=self.criterion,\n            n_jobs=-1,\n            random_state=self._get_random_seed(),\n        )\n        self._classifier.fit(X, y_cat)\n\n    def predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:  # noqa: N803\n        \"\"\"Predict using a random forest surrogate model.\"\"\"\n        # Predict quantiles\n        self._classifier = cast(\"RandomForestClassifier\", self._classifier)\n        predicted_points_quantiles: NDArray[np.float64] = self._classifier.predict(X)\n\n        return predicted_points_quantiles\n\n    @staticmethod\n    def prepare_data_for_classifier(\n        existing_points: NDArray[np.float64],\n        existing_losses: NDArray[np.float64],\n        num_bins: int,\n    ) -&gt; tuple[NDArray[np.float64], NDArray[np.int64], NDArray[np.float64]]:\n        \"\"\"Prepare data for the classifier.\n\n        Args:\n            existing_points: the parameters already sampled\n            existing_losses: the loss corresponding to the sampled parameters\n            num_bins: the number of bins\n\n        Returns:\n            A triple (x, y, quantiles), where\n                - x is the vector of training data\n                - y is the vector of targets\n                - the quantiles\n        \"\"\"\n        x: NDArray[np.float64] = existing_points\n        y: NDArray[np.float64] = existing_losses\n\n        cutoffs: NDArray[np.float64] = np.linspace(0, 1, num_bins + 1)\n        quantiles: NDArray[np.float64] = np.zeros(num_bins + 1)\n\n        for i in range(num_bins - 1):\n            quantiles[i + 1] = np.quantile(y, cutoffs[i + 1])\n\n        quantiles[-1] = np.max(y)\n\n        y_cat: NDArray[np.int64] = np.digitize(y, quantiles, right=True)\n        y_cat = y_cat - 1\n\n        return x, y_cat, quantiles\n</code></pre> <p>This class implements xgboost sampling.</p> Source code in <code>black_it/samplers/xgboost.py</code> <pre><code>class XGBoostSampler(MLSurrogateSampler):\n    \"\"\"This class implements xgboost sampling.\"\"\"\n\n    def __init__(  # noqa: PLR0913\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        max_deduplication_passes: int = 5,\n        candidate_pool_size: int | None = None,\n        colsample_bytree: float = 0.3,\n        learning_rate: float = 0.1,\n        max_depth: int = 5,\n        alpha: float = 1.0,\n        n_estimators: int = 10,\n    ) -&gt; None:\n        \"\"\"Sampler based on a xgboost surrogate model of the loss function.\n\n        Note: this class makes use of the xgboost library, for more information on the XGBoost parameters\n            visit https://xgboost.readthedocs.io/en/stable/parameter.html.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n            max_deduplication_passes: the maximum number of deduplication passes\n            candidate_pool_size: number of randomly sampled points on which the random forest predictions are evaluated\n            colsample_bytree: subsample ratio of columns when constructing each tree\n            learning_rate: the learning rate\n            max_depth: maximum depth of XGBoost trees\n            alpha: L1 regularization term on weights\n            n_estimators: number of estimators\n\n        References:\n            Lamperti, Roventini, and Sani, \"Agent-based model calibration using machine learning surrogates\"\n        \"\"\"\n        super().__init__(\n            batch_size,\n            random_state,\n            max_deduplication_passes,\n            candidate_pool_size,\n        )\n\n        self._colsample_bytree = colsample_bytree\n        self._learning_rate = learning_rate\n        self._max_depth = max_depth\n        self._alpha = alpha\n        self._n_estimators = n_estimators\n        self._xg_regressor: xgb.XGBRegressor | None = None\n\n    @property\n    def colsample_bytree(self) -&gt; float:\n        \"\"\"Get the colsample_bytree parameter.\"\"\"\n        return self._colsample_bytree\n\n    @property\n    def learning_rate(self) -&gt; float:\n        \"\"\"Get the learning rate.\"\"\"\n        return self._learning_rate\n\n    @property\n    def max_depth(self) -&gt; int:\n        \"\"\"Get the maximum tree depth.\"\"\"\n        return self._max_depth\n\n    @property\n    def alpha(self) -&gt; float:\n        \"\"\"Get the alpha regularisation parameter.\"\"\"\n        return self._alpha\n\n    @property\n    def n_estimators(self) -&gt; int:\n        \"\"\"Get the number of estimators.\"\"\"\n        return self._n_estimators\n\n    @staticmethod\n    def _clip_losses(y: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n        \"\"\"Check that loss values fall within the float32 limits needed for XGBoost to work.\"\"\"\n        large_floats = np.where(y &gt;= MAX_FLOAT32)[0]\n        small_floats = np.where(y &lt;= MIN_FLOAT32)[0]\n\n        if len(large_floats) == 0 and len(small_floats) == 0:\n            return y\n\n        warnings.warn(  # noqa: B028\n            \"Found loss values out of float32 limits, clipping them for XGBoost.\",\n            RuntimeWarning,\n        )\n        if len(large_floats) &gt; 0:\n            y[large_floats] = MAX_FLOAT32 - EPS_FLOAT32\n\n        if len(small_floats) &gt; 0:\n            y[small_floats] = MIN_FLOAT32 + EPS_FLOAT32\n\n        return y\n\n    def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; None:  # noqa: N803\n        \"\"\"Fit a xgboost surrogate model.\"\"\"\n        # prepare data\n        y = self._clip_losses(y)\n        _ = xgb.DMatrix(data=X, label=y)\n\n        # train surrogate\n        self._xg_regressor = xgb.XGBRegressor(\n            objective=\"reg:squarederror\",  # original: objective ='reg:linear',\n            colsample_bytree=self.colsample_bytree,\n            learning_rate=self.learning_rate,\n            max_depth=self.max_depth,  # original: 5\n            alpha=self.alpha,\n            n_estimators=self.n_estimators,\n            random_state=self._get_random_seed(),\n        )  # original: 10\n\n        self._xg_regressor.fit(X, y)\n\n    def predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:  # noqa: N803\n        \"\"\"Predict using a xgboost surrogate model.\"\"\"\n        # predict over large pool of candidates\n        _ = xgb.DMatrix(data=X)\n\n        self._xg_regressor = cast(\"xgb.XGBRegressor\", self._xg_regressor)\n        return self._xg_regressor.predict(X)\n</code></pre> <p>Implementation of a particle swarm sampler.</p> <p>This sampler implements the particle swarm sampling method commonly used in particle swarm optimization (PSO), introduced in:</p> <pre><code>Eberhart, Russell, and James Kennedy. \"A new optimizer using particle swarm theory.\"\n  MHS'95. Proceedings of the sixth international symposium on micro machine and human science. IEEE, 1995.\n</code></pre> <p>In a particle swarm optimizer, there is a set of particles that are \"evolved\" by cooperation and competition among the individuals themselves through generations. Each particle adjusts its flying  according to its own flying experience and its companions' flying experience. Each particle, in fact, represents a potential solution to a problem. Each particle is treated as a point in a D-dimensional space.  The ith particle is represented as Xi = (x_{i1},...,x_{iD}). The best previous position (the position giving the best fitness value) of any particle is recorded and represented as Pi = (p_{i1},...,p_{iD}). The index of the best particle among all the particles in the population is represented by the symbol g. The rate of the position change (velocity) for particle i is represented as Vi = (v_{i1},...,v_{iD}). The particles are manipulated according to the following equation:</p> <pre><code>v_{id} = (\u03c9 * v_{id}) + (c1 * r1 * (p_{id} - x_{id})) + (c2 * r2 * (p_{gd} - x_{id})\nx_{id} = x_{id} + v_{id}\n</code></pre> <p>Where</p> <ul> <li>\u03c9 is the inertia weight  to control the influence of the previous velocity;</li> <li>c1 and c2 are positive values that represent the acceleration constants;</li> <li>r1 and r2 are two random numbers uniformly distributed in the range of (0, 1).</li> </ul> <p>Note that p_{gd}, the global best position found across the dynamics, can optionally be computed by also considering the sampling performed by other samplers in order to let them interfere constructively with the Particle Swarm Sampler.</p> Source code in <code>black_it/samplers/particle_swarm.py</code> <pre><code>class ParticleSwarmSampler(BaseSampler):\n    \"\"\"Implementation of a particle swarm sampler.\n\n    This sampler implements the particle swarm sampling method commonly used in particle swarm optimization (PSO),\n    introduced in:\n\n        Eberhart, Russell, and James Kennedy. \"A new optimizer using particle swarm theory.\"\n          MHS'95. Proceedings of the sixth international symposium on micro machine and human science. IEEE, 1995.\n\n    In a particle swarm optimizer, there is a set of particles that are \"evolved\" by cooperation and competition\n    among the individuals themselves through generations. Each particle adjusts its flying  according to its own\n    flying experience and its companions' flying experience. Each particle, in fact, represents a potential solution\n    to a problem. Each particle is treated as a point in a D-dimensional space.  The ith particle is represented as\n    Xi = (x_{i1},...,x_{iD}). The best previous position (the position giving the best fitness value) of any particle\n    is recorded and represented as Pi = (p_{i1},...,p_{iD}). The index of the best particle among all the particles\n    in the population is represented by the symbol g. The rate of the position change (velocity) for particle i is\n    represented as Vi = (v_{i1},...,v_{iD}). The particles are manipulated according to the following equation:\n\n        v_{id} = (\u03c9 * v_{id}) + (c1 * r1 * (p_{id} - x_{id})) + (c2 * r2 * (p_{gd} - x_{id})\n        x_{id} = x_{id} + v_{id}\n\n    Where:\n        - \u03c9 is the inertia weight  to control the influence of the previous velocity;\n        - c1 and c2 are positive values that represent the acceleration constants;\n        - r1 and r2 are two random numbers uniformly distributed in the range of (0, 1).\n\n    Note that p_{gd}, the global best position found across the dynamics, can optionally be computed by also\n    considering the sampling performed by other samplers in order to let them interfere constructively with the\n    Particle Swarm Sampler.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        random_state: int | None = None,\n        inertia: float = 0.9,\n        c1: float = 0.1,\n        c2: float = 0.1,\n        global_minimum_across_samplers: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the sampler.\n\n        Args:\n            batch_size: the number of points sampled every time the sampler is called\n            random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n            inertia: the inertia of the particles' motion\n            c1: first acceleration constant\n            c2: second acceleration constant\n            global_minimum_across_samplers: if True, the global minimum attractor of the particles' dynamics is computed\n                taking into consideration also parameters sampled by other samplers, default is False\n        \"\"\"\n        # max_duplication_passes must be zero because the sampler is stateful\n        super().__init__(\n            batch_size,\n            random_state=random_state,\n            max_deduplication_passes=0,\n        )\n\n        # The batch size is the number of sampled parameters per iteration. In a Black-it sampler, each call to\n        # sample_batch represent an iteration of the particle swarm sampler, so it seems natural to set the number of\n        # particles to the batch size, as at each iteration sample_batch returns the current positions of the\n        # particles.\n        self.nb_particles = batch_size\n\n        self._inertia = positive_float(inertia)\n        self._c1 = positive_float(c1)\n        self._c2 = positive_float(c2)\n        self._global_minimum_across_samplers = global_minimum_across_samplers\n\n        # all current particle positions; shape=(nb_particles, space dimensions)\n        self._curr_particle_positions: NDArray | None = None\n        # all current particle velocities; shape=(nb_particles, space dimensions)\n        self._curr_particle_velocities: NDArray | None = None\n        # best particle positions, i.e. ; shape=(nb_particles, space dimensions)\n        self._best_particle_positions: NDArray | None = None\n        # losses of the best positions\n        self._best_position_losses: NDArray | None = None\n        # particle id of the global best particle position\n        self._global_best_particle_id: int | None = None\n\n        # best point in parameter space - could be the best across samplers\n        self._best_point: NDArray | None = None\n\n        self._previous_batch_index_start: int | None = None\n\n    @property\n    def is_set_up(self) -&gt; bool:\n        \"\"\"Return true iff the sampler is already set up.\"\"\"\n        return self._curr_particle_positions is not None\n\n    @property\n    def inertia(self) -&gt; float:\n        \"\"\"Get the inertia weight.\"\"\"\n        return self._inertia\n\n    @property\n    def c1(self) -&gt; float:\n        \"\"\"Get the c1 constant.\"\"\"\n        return self._c1\n\n    @property\n    def c2(self) -&gt; float:\n        \"\"\"Get the c2 constant.\"\"\"\n        return self._c2\n\n    def _set_up(self, dims: int) -&gt; None:\n        \"\"\"Set up the sampler.\"\"\"\n        self._curr_particle_positions = self.random_generator.random(\n            size=(self.batch_size, dims),\n        )\n        self._curr_particle_velocities = (\n            self.random_generator.random(\n                size=cast(\"NDArray\", self._curr_particle_positions).shape,\n            )\n            - 0.5\n        )\n        self._best_particle_positions = self._curr_particle_positions\n        # set losses to inf as we are interested to the min\n        self._best_position_losses = np.full(self.nb_particles, np.inf)\n        # we don't know yet which is the best index - initialize to 0\n        self._global_best_particle_id = 0\n\n    def _get_best_position(self) -&gt; NDArray[np.float64]:\n        \"\"\"Get the position corresponding to the global optimum the particles should converge to.\n\n        If _global_minimum_across_samplers is False, then this method returns the current position\n        of the particle that in its history has sampled, so far, the best set of parameters.\n\n        Else, if _global_minimum_across_samplers is True, then this method returns the point\n        in parameter space that achieved the minimum loss. Note that this point could have been\n        sampled by a different sampler than \"self\".\n\n        Returns:\n            a Numpy array\n        \"\"\"\n        if not self._global_minimum_across_samplers:\n            best_particle_positions = cast(\"NDArray\", self._best_particle_positions)\n            return best_particle_positions[self._global_best_particle_id]\n        return cast(\"NDArray\", self._best_point)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the sampler.\"\"\"\n        self._curr_particle_positions = None\n        self._curr_particle_velocities = None\n        self._best_particle_positions = None\n        self._best_position_losses = None\n        self._global_best_particle_id = None\n        self._previous_batch_index_start = None\n        _assert(\n            not self.is_set_up,\n            error_message=\"reset call did not work, sampler still set up\",\n            exception_class=RuntimeError,\n        )\n\n    def sample_batch(\n        self,\n        batch_size: int,  # noqa: ARG002\n        search_space: SearchSpace,\n        existing_points: NDArray[np.float64],\n        existing_losses: NDArray[np.float64],\n    ) -&gt; NDArray[np.float64]:\n        \"\"\"Sample a batch of parameters.\"\"\"\n        if not self.is_set_up:\n            self._set_up(search_space.dims)\n            self._previous_batch_index_start = len(existing_points)\n            return digitize_data(\n                cast(\"NDArray[np.float64]\", self._best_particle_positions),\n                search_space.param_grid,\n            )\n\n        self._update_best(existing_points, existing_losses)\n        self._do_step()\n\n        p_bounds: NDArray[np.float64] = search_space.parameters_bounds\n        sampled_points = p_bounds[0] + self._curr_particle_positions * (p_bounds[1] - p_bounds[0])\n        self._previous_batch_index_start = len(existing_points)\n\n        return digitize_data(sampled_points, search_space.param_grid)\n\n    def _update_best(\n        self,\n        existing_points: NDArray[np.float64],\n        existing_losses: NDArray[np.float64],\n    ) -&gt; None:\n        \"\"\"Update the best local and global positions.\"\"\"\n        _assert(\n            self._previous_batch_index_start is not None,\n            exception_class=AssertionError,\n            error_message=\"should have been set\",\n        )\n\n        # set best loss and best point\n        best_point_index = np.argmin(existing_losses)\n        self._best_point = existing_points[best_point_index]\n\n        # set best particle position\n        batch_index_start = cast(\"int\", self._previous_batch_index_start)\n        batch_index_stop = batch_index_start + self.batch_size\n        previous_points = existing_points[batch_index_start:batch_index_stop]\n        previous_losses = existing_losses[batch_index_start:batch_index_stop]\n        for particle_id, (point, loss) in enumerate(\n            zip(previous_points, previous_losses),\n        ):\n            best_particle_positions = cast(\"NDArray\", self._best_particle_positions)\n            best_position_losses = cast(\"NDArray\", self._best_position_losses)\n            if best_position_losses[particle_id] &gt; loss:\n                best_particle_positions[particle_id] = point\n                best_position_losses[particle_id] = loss\n\n                # check if also the global best should be updated\n                best_global_loss = best_position_losses[self._global_best_particle_id]\n                if loss &lt; best_global_loss:\n                    self._global_best_particle_id = particle_id\n\n    def _do_step(self) -&gt; None:\n        \"\"\"Do a step by updating particle positions and velocities.\"\"\"\n        curr_particle_positions = cast(\"NDArray\", self._curr_particle_positions)\n        curr_particle_velocities = cast(\"NDArray\", self._curr_particle_velocities)\n        best_particle_positions = cast(\"NDArray\", self._best_particle_positions)\n        r1_vec = self.random_generator.random(size=curr_particle_positions.shape)\n        r2_vec = self.random_generator.random(size=curr_particle_positions.shape)\n        new_particle_velocities = (\n            self.inertia * curr_particle_velocities\n            + self.c1 * r1_vec * (best_particle_positions - self._curr_particle_positions)\n            + self.c2 * r2_vec * (self._get_best_position() - self._curr_particle_positions)  # type: ignore[operator]\n        )\n\n        self._curr_particle_positions = np.clip(\n            self._curr_particle_positions + new_particle_velocities,\n            a_min=0.0,\n            a_max=1.0,\n        )\n        self._curr_particle_velocities = new_particle_velocities\n</code></pre>"},{"location":"samplers/#black_it.samplers.base.BaseSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, max_deduplication_passes=5)</code>  <code>special</code>","text":"<p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the internal state of the sampler, fixing this numbers the sampler behaves deterministically</p> <code>None</code> <code>max_deduplication_passes</code> <code>int</code> <p>maximum number of duplication passes done to avoid sampling repeated parameters</p> <code>5</code> Source code in <code>black_it/samplers/base.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    max_deduplication_passes: int = 5,\n) -&gt; None:\n    \"\"\"Initialize the sampler.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the internal state of the sampler, fixing this numbers the sampler behaves deterministically\n        max_deduplication_passes: maximum number of duplication passes done to avoid sampling repeated parameters\n    \"\"\"\n    BaseSeedable.__init__(self, random_state=random_state)\n    self.batch_size: int = batch_size\n    self.max_deduplication_passes = max_deduplication_passes\n</code></pre>"},{"location":"samplers/#black_it.samplers.base.BaseSampler.find_and_get_duplicates","title":"<code>find_and_get_duplicates(new_points, existing_points)</code>  <code>staticmethod</code>","text":"<p>Find the points in 'new_points' that are already present in 'existing_points'.</p> <p>Parameters:</p> Name Type Description Default <code>new_points</code> <code>NDArray[np.float64]</code> <p>candidates points for the sampler</p> required <code>existing_points</code> <code>NDArray[np.float64]</code> <p>previously sampled points</p> required <p>Returns:</p> Type Description <code>list</code> <p>the location of the duplicates in 'new_points'</p> Source code in <code>black_it/samplers/base.py</code> <pre><code>@staticmethod\ndef find_and_get_duplicates(\n    new_points: NDArray[np.float64],\n    existing_points: NDArray[np.float64],\n) -&gt; list:\n    \"\"\"Find the points in 'new_points' that are already present in 'existing_points'.\n\n    Args:\n        new_points: candidates points for the sampler\n        existing_points: previously sampled points\n\n    Returns:\n        the location of the duplicates in 'new_points'\n    \"\"\"\n    all_points = np.concatenate((existing_points, new_points))\n    unq, count = np.unique(all_points, axis=0, return_counts=True)\n    repeated_groups = unq[count &gt; 1]\n\n    repeated_pos = []\n    if len(repeated_groups) &gt; 0:\n        for repeated_group in repeated_groups:\n            repeated_idx = np.argwhere(np.all(new_points == repeated_group, axis=1))\n            for index in repeated_idx:\n                repeated_pos.append(index[0])  # noqa: PERF401\n\n    return repeated_pos\n</code></pre>"},{"location":"samplers/#black_it.samplers.base.BaseSampler.sample","title":"<code>sample(self, search_space, existing_points, existing_losses)</code>","text":"<p>Sample from the search space.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>SearchSpace</code> <p>an object containing the details of the parameter search space</p> required <code>existing_points</code> <code>NDArray[np.float64]</code> <p>the parameters already sampled</p> required <code>existing_losses</code> <code>NDArray[np.float64]</code> <p>the loss corresponding to the sampled parameters</p> required <p>Returns:</p> Type Description <code>NDArray[np.float64]</code> <p>the sampled parameters</p> Source code in <code>black_it/samplers/base.py</code> <pre><code>def sample(\n    self,\n    search_space: SearchSpace,\n    existing_points: NDArray[np.float64],\n    existing_losses: NDArray[np.float64],\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sample from the search space.\n\n    Args:\n        search_space: an object containing the details of the parameter search space\n        existing_points: the parameters already sampled\n        existing_losses: the loss corresponding to the sampled parameters\n\n    Returns:\n        the sampled parameters\n    \"\"\"\n    samples = self.sample_batch(\n        self.batch_size,\n        search_space,\n        existing_points,\n        existing_losses,\n    )\n\n    for n in range(self.max_deduplication_passes):\n        duplicates = self.find_and_get_duplicates(samples, existing_points)\n\n        num_duplicates = len(duplicates)\n\n        if num_duplicates == 0:\n            break\n\n        new_samples = self.sample_batch(\n            num_duplicates,\n            search_space,\n            existing_points,\n            existing_losses,\n        )\n        samples[duplicates] = new_samples\n\n        if n == self.max_deduplication_passes - 1:\n            print(\n                f\"Warning: Repeated samples still found after {self.max_deduplication_passes} duplication passes.\"\n                \" This is probably due to a small search space.\",\n            )\n\n    return samples\n</code></pre>"},{"location":"samplers/#black_it.samplers.base.BaseSampler.sample_batch","title":"<code>sample_batch(self, batch_size, search_space, existing_points, existing_losses)</code>","text":"<p>Sample a number of new parameters fixed by the 'batch_size' attribute.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>number of samples to collect</p> required <code>search_space</code> <code>SearchSpace</code> <p>an object containing the details of the parameter search space</p> required <code>existing_points</code> <code>NDArray[np.float64]</code> <p>the parameters already sampled</p> required <code>existing_losses</code> <code>NDArray[np.float64]</code> <p>the loss corresponding to the sampled parameters</p> required <p>Returns:</p> Type Description <code>NDArray[np.float64]</code> <p>the new parameters</p> Source code in <code>black_it/samplers/base.py</code> <pre><code>@abstractmethod\ndef sample_batch(\n    self,\n    batch_size: int,\n    search_space: SearchSpace,\n    existing_points: NDArray[np.float64],\n    existing_losses: NDArray[np.float64],\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sample a number of new parameters fixed by the 'batch_size' attribute.\n\n    Args:\n        batch_size: number of samples to collect\n        search_space: an object containing the details of the parameter search space\n        existing_points: the parameters already sampled\n        existing_losses: the loss corresponding to the sampled parameters\n\n    Returns:\n        the new parameters\n    \"\"\"\n</code></pre>"},{"location":"samplers/#black_it.samplers.random_uniform.RandomUniformSampler.sample_batch","title":"<code>sample_batch(self, batch_size, search_space, existing_points, existing_losses)</code>","text":"<p>Sample uniformly from the search space.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points to sample</p> required <code>search_space</code> <code>SearchSpace</code> <p>an object containing the details of the parameter search space</p> required <code>existing_points</code> <code>ndarray</code> <p>the parameters already sampled</p> required <code>existing_losses</code> <code>ndarray</code> <p>the loss corresponding to the sampled parameters</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the sampled parameters (an array of shape <code>(self.batch_size, search_space.dims)</code>)</p> Source code in <code>black_it/samplers/random_uniform.py</code> <pre><code>def sample_batch(\n    self,\n    batch_size: int,\n    search_space: SearchSpace,\n    existing_points: NDArray[np.float64],  # noqa: ARG002\n    existing_losses: NDArray[np.float64],  # noqa: ARG002\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sample uniformly from the search space.\n\n    Args:\n        batch_size: the number of points to sample\n        search_space: an object containing the details of the parameter search space\n        existing_points: the parameters already sampled\n        existing_losses: the loss corresponding to the sampled parameters\n\n    Returns:\n        the sampled parameters (an array of shape `(self.batch_size, search_space.dims)`)\n    \"\"\"\n    candidates = np.zeros((batch_size, search_space.dims))\n    for i, params in enumerate(search_space.param_grid):\n        candidates[:, i] = self.random_generator.choice(params, size=(batch_size,))\n    return candidates\n</code></pre>"},{"location":"samplers/#black_it.samplers.halton.HaltonSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, max_deduplication_passes=5)</code>  <code>special</code>","text":"<p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the random state of the sampler, fixing this number the sampler behaves deterministically</p> <code>None</code> <code>max_deduplication_passes</code> <code>int</code> <p>the maximum number of sample deduplication passes.</p> <code>5</code> Source code in <code>black_it/samplers/halton.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    max_deduplication_passes: int = 5,\n) -&gt; None:\n    \"\"\"Initialize the sampler.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n        max_deduplication_passes: the maximum number of sample deduplication passes.\n    \"\"\"\n    super().__init__(batch_size, random_state, max_deduplication_passes)\n    self._prime_number_generator = _CachedPrimesCalculator()\n\n    # drop first N entries to avoid linear correlation\n    self._reset_sequence_index()\n</code></pre>"},{"location":"samplers/#black_it.samplers.halton.HaltonSampler.sample_batch","title":"<code>sample_batch(self, batch_size, search_space, existing_points, existing_losses)</code>","text":"<p>Sample points using Halton sequence.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of samples</p> required <code>search_space</code> <code>SearchSpace</code> <p>an object containing the details of the parameter search space</p> required <code>existing_points</code> <code>NDArray[np.float64]</code> <p>the parameters already sampled (not used)</p> required <code>existing_losses</code> <code>NDArray[np.float64]</code> <p>the loss corresponding to the sampled parameters (not used)</p> required <p>Returns:</p> Type Description <code>NDArray[np.float64]</code> <p>the parameter sampled</p> Source code in <code>black_it/samplers/halton.py</code> <pre><code>def sample_batch(\n    self,\n    batch_size: int,\n    search_space: SearchSpace,\n    existing_points: NDArray[np.float64],  # noqa: ARG002\n    existing_losses: NDArray[np.float64],  # noqa: ARG002\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sample points using Halton sequence.\n\n    Args:\n        batch_size: the number of samples\n        search_space: an object containing the details of the parameter search space\n        existing_points: the parameters already sampled (not used)\n        existing_losses: the loss corresponding to the sampled parameters (not used)\n\n    Returns:\n        the parameter sampled\n    \"\"\"\n    unit_cube_points: NDArray[np.float64] = self._halton(\n        batch_size,\n        search_space.dims,\n    )\n    p_bounds: NDArray[np.float64] = search_space.parameters_bounds\n    sampled_points = p_bounds[0] + unit_cube_points * (p_bounds[1] - p_bounds[0])\n    return digitize_data(sampled_points, search_space.param_grid)\n</code></pre>"},{"location":"samplers/#black_it.samplers.r_sequence.RSequenceSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, max_deduplication_passes=5)</code>  <code>special</code>","text":"<p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the random state of the sampler, fixing this number the sampler behaves deterministically</p> <code>None</code> <code>max_deduplication_passes</code> <code>int</code> <p>(non-negative integer) the maximum number of deduplication passes that are made after every batch sampling. Default: 0, i.e. no deduplication happens.</p> <code>5</code> Source code in <code>black_it/samplers/r_sequence.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    max_deduplication_passes: int = 5,\n) -&gt; None:\n    \"\"\"Initialize the sampler.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n        max_deduplication_passes: (non-negative integer) the maximum number of deduplication passes that are made\n            after every batch sampling. Default: 0, i.e. no deduplication happens.\n    \"\"\"\n    super().__init__(batch_size, random_state, max_deduplication_passes)\n\n    self._sequence_index: int\n    self._sequence_start: float\n    self._reset()\n</code></pre>"},{"location":"samplers/#black_it.samplers.r_sequence.RSequenceSampler.compute_phi","title":"<code>compute_phi(nb_dims)</code>  <code>classmethod</code>","text":"<p>Get an approximation of phi^nb_dims.</p> <p>Parameters:</p> Name Type Description Default <code>nb_dims</code> <code>int</code> <p>the number of dimensions.</p> required <p>Returns:</p> Type Description <code>float</code> <p>phi^nb_dims</p> Source code in <code>black_it/samplers/r_sequence.py</code> <pre><code>@classmethod\ndef compute_phi(cls, nb_dims: int) -&gt; float:\n    \"\"\"Get an approximation of phi^nb_dims.\n\n    Args:\n        nb_dims: the number of dimensions.\n\n    Returns:\n        phi^nb_dims\n    \"\"\"\n    check_arg(nb_dims &gt;= 1, f\"nb_dims should be greater than 0, got {nb_dims}\")\n    phi: float = 2.0\n    old_phi = None\n    while old_phi != phi:\n        old_phi = phi\n        phi = pow(1 + phi, 1.0 / (nb_dims + 1))\n    return phi\n</code></pre>"},{"location":"samplers/#black_it.samplers.r_sequence.RSequenceSampler.sample_batch","title":"<code>sample_batch(self, batch_size, search_space, existing_points, existing_losses)</code>","text":"<p>Sample points using the R-sequence.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of samples</p> required <code>search_space</code> <code>SearchSpace</code> <p>an object containing the details of the parameter search space</p> required <code>existing_points</code> <code>NDArray[np.float64]</code> <p>the parameters already sampled (not used)</p> required <code>existing_losses</code> <code>NDArray[np.float64]</code> <p>the loss corresponding to the sampled parameters (not used)</p> required <p>Returns:</p> Type Description <code>NDArray[np.float64]</code> <p>the parameter sampled</p> Source code in <code>black_it/samplers/r_sequence.py</code> <pre><code>def sample_batch(\n    self,\n    batch_size: int,\n    search_space: SearchSpace,\n    existing_points: NDArray[np.float64],  # noqa: ARG002\n    existing_losses: NDArray[np.float64],  # noqa: ARG002\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sample points using the R-sequence.\n\n    Args:\n        batch_size: the number of samples\n        search_space: an object containing the details of the parameter search space\n        existing_points: the parameters already sampled (not used)\n        existing_losses: the loss corresponding to the sampled parameters (not used)\n\n    Returns:\n        the parameter sampled\n    \"\"\"\n    unit_cube_points: NDArray[np.float64] = self._r_sequence(\n        batch_size,\n        search_space.dims,\n    )\n    p_bounds: NDArray[np.float64] = search_space.parameters_bounds\n    sampled_points = p_bounds[0] + unit_cube_points * (p_bounds[1] - p_bounds[0])\n    return digitize_data(sampled_points, search_space.param_grid)\n</code></pre>"},{"location":"samplers/#black_it.samplers.best_batch.BestBatchSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, max_deduplication_passes=5, a=3.0, b=1.0, perturbation_range=6)</code>  <code>special</code>","text":"<p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the random state of the sampler, fixing this number the sampler behaves deterministically</p> <code>None</code> <code>max_deduplication_passes</code> <code>int</code> <p>the maximum number of deduplication passes that are made</p> <code>5</code> <code>a</code> <code>float</code> <p>the a parameter of the beta-binomial distribution</p> <code>3.0</code> <code>b</code> <code>float</code> <p>the b parameter of the beta-binomial distribution</p> <code>1.0</code> <code>perturbation_range</code> <code>int</code> <p>the range of the perturbation applied. The actual perturbation will be in the range plus/minus the perturbation_range times the precision of the specific parameter coordinate</p> <code>6</code> Source code in <code>black_it/samplers/best_batch.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    max_deduplication_passes: int = 5,\n    a: float = 3.0,\n    b: float = 1.0,\n    perturbation_range: int = 6,\n) -&gt; None:\n    \"\"\"Initialize the sampler.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n        max_deduplication_passes: the maximum number of deduplication passes that are made\n        a: the a parameter of the beta-binomial distribution\n        b: the b parameter of the beta-binomial distribution\n        perturbation_range: the range of the perturbation applied. The actual perturbation will be in the range\n            plus/minus the perturbation_range times the precision of the specific parameter coordinate\n    \"\"\"\n    _assert(\n        a &gt; 0,\n        \"'a' should be greater than zero\",\n    )\n    _assert(\n        b &gt; 0,\n        \"'b' should be greater than zero\",\n    )\n    _assert(\n        perturbation_range &gt; 1,\n        \"'perturbation_range' should be greater than one\",\n    )\n\n    super().__init__(batch_size, random_state, max_deduplication_passes)\n    self.a = a\n    self.b = b\n    self.perturbation_range = perturbation_range\n</code></pre>"},{"location":"samplers/#black_it.samplers.best_batch.BestBatchSampler.sample_batch","title":"<code>sample_batch(self, batch_size, search_space, existing_points, existing_losses)</code>","text":"<p>Sample from the search space using a genetic algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points to sample</p> required <code>search_space</code> <code>SearchSpace</code> <p>an object containing the details of the parameter search space</p> required <code>existing_points</code> <code>NDArray[np.float64]</code> <p>the parameters already sampled</p> required <code>existing_losses</code> <code>NDArray[np.float64]</code> <p>the loss corresponding to the sampled parameters</p> required <p>Returns:</p> Type Description <code>NDArray[np.float64]</code> <p>the sampled parameters (an array of shape <code>(self.batch_size, search_space.dims)</code>)</p> Source code in <code>black_it/samplers/best_batch.py</code> <pre><code>def sample_batch(\n    self,\n    batch_size: int,\n    search_space: SearchSpace,\n    existing_points: NDArray[np.float64],\n    existing_losses: NDArray[np.float64],\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sample from the search space using a genetic algorithm.\n\n    Args:\n        batch_size: the number of points to sample\n        search_space: an object containing the details of the parameter search space\n        existing_points: the parameters already sampled\n        existing_losses: the loss corresponding to the sampled parameters\n\n    Returns:\n        the sampled parameters (an array of shape `(self.batch_size, search_space.dims)`)\n    \"\"\"\n    if len(existing_points) &lt; batch_size:\n        msg = (\n            \"best-batch sampler requires a number of existing points \"\n            f\"which is at least the batch size {batch_size}, got {len(existing_points)}\"\n        )\n        raise ValueError(\n            msg,\n        )\n\n    # sort existing params\n    candidate_points: NDArray[np.float64] = existing_points[np.argsort(existing_losses)][:batch_size, :]\n\n    candidate_point_indexes: NDArray[np.int64] = self.random_generator.integers(\n        0,\n        batch_size,\n        size=batch_size,\n    )\n    sampled_points: NDArray[np.float64] = np.copy(\n        candidate_points[candidate_point_indexes],\n    )\n\n    beta_binom_rv = betabinom(n=search_space.dims - 1, a=self.a, b=self.b)\n    beta_binom_rv.random_state = self.random_generator\n\n    for sampled_point in sampled_points:\n        num_shocks: NDArray[np.int64] = beta_binom_rv.rvs(size=1) + 1\n        params_shocked: NDArray[np.int64] = self.random_generator.choice(\n            search_space.dims,\n            tuple(num_shocks),\n            replace=False,\n        )\n        for index in params_shocked:\n            shock_size: int = self.random_generator.integers(\n                1,\n                self.perturbation_range,\n            )\n            shock_sign: int = (self.random_generator.integers(0, 2) * 2) - 1\n\n            delta: float = search_space.parameters_precision[index]\n            shift: float = delta * shock_sign * shock_size\n            sampled_point[index] += shift\n\n            sampled_point[index] = np.clip(\n                sampled_point[index],\n                search_space.parameters_bounds[0][index],\n                search_space.parameters_bounds[1][index],\n            )\n\n    return sampled_points\n</code></pre>"},{"location":"samplers/#black_it.samplers.gaussian_process.GaussianProcessSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, max_deduplication_passes=5, candidate_pool_size=None, optimize_restarts=5, acquisition='expected_improvement', jitter=0.1)</code>  <code>special</code>","text":"<p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the random state of the sampler, fixing this number the sampler behaves deterministically</p> <code>None</code> <code>max_deduplication_passes</code> <code>int</code> <p>the maximum number of deduplication passes that are made</p> <code>5</code> <code>candidate_pool_size</code> <code>int | None</code> <p>number of randomly sampled points on which the random forest predictions are evaluated</p> <code>None</code> <code>optimize_restarts</code> <code>int</code> <p>number of independent random trials of the optimization of the GP hyperparameters</p> <code>5</code> <code>acquisition</code> <code>str</code> <p>type of acquisition function, it can be 'expected_improvement' of simply 'mean'</p> <code>'expected_improvement'</code> <code>jitter</code> <code>float</code> <p>positive value to make the \"expected_improvement\" acquisition more explorative.</p> <code>0.1</code> Source code in <code>black_it/samplers/gaussian_process.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    max_deduplication_passes: int = 5,\n    candidate_pool_size: int | None = None,\n    optimize_restarts: int = 5,\n    acquisition: str = \"expected_improvement\",\n    jitter: float = 0.1,\n) -&gt; None:\n    \"\"\"Initialize the sampler.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n        max_deduplication_passes: the maximum number of deduplication passes that are made\n        candidate_pool_size: number of randomly sampled points on which the random forest predictions are evaluated\n        optimize_restarts: number of independent random trials of the optimization of the GP hyperparameters\n        acquisition: type of acquisition function, it can be 'expected_improvement' of simply 'mean'\n        jitter: positive value to make the \"expected_improvement\" acquisition more explorative.\n    \"\"\"\n    self._validate_acquisition(acquisition)\n\n    super().__init__(\n        batch_size,\n        random_state,\n        max_deduplication_passes,\n        candidate_pool_size,\n    )\n    self.optimize_restarts = optimize_restarts\n    self.acquisition = acquisition\n    self.jitter = jitter\n    self._gpmodel: GaussianProcessRegressor | None = None\n    self._fmin: np.double | float | None = None\n</code></pre>"},{"location":"samplers/#black_it.samplers.gaussian_process.GaussianProcessSampler.fit","title":"<code>fit(self, X, y)</code>","text":"<p>Fit a gaussian process surrogate model.</p> Source code in <code>black_it/samplers/gaussian_process.py</code> <pre><code>def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; None:  # noqa: N803\n    \"\"\"Fit a gaussian process surrogate model.\"\"\"\n    y = np.atleast_2d(y).T\n\n    if X.shape[0] &gt; _BIG_DATASET_SIZE_WARNING_THRESHOLD:\n        warnings.warn(  # noqa: B028\n            \"Standard GP evaluations can be expensive for large datasets, consider implementing a sparse GP\",\n            RuntimeWarning,\n        )\n\n    # initialize GP class from scikit-learn with a Matern kernel\n    kernel = kernels.Matern(length_scale=1, length_scale_bounds=(1e-5, 1e5), nu=2.5)\n\n    noise_var = y.var() * 0.01\n\n    self._gpmodel = GaussianProcessRegressor(\n        kernel=kernel,\n        alpha=noise_var,\n        n_restarts_optimizer=self.optimize_restarts,\n        optimizer=\"fmin_l_bfgs_b\",\n        random_state=self._get_random_seed(),\n    )\n\n    self._gpmodel.fit(X, y)\n\n    # store minimum\n    if self.acquisition == \"expected_improvement\":\n        m, _ = self._predict_mean_std(X)\n        self._fmin = np.min(m)\n</code></pre>"},{"location":"samplers/#black_it.samplers.gaussian_process.GaussianProcessSampler.get_quantiles","title":"<code>get_quantiles(acquisition_par, fmin, m, s)</code>  <code>staticmethod</code>","text":"<p>Quantiles of the Gaussian distribution useful to determine the acquisition function values.</p> <p>Parameters:</p> Name Type Description Default <code>acquisition_par</code> <code>float</code> <p>parameter of the acquisition function</p> required <code>fmin</code> <code>float</code> <p>current minimum.</p> required <code>m</code> <code>NDArray[np.float64]</code> <p>vector of means.</p> required <code>s</code> <code>NDArray[np.float64]</code> <p>vector of standard deviations.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]</code> <p>the quantiles.</p> Source code in <code>black_it/samplers/gaussian_process.py</code> <pre><code>@staticmethod\ndef get_quantiles(\n    acquisition_par: float,\n    fmin: float,\n    m: NDArray[np.float64],\n    s: NDArray[np.float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:\n    \"\"\"Quantiles of the Gaussian distribution useful to determine the acquisition function values.\n\n    Args:\n        acquisition_par: parameter of the acquisition function\n        fmin: current minimum.\n        m: vector of means.\n        s: vector of standard deviations.\n\n    Returns:\n        the quantiles.\n    \"\"\"\n    # remove values of variance that are too small\n    s[s &lt; _SMALL_VARIANCE_VALUES] = _SMALL_VARIANCE_VALUES\n\n    u: NDArray[np.float64] = (fmin - m - acquisition_par) / s\n    phi: NDArray[np.float64] = np.exp(-0.5 * u**2) / np.sqrt(2 * np.pi)\n    Phi: NDArray[np.float64] = 0.5 * erfc(-u / np.sqrt(2))  # noqa: N806\n\n    return phi, Phi, u\n</code></pre>"},{"location":"samplers/#black_it.samplers.gaussian_process.GaussianProcessSampler.predict","title":"<code>predict(self, X)</code>","text":"<p>Predict using a gaussian process surrogate model.</p> Source code in <code>black_it/samplers/gaussian_process.py</code> <pre><code>def predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:  # noqa: N803\n    \"\"\"Predict using a gaussian process surrogate model.\"\"\"\n    # predict mean or expected improvement on the full sample set\n    if self.acquisition == _AcquisitionTypes.EI.value:\n        # minus sign needed for subsequent sorting\n        candidates_score = -self._predict_EI(X, self.jitter)\n    else:  # acquisition is \"mean\"\n        candidates_score = self._predict_mean_std(X)[0]\n\n    return candidates_score\n</code></pre>"},{"location":"samplers/#black_it.samplers.random_forest.RandomForestSampler.criterion","title":"<code>criterion: str</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the criterion.</p>"},{"location":"samplers/#black_it.samplers.random_forest.RandomForestSampler.n_classes","title":"<code>n_classes: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the number of classes.</p>"},{"location":"samplers/#black_it.samplers.random_forest.RandomForestSampler.n_estimators","title":"<code>n_estimators: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the number of estimators.</p>"},{"location":"samplers/#black_it.samplers.random_forest.RandomForestSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, max_deduplication_passes=5, candidate_pool_size=None, n_estimators=500, criterion='gini', n_classes=10)</code>  <code>special</code>","text":"<p>Random forest sampling.</p> <p>Note: this class makes use of sklearn.ensemble.RandomForestClassifier.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the random state of the sampler, fixing this number the sampler behaves deterministically</p> <code>None</code> <code>max_deduplication_passes</code> <code>int</code> <p>the maximum number of deduplication passes</p> <code>5</code> <code>candidate_pool_size</code> <code>int | None</code> <p>number of randomly sampled points on which the random forest predictions are evaluated</p> <code>None</code> <code>n_estimators</code> <code>int</code> <p>number of trees in the forest</p> <code>500</code> <code>criterion</code> <code>str</code> <p>the function to measure the quality of a split.</p> <code>'gini'</code> <code>n_classes</code> <code>int</code> <p>the number of classes used in the random forest. The classes are selected as the quantiles of the distribution of loss values.</p> <code>10</code> Source code in <code>black_it/samplers/random_forest.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    max_deduplication_passes: int = 5,\n    candidate_pool_size: int | None = None,\n    n_estimators: int = 500,\n    criterion: str = \"gini\",\n    n_classes: int = 10,\n) -&gt; None:\n    \"\"\"Random forest sampling.\n\n    Note: this class makes use of sklearn.ensemble.RandomForestClassifier.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n        max_deduplication_passes: the maximum number of deduplication passes\n        candidate_pool_size: number of randomly sampled points on which the random forest predictions are evaluated\n        n_estimators: number of trees in the forest\n        criterion: the function to measure the quality of a split.\n        n_classes: the number of classes used in the random forest. The classes are selected as the quantiles\n            of the distribution of loss values.\n    \"\"\"\n    _assert(\n        n_classes &gt; _MIN_RF_CLASSES,\n        \"'n_classes' should be at least 2 to provide meaningful results\",\n    )\n\n    super().__init__(\n        batch_size,\n        random_state,\n        max_deduplication_passes,\n        candidate_pool_size,\n    )\n\n    self._n_estimators = n_estimators\n    self._criterion = criterion\n    self._n_classes = n_classes\n    self._classifier: RandomForestClassifier | None = None\n</code></pre>"},{"location":"samplers/#black_it.samplers.random_forest.RandomForestSampler.fit","title":"<code>fit(self, X, y)</code>","text":"<p>Fit a random forest surrogate model.</p> Source code in <code>black_it/samplers/random_forest.py</code> <pre><code>def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; None:  # noqa: N803\n    \"\"\"Fit a random forest surrogate model.\"\"\"\n    # Train surrogate\n\n    (\n        X,  # noqa: N806\n        y_cat,\n        _existing_points_quantiles,\n    ) = self.prepare_data_for_classifier(X, y, self.n_classes)\n\n    self._classifier = RandomForestClassifier(\n        n_estimators=self.n_estimators,\n        criterion=self.criterion,\n        n_jobs=-1,\n        random_state=self._get_random_seed(),\n    )\n    self._classifier.fit(X, y_cat)\n</code></pre>"},{"location":"samplers/#black_it.samplers.random_forest.RandomForestSampler.predict","title":"<code>predict(self, X)</code>","text":"<p>Predict using a random forest surrogate model.</p> Source code in <code>black_it/samplers/random_forest.py</code> <pre><code>def predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:  # noqa: N803\n    \"\"\"Predict using a random forest surrogate model.\"\"\"\n    # Predict quantiles\n    self._classifier = cast(\"RandomForestClassifier\", self._classifier)\n    predicted_points_quantiles: NDArray[np.float64] = self._classifier.predict(X)\n\n    return predicted_points_quantiles\n</code></pre>"},{"location":"samplers/#black_it.samplers.random_forest.RandomForestSampler.prepare_data_for_classifier","title":"<code>prepare_data_for_classifier(existing_points, existing_losses, num_bins)</code>  <code>staticmethod</code>","text":"<p>Prepare data for the classifier.</p> <p>Parameters:</p> Name Type Description Default <code>existing_points</code> <code>NDArray[np.float64]</code> <p>the parameters already sampled</p> required <code>existing_losses</code> <code>NDArray[np.float64]</code> <p>the loss corresponding to the sampled parameters</p> required <code>num_bins</code> <code>int</code> <p>the number of bins</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[np.float64], NDArray[np.int64], NDArray[np.float64]]</code> <p>A triple (x, y, quantiles), where     - x is the vector of training data     - y is the vector of targets     - the quantiles</p> Source code in <code>black_it/samplers/random_forest.py</code> <pre><code>@staticmethod\ndef prepare_data_for_classifier(\n    existing_points: NDArray[np.float64],\n    existing_losses: NDArray[np.float64],\n    num_bins: int,\n) -&gt; tuple[NDArray[np.float64], NDArray[np.int64], NDArray[np.float64]]:\n    \"\"\"Prepare data for the classifier.\n\n    Args:\n        existing_points: the parameters already sampled\n        existing_losses: the loss corresponding to the sampled parameters\n        num_bins: the number of bins\n\n    Returns:\n        A triple (x, y, quantiles), where\n            - x is the vector of training data\n            - y is the vector of targets\n            - the quantiles\n    \"\"\"\n    x: NDArray[np.float64] = existing_points\n    y: NDArray[np.float64] = existing_losses\n\n    cutoffs: NDArray[np.float64] = np.linspace(0, 1, num_bins + 1)\n    quantiles: NDArray[np.float64] = np.zeros(num_bins + 1)\n\n    for i in range(num_bins - 1):\n        quantiles[i + 1] = np.quantile(y, cutoffs[i + 1])\n\n    quantiles[-1] = np.max(y)\n\n    y_cat: NDArray[np.int64] = np.digitize(y, quantiles, right=True)\n    y_cat = y_cat - 1\n\n    return x, y_cat, quantiles\n</code></pre>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.alpha","title":"<code>alpha: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the alpha regularisation parameter.</p>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.colsample_bytree","title":"<code>colsample_bytree: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the colsample_bytree parameter.</p>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.learning_rate","title":"<code>learning_rate: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the learning rate.</p>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.max_depth","title":"<code>max_depth: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the maximum tree depth.</p>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.n_estimators","title":"<code>n_estimators: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the number of estimators.</p>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, max_deduplication_passes=5, candidate_pool_size=None, colsample_bytree=0.3, learning_rate=0.1, max_depth=5, alpha=1.0, n_estimators=10)</code>  <code>special</code>","text":"<p>Sampler based on a xgboost surrogate model of the loss function.</p> <p>this class makes use of the xgboost library, for more information on the XGBoost parameters</p> <p>visit https://xgboost.readthedocs.io/en/stable/parameter.html.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the random state of the sampler, fixing this number the sampler behaves deterministically</p> <code>None</code> <code>max_deduplication_passes</code> <code>int</code> <p>the maximum number of deduplication passes</p> <code>5</code> <code>candidate_pool_size</code> <code>int | None</code> <p>number of randomly sampled points on which the random forest predictions are evaluated</p> <code>None</code> <code>colsample_bytree</code> <code>float</code> <p>subsample ratio of columns when constructing each tree</p> <code>0.3</code> <code>learning_rate</code> <code>float</code> <p>the learning rate</p> <code>0.1</code> <code>max_depth</code> <code>int</code> <p>maximum depth of XGBoost trees</p> <code>5</code> <code>alpha</code> <code>float</code> <p>L1 regularization term on weights</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>number of estimators</p> <code>10</code> <p>References</p> <p>Lamperti, Roventini, and Sani, \"Agent-based model calibration using machine learning surrogates\"</p> Source code in <code>black_it/samplers/xgboost.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    max_deduplication_passes: int = 5,\n    candidate_pool_size: int | None = None,\n    colsample_bytree: float = 0.3,\n    learning_rate: float = 0.1,\n    max_depth: int = 5,\n    alpha: float = 1.0,\n    n_estimators: int = 10,\n) -&gt; None:\n    \"\"\"Sampler based on a xgboost surrogate model of the loss function.\n\n    Note: this class makes use of the xgboost library, for more information on the XGBoost parameters\n        visit https://xgboost.readthedocs.io/en/stable/parameter.html.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n        max_deduplication_passes: the maximum number of deduplication passes\n        candidate_pool_size: number of randomly sampled points on which the random forest predictions are evaluated\n        colsample_bytree: subsample ratio of columns when constructing each tree\n        learning_rate: the learning rate\n        max_depth: maximum depth of XGBoost trees\n        alpha: L1 regularization term on weights\n        n_estimators: number of estimators\n\n    References:\n        Lamperti, Roventini, and Sani, \"Agent-based model calibration using machine learning surrogates\"\n    \"\"\"\n    super().__init__(\n        batch_size,\n        random_state,\n        max_deduplication_passes,\n        candidate_pool_size,\n    )\n\n    self._colsample_bytree = colsample_bytree\n    self._learning_rate = learning_rate\n    self._max_depth = max_depth\n    self._alpha = alpha\n    self._n_estimators = n_estimators\n    self._xg_regressor: xgb.XGBRegressor | None = None\n</code></pre>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.fit","title":"<code>fit(self, X, y)</code>","text":"<p>Fit a xgboost surrogate model.</p> Source code in <code>black_it/samplers/xgboost.py</code> <pre><code>def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -&gt; None:  # noqa: N803\n    \"\"\"Fit a xgboost surrogate model.\"\"\"\n    # prepare data\n    y = self._clip_losses(y)\n    _ = xgb.DMatrix(data=X, label=y)\n\n    # train surrogate\n    self._xg_regressor = xgb.XGBRegressor(\n        objective=\"reg:squarederror\",  # original: objective ='reg:linear',\n        colsample_bytree=self.colsample_bytree,\n        learning_rate=self.learning_rate,\n        max_depth=self.max_depth,  # original: 5\n        alpha=self.alpha,\n        n_estimators=self.n_estimators,\n        random_state=self._get_random_seed(),\n    )  # original: 10\n\n    self._xg_regressor.fit(X, y)\n</code></pre>"},{"location":"samplers/#black_it.samplers.xgboost.XGBoostSampler.predict","title":"<code>predict(self, X)</code>","text":"<p>Predict using a xgboost surrogate model.</p> Source code in <code>black_it/samplers/xgboost.py</code> <pre><code>def predict(self, X: NDArray[np.float64]) -&gt; NDArray[np.float64]:  # noqa: N803\n    \"\"\"Predict using a xgboost surrogate model.\"\"\"\n    # predict over large pool of candidates\n    _ = xgb.DMatrix(data=X)\n\n    self._xg_regressor = cast(\"xgb.XGBRegressor\", self._xg_regressor)\n    return self._xg_regressor.predict(X)\n</code></pre>"},{"location":"samplers/#black_it.samplers.particle_swarm.ParticleSwarmSampler.c1","title":"<code>c1: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the c1 constant.</p>"},{"location":"samplers/#black_it.samplers.particle_swarm.ParticleSwarmSampler.c2","title":"<code>c2: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the c2 constant.</p>"},{"location":"samplers/#black_it.samplers.particle_swarm.ParticleSwarmSampler.inertia","title":"<code>inertia: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the inertia weight.</p>"},{"location":"samplers/#black_it.samplers.particle_swarm.ParticleSwarmSampler.is_set_up","title":"<code>is_set_up: bool</code>  <code>property</code> <code>readonly</code>","text":"<p>Return true iff the sampler is already set up.</p>"},{"location":"samplers/#black_it.samplers.particle_swarm.ParticleSwarmSampler.__init__","title":"<code>__init__(self, batch_size, random_state=None, inertia=0.9, c1=0.1, c2=0.1, global_minimum_across_samplers=False)</code>  <code>special</code>","text":"<p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the number of points sampled every time the sampler is called</p> required <code>random_state</code> <code>int | None</code> <p>the random state of the sampler, fixing this number the sampler behaves deterministically</p> <code>None</code> <code>inertia</code> <code>float</code> <p>the inertia of the particles' motion</p> <code>0.9</code> <code>c1</code> <code>float</code> <p>first acceleration constant</p> <code>0.1</code> <code>c2</code> <code>float</code> <p>second acceleration constant</p> <code>0.1</code> <code>global_minimum_across_samplers</code> <code>bool</code> <p>if True, the global minimum attractor of the particles' dynamics is computed taking into consideration also parameters sampled by other samplers, default is False</p> <code>False</code> Source code in <code>black_it/samplers/particle_swarm.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    random_state: int | None = None,\n    inertia: float = 0.9,\n    c1: float = 0.1,\n    c2: float = 0.1,\n    global_minimum_across_samplers: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the sampler.\n\n    Args:\n        batch_size: the number of points sampled every time the sampler is called\n        random_state: the random state of the sampler, fixing this number the sampler behaves deterministically\n        inertia: the inertia of the particles' motion\n        c1: first acceleration constant\n        c2: second acceleration constant\n        global_minimum_across_samplers: if True, the global minimum attractor of the particles' dynamics is computed\n            taking into consideration also parameters sampled by other samplers, default is False\n    \"\"\"\n    # max_duplication_passes must be zero because the sampler is stateful\n    super().__init__(\n        batch_size,\n        random_state=random_state,\n        max_deduplication_passes=0,\n    )\n\n    # The batch size is the number of sampled parameters per iteration. In a Black-it sampler, each call to\n    # sample_batch represent an iteration of the particle swarm sampler, so it seems natural to set the number of\n    # particles to the batch size, as at each iteration sample_batch returns the current positions of the\n    # particles.\n    self.nb_particles = batch_size\n\n    self._inertia = positive_float(inertia)\n    self._c1 = positive_float(c1)\n    self._c2 = positive_float(c2)\n    self._global_minimum_across_samplers = global_minimum_across_samplers\n\n    # all current particle positions; shape=(nb_particles, space dimensions)\n    self._curr_particle_positions: NDArray | None = None\n    # all current particle velocities; shape=(nb_particles, space dimensions)\n    self._curr_particle_velocities: NDArray | None = None\n    # best particle positions, i.e. ; shape=(nb_particles, space dimensions)\n    self._best_particle_positions: NDArray | None = None\n    # losses of the best positions\n    self._best_position_losses: NDArray | None = None\n    # particle id of the global best particle position\n    self._global_best_particle_id: int | None = None\n\n    # best point in parameter space - could be the best across samplers\n    self._best_point: NDArray | None = None\n\n    self._previous_batch_index_start: int | None = None\n</code></pre>"},{"location":"samplers/#black_it.samplers.particle_swarm.ParticleSwarmSampler.reset","title":"<code>reset(self)</code>","text":"<p>Reset the sampler.</p> Source code in <code>black_it/samplers/particle_swarm.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the sampler.\"\"\"\n    self._curr_particle_positions = None\n    self._curr_particle_velocities = None\n    self._best_particle_positions = None\n    self._best_position_losses = None\n    self._global_best_particle_id = None\n    self._previous_batch_index_start = None\n    _assert(\n        not self.is_set_up,\n        error_message=\"reset call did not work, sampler still set up\",\n        exception_class=RuntimeError,\n    )\n</code></pre>"},{"location":"samplers/#black_it.samplers.particle_swarm.ParticleSwarmSampler.sample_batch","title":"<code>sample_batch(self, batch_size, search_space, existing_points, existing_losses)</code>","text":"<p>Sample a batch of parameters.</p> Source code in <code>black_it/samplers/particle_swarm.py</code> <pre><code>def sample_batch(\n    self,\n    batch_size: int,  # noqa: ARG002\n    search_space: SearchSpace,\n    existing_points: NDArray[np.float64],\n    existing_losses: NDArray[np.float64],\n) -&gt; NDArray[np.float64]:\n    \"\"\"Sample a batch of parameters.\"\"\"\n    if not self.is_set_up:\n        self._set_up(search_space.dims)\n        self._previous_batch_index_start = len(existing_points)\n        return digitize_data(\n            cast(\"NDArray[np.float64]\", self._best_particle_positions),\n            search_space.param_grid,\n        )\n\n    self._update_best(existing_points, existing_losses)\n    self._do_step()\n\n    p_bounds: NDArray[np.float64] = search_space.parameters_bounds\n    sampled_points = p_bounds[0] + self._curr_particle_positions * (p_bounds[1] - p_bounds[0])\n    self._previous_batch_index_start = len(existing_points)\n\n    return digitize_data(sampled_points, search_space.param_grid)\n</code></pre>"},{"location":"schedulers/","title":"Schedulers","text":"<p>BaseScheduler interface.</p> <p>This is the base class for all schedulers.</p> Source code in <code>black_it/schedulers/base.py</code> <pre><code>class BaseScheduler(BaseSeedable, ABC):\n    \"\"\"BaseScheduler interface.\n\n    This is the base class for all schedulers.\n    \"\"\"\n\n    def __init__(\n        self,\n        samplers: Sequence[BaseSampler],\n        random_state: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            samplers: the list of samplers to be scheduled\n            random_state: the random seed for the scheduler behaviour\n        \"\"\"\n        # need to set __samplers first because _set_random_state requires samplers to be set\n        self._samplers = tuple(samplers)\n        BaseSeedable.__init__(self, random_state)\n\n    @property\n    def samplers(self) -&gt; tuple[BaseSampler, ...]:\n        \"\"\"Get the sequence of samplers.\"\"\"\n        return self._samplers\n\n    def _set_random_state(self, random_state: int | None) -&gt; None:\n        \"\"\"Set the random state (private use).\"\"\"\n        super()._set_random_state(random_state)\n        for sampler in self.samplers:\n            sampler.random_state = self._get_random_seed()\n\n    def start_session(self) -&gt; None:\n        \"\"\"Set up the scheduler for a new session.\n\n        The default is a no-op.\n        \"\"\"\n\n    @abstractmethod\n    def get_next_sampler(self) -&gt; BaseSampler:\n        \"\"\"Get the sampler to use for the next batch.\"\"\"\n\n    @abstractmethod\n    def update(\n        self,\n        batch_id: int,\n        new_params: NDArray[np.float64],\n        new_losses: NDArray[np.float64],\n        new_simulated_data: NDArray[np.float64],\n    ) -&gt; None:\n        \"\"\"Update the state of the scheduler after each batch.\n\n        Args:\n            batch_id: the batch id of the . Must be an integer equal or greater than 0.\n            new_params: the new set of parameters sampled in this batch.\n            new_losses: the new set of losses corresponding to the batch.\n            new_simulated_data: the new set of simulated data, one for each sampled parameter.\n        \"\"\"\n\n    def end_session(self) -&gt; None:\n        \"\"\"Tear down the scheduler at the end of the session.\n\n        The default is a no-op.\n        \"\"\"\n\n    @contextlib.contextmanager\n    def session(self) -&gt; Generator[None, None, None]:\n        \"\"\"Start the session of the scheduler with a context manager.\"\"\"\n        self.start_session()\n        yield\n        self.end_session()\n</code></pre> <p>This class implement a simple round-robin sampler scheduler.</p> <p>The round-robin scheduler takes in input a list of samplers [S_0, S_1, ..., S_{n-1}],   and, at batch i, it proposes the (i % n)-th sampler.</p> Source code in <code>black_it/schedulers/round_robin.py</code> <pre><code>class RoundRobinScheduler(BaseScheduler):\n    \"\"\"This class implement a simple round-robin sampler scheduler.\n\n    The round-robin scheduler takes in input a list of samplers [S_0, S_1, ..., S_{n-1}],\n      and, at batch i, it proposes the (i % n)-th sampler.\n    \"\"\"\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        *args,  # noqa: ANN002\n        **kwargs,  # noqa: ANN003\n    ) -&gt; None:\n        \"\"\"Initialize the round-robin scheduler.\"\"\"\n        super().__init__(*args, **kwargs)\n\n        self._batch_id = 0\n\n    def get_next_sampler(self) -&gt; BaseSampler:\n        \"\"\"Get the next sampler.\"\"\"\n        return self.samplers[self._batch_id % len(self.samplers)]\n\n    def update(\n        self,\n        batch_id: int,  # noqa: ARG002\n        new_params: NDArray[np.float64],  # noqa: ARG002\n        new_losses: NDArray[np.float64],  # noqa: ARG002\n        new_simulated_data: NDArray[np.float64],  # noqa: ARG002\n    ) -&gt; None:\n        \"\"\"Update the state of the scheduler after each batch.\"\"\"\n        self._batch_id += 1\n</code></pre> <p>This class implement a RL-based scheduler.</p> <p>It is agnostic wrt the RL algorithm being used.</p> Source code in <code>black_it/schedulers/rl/rl_scheduler.py</code> <pre><code>class RLScheduler(BaseScheduler):\n    \"\"\"This class implement a RL-based scheduler.\n\n    It is agnostic wrt the RL algorithm being used.\n    \"\"\"\n\n    def __init__(\n        self,\n        samplers: Sequence[BaseSampler],\n        agent: Agent,\n        env: CalibrationEnv,\n        random_state: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the scheduler.\"\"\"\n        self._original_samplers = samplers\n        new_samplers, self._halton_sampler_id = self._add_or_get_bootstrap_sampler(\n            samplers,\n        )\n\n        self._agent = agent\n        self._env = env\n\n        super().__init__(new_samplers, random_state)\n\n        self._in_queue: Queue = self._env._out_queue  # noqa: SLF001\n        self._out_queue: Queue = self._env._in_queue  # noqa: SLF001\n\n        self._best_param: float | None = None\n        self._best_loss: float | None = None\n\n        self._agent_thread: threading.Thread | None = None\n        self._stopped: bool = True\n\n    def _set_random_state(self, random_state: int | None) -&gt; None:\n        \"\"\"Set the random state (private use).\"\"\"\n        super()._set_random_state(random_state)\n        for sampler in self.samplers:\n            sampler.random_state = self._get_random_seed()\n        self._agent.random_state = self._get_random_seed()\n        self._env.reset(seed=self._get_random_seed())\n\n    @classmethod\n    def _add_or_get_bootstrap_sampler(\n        cls,\n        samplers: Sequence[BaseSampler],\n    ) -&gt; tuple[Sequence[BaseSampler], int]:\n        \"\"\"Add or retrieve a sampler for bootstrapping.\n\n        Many samplers do require some \"bootstrapping\" of the calibration process, i.e. a set of parameters\n          whose loss has been already evaluated, e.g. samplers based on ML surrogates or on evolutionary approaches.\n          Therefore, this scheduler must guarantee that the first proposed sampler is one that does not need previous\n          model evaluations in input. One of such samplers is the Halton sampler\n\n        Therefore, this function checks that the HaltonSampler is present in the set of samplers. If so, it returns\n          the same set of samplers, and the index corresponding to that sampler in the sequence. Otherwise, a new\n          instance of HaltonSampler is added to the list as first element.\n\n        Args:\n            samplers: the list of available samplers\n\n        Returns:\n            The pair (new_samplers, halton_sampler_id).\n        \"\"\"\n        sampler_types = {type(s): i for i, s in enumerate(samplers)}\n        if HaltonSampler in sampler_types:\n            return samplers, sampler_types[HaltonSampler]\n\n        new_sampler = HaltonSampler(batch_size=1)\n        return tuple(list(samplers) + cast(\"list[BaseSampler]\", [new_sampler])), len(\n            samplers,\n        )\n\n    def _train(self) -&gt; None:\n        \"\"\"Run the training loop.\"\"\"\n        state = self._env.reset()\n        while not self._stopped:\n            # Get the action chosen by the agent\n            action = self._agent.policy(state)\n            # Interact with the environment\n            next_state, reward, _, _, _ = self._env.step(action)\n            # Learn from interaction\n            self._agent.learn(state, action, reward, next_state)\n            state = next_state\n\n    def start_session(self) -&gt; None:\n        \"\"\"Set up the scheduler for a new session.\"\"\"\n        if not self._stopped:\n            msg = \"cannot start session: the session has already started\"\n            raise ValueError(msg)\n        self._stopped = False\n        self._agent_thread = threading.Thread(target=self._train)\n        self._agent_thread.start()\n\n    def get_next_sampler(self) -&gt; BaseSampler:\n        \"\"\"Get the next sampler.\"\"\"\n        if self._best_loss is None:\n            # first call, return halton sampler\n            return self.samplers[self._halton_sampler_id]\n        chosen_sampler_id = self._in_queue.get()\n        return self.samplers[chosen_sampler_id]\n\n    def update(\n        self,\n        batch_id: int,  # noqa: ARG002\n        new_params: NDArray[np.float64],\n        new_losses: NDArray[np.float64],\n        new_simulated_data: NDArray[np.float64],  # noqa: ARG002\n    ) -&gt; None:\n        \"\"\"Update the RL scheduler.\"\"\"\n        best_new_loss = float(np.min(new_losses))\n        if self._best_loss is None:\n            self._best_loss = best_new_loss\n            self._best_param = new_params[np.argmin(new_losses)]\n            self._env._curr_best_loss = best_new_loss  # noqa: SLF001\n            return\n        if best_new_loss &lt; cast(\"float\", self._best_loss):\n            self._best_loss = best_new_loss\n            self._best_param = new_params[np.argmin(new_losses)]\n\n        self._out_queue.put((self._best_param, self._best_loss))\n\n    def end_session(self) -&gt; None:\n        \"\"\"Tear down the scheduler at the end of the session.\"\"\"\n        if self._stopped:\n            msg = \"cannot start session: the session has not started yet\"\n            raise ValueError(msg)\n        self._stopped = True\n        self._out_queue.put(None)\n        cast(\"threading.Thread\", self._agent_thread).join()\n</code></pre> <p>Implementation of a MAB eps-greedy algorithm.</p> Source code in <code>black_it/schedulers/rl/agents/epsilon_greedy.py</code> <pre><code>class MABEpsilonGreedy(Agent[int, int]):\n    \"\"\"Implementation of a MAB eps-greedy algorithm.\"\"\"\n\n    def __init__(\n        self,\n        n_actions: int,\n        alpha: float,\n        eps: float,\n        initial_values: float = 0.0,\n        random_state: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the agent object.\n\n        Args:\n            n_actions: the number of actions\n            alpha: the learning rate\n            eps: the epsilon parameter\n            initial_values: the initial value for the Q-function\n            random_state: the random state\n        \"\"\"\n        super().__init__(random_state=random_state)\n        self.n_actions = n_actions\n        self.actions_count = [0] * self.n_actions\n\n        self.Q = [initial_values] * self.n_actions\n        self.alpha = alpha\n        self.eps = eps\n        self.initial_values = initial_values\n\n    def get_step_size(self, action: int) -&gt; float:\n        \"\"\"Get the step size.\"\"\"\n        return 1 / self.actions_count[action] if self.alpha == -1 else self.alpha\n\n    def learn(\n        self,\n        state: int,  # noqa: ARG002\n        action: int,\n        reward: SupportsFloat,\n        next_state: int,  # noqa: ARG002\n    ) -&gt; None:\n        \"\"\"Learn from an agent-environment interaction timestep.\"\"\"\n        self.actions_count[action] += 1\n\n        step_size = self.get_step_size(action)\n\n        # do the learning\n        self.Q[action] += step_size * (cast(\"float\", reward) - self.Q[action])\n\n    def policy(self, _obs: int) -&gt; int:\n        \"\"\"Get the action for this observation.\"\"\"\n        best_action = np.argmax(self.Q)\n\n        random_e = self.random_generator.random()\n        if not random_e &lt; self.eps:\n            action = best_action\n        else:\n            options = np.arange(self.n_actions)\n            action = self.random_generator.choice(options, 1)[0]\n\n        return int(action)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the agent.\"\"\"\n        self.Q = [0.0] * self.n_actions\n        self.actions_count = [0] * self.n_actions\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.base.BaseScheduler.samplers","title":"<code>samplers: tuple[BaseSampler, ...]</code>  <code>property</code> <code>readonly</code>","text":"<p>Get the sequence of samplers.</p>"},{"location":"schedulers/#black_it.schedulers.base.BaseScheduler.__init__","title":"<code>__init__(self, samplers, random_state=None)</code>  <code>special</code>","text":"<p>Initialize the scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>samplers</code> <code>Sequence[BaseSampler]</code> <p>the list of samplers to be scheduled</p> required <code>random_state</code> <code>int | None</code> <p>the random seed for the scheduler behaviour</p> <code>None</code> Source code in <code>black_it/schedulers/base.py</code> <pre><code>def __init__(\n    self,\n    samplers: Sequence[BaseSampler],\n    random_state: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        samplers: the list of samplers to be scheduled\n        random_state: the random seed for the scheduler behaviour\n    \"\"\"\n    # need to set __samplers first because _set_random_state requires samplers to be set\n    self._samplers = tuple(samplers)\n    BaseSeedable.__init__(self, random_state)\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.base.BaseScheduler.end_session","title":"<code>end_session(self)</code>","text":"<p>Tear down the scheduler at the end of the session.</p> <p>The default is a no-op.</p> Source code in <code>black_it/schedulers/base.py</code> <pre><code>def end_session(self) -&gt; None:\n    \"\"\"Tear down the scheduler at the end of the session.\n\n    The default is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.base.BaseScheduler.get_next_sampler","title":"<code>get_next_sampler(self)</code>","text":"<p>Get the sampler to use for the next batch.</p> Source code in <code>black_it/schedulers/base.py</code> <pre><code>@abstractmethod\ndef get_next_sampler(self) -&gt; BaseSampler:\n    \"\"\"Get the sampler to use for the next batch.\"\"\"\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.base.BaseScheduler.session","title":"<code>session(self)</code>","text":"<p>Start the session of the scheduler with a context manager.</p> Source code in <code>black_it/schedulers/base.py</code> <pre><code>@contextlib.contextmanager\ndef session(self) -&gt; Generator[None, None, None]:\n    \"\"\"Start the session of the scheduler with a context manager.\"\"\"\n    self.start_session()\n    yield\n    self.end_session()\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.base.BaseScheduler.start_session","title":"<code>start_session(self)</code>","text":"<p>Set up the scheduler for a new session.</p> <p>The default is a no-op.</p> Source code in <code>black_it/schedulers/base.py</code> <pre><code>def start_session(self) -&gt; None:\n    \"\"\"Set up the scheduler for a new session.\n\n    The default is a no-op.\n    \"\"\"\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.base.BaseScheduler.update","title":"<code>update(self, batch_id, new_params, new_losses, new_simulated_data)</code>","text":"<p>Update the state of the scheduler after each batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>int</code> <p>the batch id of the . Must be an integer equal or greater than 0.</p> required <code>new_params</code> <code>NDArray[np.float64]</code> <p>the new set of parameters sampled in this batch.</p> required <code>new_losses</code> <code>NDArray[np.float64]</code> <p>the new set of losses corresponding to the batch.</p> required <code>new_simulated_data</code> <code>NDArray[np.float64]</code> <p>the new set of simulated data, one for each sampled parameter.</p> required Source code in <code>black_it/schedulers/base.py</code> <pre><code>@abstractmethod\ndef update(\n    self,\n    batch_id: int,\n    new_params: NDArray[np.float64],\n    new_losses: NDArray[np.float64],\n    new_simulated_data: NDArray[np.float64],\n) -&gt; None:\n    \"\"\"Update the state of the scheduler after each batch.\n\n    Args:\n        batch_id: the batch id of the . Must be an integer equal or greater than 0.\n        new_params: the new set of parameters sampled in this batch.\n        new_losses: the new set of losses corresponding to the batch.\n        new_simulated_data: the new set of simulated data, one for each sampled parameter.\n    \"\"\"\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.round_robin.RoundRobinScheduler.__init__","title":"<code>__init__(self, *args, **kwargs)</code>  <code>special</code>","text":"<p>Initialize the round-robin scheduler.</p> Source code in <code>black_it/schedulers/round_robin.py</code> <pre><code>def __init__(  # type: ignore[no-untyped-def]\n    self,\n    *args,  # noqa: ANN002\n    **kwargs,  # noqa: ANN003\n) -&gt; None:\n    \"\"\"Initialize the round-robin scheduler.\"\"\"\n    super().__init__(*args, **kwargs)\n\n    self._batch_id = 0\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.round_robin.RoundRobinScheduler.get_next_sampler","title":"<code>get_next_sampler(self)</code>","text":"<p>Get the next sampler.</p> Source code in <code>black_it/schedulers/round_robin.py</code> <pre><code>def get_next_sampler(self) -&gt; BaseSampler:\n    \"\"\"Get the next sampler.\"\"\"\n    return self.samplers[self._batch_id % len(self.samplers)]\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.round_robin.RoundRobinScheduler.update","title":"<code>update(self, batch_id, new_params, new_losses, new_simulated_data)</code>","text":"<p>Update the state of the scheduler after each batch.</p> Source code in <code>black_it/schedulers/round_robin.py</code> <pre><code>def update(\n    self,\n    batch_id: int,  # noqa: ARG002\n    new_params: NDArray[np.float64],  # noqa: ARG002\n    new_losses: NDArray[np.float64],  # noqa: ARG002\n    new_simulated_data: NDArray[np.float64],  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Update the state of the scheduler after each batch.\"\"\"\n    self._batch_id += 1\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.rl_scheduler.RLScheduler.__init__","title":"<code>__init__(self, samplers, agent, env, random_state=None)</code>  <code>special</code>","text":"<p>Initialize the scheduler.</p> Source code in <code>black_it/schedulers/rl/rl_scheduler.py</code> <pre><code>def __init__(\n    self,\n    samplers: Sequence[BaseSampler],\n    agent: Agent,\n    env: CalibrationEnv,\n    random_state: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize the scheduler.\"\"\"\n    self._original_samplers = samplers\n    new_samplers, self._halton_sampler_id = self._add_or_get_bootstrap_sampler(\n        samplers,\n    )\n\n    self._agent = agent\n    self._env = env\n\n    super().__init__(new_samplers, random_state)\n\n    self._in_queue: Queue = self._env._out_queue  # noqa: SLF001\n    self._out_queue: Queue = self._env._in_queue  # noqa: SLF001\n\n    self._best_param: float | None = None\n    self._best_loss: float | None = None\n\n    self._agent_thread: threading.Thread | None = None\n    self._stopped: bool = True\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.rl_scheduler.RLScheduler.end_session","title":"<code>end_session(self)</code>","text":"<p>Tear down the scheduler at the end of the session.</p> Source code in <code>black_it/schedulers/rl/rl_scheduler.py</code> <pre><code>def end_session(self) -&gt; None:\n    \"\"\"Tear down the scheduler at the end of the session.\"\"\"\n    if self._stopped:\n        msg = \"cannot start session: the session has not started yet\"\n        raise ValueError(msg)\n    self._stopped = True\n    self._out_queue.put(None)\n    cast(\"threading.Thread\", self._agent_thread).join()\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.rl_scheduler.RLScheduler.get_next_sampler","title":"<code>get_next_sampler(self)</code>","text":"<p>Get the next sampler.</p> Source code in <code>black_it/schedulers/rl/rl_scheduler.py</code> <pre><code>def get_next_sampler(self) -&gt; BaseSampler:\n    \"\"\"Get the next sampler.\"\"\"\n    if self._best_loss is None:\n        # first call, return halton sampler\n        return self.samplers[self._halton_sampler_id]\n    chosen_sampler_id = self._in_queue.get()\n    return self.samplers[chosen_sampler_id]\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.rl_scheduler.RLScheduler.start_session","title":"<code>start_session(self)</code>","text":"<p>Set up the scheduler for a new session.</p> Source code in <code>black_it/schedulers/rl/rl_scheduler.py</code> <pre><code>def start_session(self) -&gt; None:\n    \"\"\"Set up the scheduler for a new session.\"\"\"\n    if not self._stopped:\n        msg = \"cannot start session: the session has already started\"\n        raise ValueError(msg)\n    self._stopped = False\n    self._agent_thread = threading.Thread(target=self._train)\n    self._agent_thread.start()\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.rl_scheduler.RLScheduler.update","title":"<code>update(self, batch_id, new_params, new_losses, new_simulated_data)</code>","text":"<p>Update the RL scheduler.</p> Source code in <code>black_it/schedulers/rl/rl_scheduler.py</code> <pre><code>def update(\n    self,\n    batch_id: int,  # noqa: ARG002\n    new_params: NDArray[np.float64],\n    new_losses: NDArray[np.float64],\n    new_simulated_data: NDArray[np.float64],  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Update the RL scheduler.\"\"\"\n    best_new_loss = float(np.min(new_losses))\n    if self._best_loss is None:\n        self._best_loss = best_new_loss\n        self._best_param = new_params[np.argmin(new_losses)]\n        self._env._curr_best_loss = best_new_loss  # noqa: SLF001\n        return\n    if best_new_loss &lt; cast(\"float\", self._best_loss):\n        self._best_loss = best_new_loss\n        self._best_param = new_params[np.argmin(new_losses)]\n\n    self._out_queue.put((self._best_param, self._best_loss))\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.agents.epsilon_greedy.MABEpsilonGreedy.__init__","title":"<code>__init__(self, n_actions, alpha, eps, initial_values=0.0, random_state=None)</code>  <code>special</code>","text":"<p>Initialize the agent object.</p> <p>Parameters:</p> Name Type Description Default <code>n_actions</code> <code>int</code> <p>the number of actions</p> required <code>alpha</code> <code>float</code> <p>the learning rate</p> required <code>eps</code> <code>float</code> <p>the epsilon parameter</p> required <code>initial_values</code> <code>float</code> <p>the initial value for the Q-function</p> <code>0.0</code> <code>random_state</code> <code>int | None</code> <p>the random state</p> <code>None</code> Source code in <code>black_it/schedulers/rl/agents/epsilon_greedy.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int,\n    alpha: float,\n    eps: float,\n    initial_values: float = 0.0,\n    random_state: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize the agent object.\n\n    Args:\n        n_actions: the number of actions\n        alpha: the learning rate\n        eps: the epsilon parameter\n        initial_values: the initial value for the Q-function\n        random_state: the random state\n    \"\"\"\n    super().__init__(random_state=random_state)\n    self.n_actions = n_actions\n    self.actions_count = [0] * self.n_actions\n\n    self.Q = [initial_values] * self.n_actions\n    self.alpha = alpha\n    self.eps = eps\n    self.initial_values = initial_values\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.agents.epsilon_greedy.MABEpsilonGreedy.get_step_size","title":"<code>get_step_size(self, action)</code>","text":"<p>Get the step size.</p> Source code in <code>black_it/schedulers/rl/agents/epsilon_greedy.py</code> <pre><code>def get_step_size(self, action: int) -&gt; float:\n    \"\"\"Get the step size.\"\"\"\n    return 1 / self.actions_count[action] if self.alpha == -1 else self.alpha\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.agents.epsilon_greedy.MABEpsilonGreedy.learn","title":"<code>learn(self, state, action, reward, next_state)</code>","text":"<p>Learn from an agent-environment interaction timestep.</p> Source code in <code>black_it/schedulers/rl/agents/epsilon_greedy.py</code> <pre><code>def learn(\n    self,\n    state: int,  # noqa: ARG002\n    action: int,\n    reward: SupportsFloat,\n    next_state: int,  # noqa: ARG002\n) -&gt; None:\n    \"\"\"Learn from an agent-environment interaction timestep.\"\"\"\n    self.actions_count[action] += 1\n\n    step_size = self.get_step_size(action)\n\n    # do the learning\n    self.Q[action] += step_size * (cast(\"float\", reward) - self.Q[action])\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.agents.epsilon_greedy.MABEpsilonGreedy.policy","title":"<code>policy(self, _obs)</code>","text":"<p>Get the action for this observation.</p> Source code in <code>black_it/schedulers/rl/agents/epsilon_greedy.py</code> <pre><code>def policy(self, _obs: int) -&gt; int:\n    \"\"\"Get the action for this observation.\"\"\"\n    best_action = np.argmax(self.Q)\n\n    random_e = self.random_generator.random()\n    if not random_e &lt; self.eps:\n        action = best_action\n    else:\n        options = np.arange(self.n_actions)\n        action = self.random_generator.choice(options, 1)[0]\n\n    return int(action)\n</code></pre>"},{"location":"schedulers/#black_it.schedulers.rl.agents.epsilon_greedy.MABEpsilonGreedy.reset","title":"<code>reset(self)</code>","text":"<p>Reset the agent.</p> Source code in <code>black_it/schedulers/rl/agents/epsilon_greedy.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the agent.\"\"\"\n    self.Q = [0.0] * self.n_actions\n    self.actions_count = [0] * self.n_actions\n</code></pre>"},{"location":"search_space/","title":"Search space","text":"<p>A class that contains information on the search grid of explorable parameters.</p> Source code in <code>black_it/search_space.py</code> <pre><code>class SearchSpace:\n    \"\"\"A class that contains information on the search grid of explorable parameters.\"\"\"\n\n    def __init__(\n        self,\n        parameters_bounds: NDArray[np.float64] | list[list[float]],\n        parameters_precision: NDArray[np.float64] | list[float],\n        verbose: bool,\n    ) -&gt; None:\n        \"\"\"Initialize the SearchSpace object.\n\n        The values of parameters_bounds and parameters_precision parameters have\n        to satisfy the following constraints, otherwise an exception (subclass\n        of SearchSpaceError) is raised:\n\n        - parameters_bounds must be a two-elements array/list (or\n          BoundsNotOfSizeTwoError is raised)\n        - the two sub-arrays/lists of parameter_bounds must have the same length (or\n          BoundsOfDifferentLengthError is raised)\n        - parameters_precision array/list must have the same number of elements than\n          the two sub-arrays/lists in parameters_bounds (or BadPrecisionLengthError is\n          raised)\n        - lower bounds and upper bounds cannot have the same value (or\n          SameLowerAndUpperBoundError is raised)\n        - every lower bound must be lower than the corresponding upper bound (or\n          LowerBoundGreaterThanUpperBoundError)\n        - 0 is an invalid value for a precision (or PrecisionZeroError is\n          raised)\n        - any given parameter precision has to be strictly lower than the\n          allowed parameter span (or PrecisionGreaterThanBoundsRangeError is\n          raised)\n\n        Args:\n            parameters_bounds: lower and upper bounds of the parameters.\n            parameters_precision: resolution of the grid of parameters.\n            verbose: whether to print or not the information on the search space.\n        \"\"\"\n        SearchSpace._check_bounds(parameters_bounds, parameters_precision)\n\n        # The bounds we were given are well formed. Save them.\n        self._parameters_bounds = np.array(parameters_bounds)\n        self._parameters_precision = np.array(parameters_precision)\n\n        # Initialize search grid\n        self._param_grid: list[NDArray[np.float64]] = []\n        self._space_size = 1\n        for i in range(self.dims):\n            new_col: NDArray[np.float64] = np.arange(\n                parameters_bounds[0][i],\n                parameters_bounds[1][i] + 0.0000001,\n                parameters_precision[i],\n                dtype=np.float64,\n            )\n            self._param_grid.append(new_col)\n            self._space_size *= len(new_col)\n\n        if verbose:\n            print(\"\\n***\")\n            print(f\"Number of free params:       {self.dims}.\")\n            print(f\"Explorable param space size: {self.space_size}.\")\n            print(\"***\\n\")\n\n    @staticmethod\n    def _check_bounds(\n        parameters_bounds: NDArray[np.float64] | list[list[float]],\n        parameters_precision: NDArray[np.float64] | list[float],\n    ) -&gt; None:\n        \"\"\"Ensure parameter_bounds and parameter_precision have acceptable values.\n\n        This is an helper function for SearchSpace.__init__().\n\n        Args:\n            parameters_bounds: lower and upper bounds of the parameters\n            parameters_precision: resolution of the grid of parameters\n        \"\"\"\n        # ensure parameters_bounds is a two-elements array\n        if len(parameters_bounds) != 2:  # noqa: PLR2004\n            raise BoundsNotOfSizeTwoError(len(parameters_bounds))\n\n        # ensure the two sub-arrays of parameter_bounds (which are the min and\n        # max bounds for each parameter) have the same length\n        if len(parameters_bounds[0]) != len(parameters_bounds[1]):\n            raise BoundsOfDifferentLengthError(\n                len(parameters_bounds[0]),\n                len(parameters_bounds[1]),\n            )\n\n        # ensure parameters_precision array has as many elements as either one\n        # of parameters_bounds sub-array\n        if len(parameters_precision) != len(parameters_bounds[0]):\n            raise BadPrecisionLengthError(\n                len(parameters_precision),\n                len(parameters_bounds[0]),\n            )\n\n        for i, (lower_bound, upper_bound, precision) in enumerate(\n            zip(parameters_bounds[0], parameters_bounds[1], parameters_precision),\n        ):\n            # ensure lower bounds and upper bounds do not have the same value\n            if lower_bound == upper_bound:\n                raise SameLowerAndUpperBoundError(i, lower_bound)\n\n            # ensure each lower bound is lower than the corresponding upper one\n            if lower_bound &gt; upper_bound:\n                raise LowerBoundGreaterThanUpperBoundError(i, lower_bound, upper_bound)\n\n            # a precision of 0 is not meaningful\n            if precision == 0:\n                raise PrecisionZeroError(i)\n\n            # any given parameter precision has to be strictly lower than the\n            # allowed parameter span\n            if precision &gt; (upper_bound - lower_bound):\n                raise PrecisionGreaterThanBoundsRangeError(\n                    i,\n                    lower_bound,\n                    upper_bound,\n                    precision,\n                )\n\n    @property\n    def param_grid(self) -&gt; list[NDArray[np.float64]]:\n        \"\"\"Discretized parameter space containing all possible candidates for calibration.\"\"\"\n        return self._param_grid\n\n    @property\n    def parameters_bounds(self) -&gt; NDArray[np.float64]:\n        \"\"\"Two dimensional array containing lower and upper bounds for each parameter.\"\"\"\n        return self._parameters_bounds\n\n    @property\n    def parameters_precision(self) -&gt; NDArray[np.float64]:\n        \"\"\"One dimensional array containing the precisions for each parameter.\"\"\"\n        return self._parameters_precision\n\n    @property\n    def dims(self) -&gt; int:\n        \"\"\"Return the number of model parameters configured for calibration.\"\"\"\n        return len(self._parameters_precision)\n\n    @property\n    def space_size(self) -&gt; int:\n        \"\"\"Cardinality of the potential parameter space to be searched.\"\"\"\n        return self._space_size\n</code></pre> <p>Base class for the exceptions raised by SearchSpace when its construction fails.</p> <p>If you need to distinguish a specific error, please do not rely on parsing the error message, because it is considered unstable: catch instead the specific exception type you want to handle, and use the additional fields each subtype exposes (for example: BadPrecisionLengthError.bounds_length)</p> Source code in <code>black_it/search_space.py</code> <pre><code>class SearchSpaceError(ValueError):\n    \"\"\"Base class for the exceptions raised by SearchSpace when its construction fails.\n\n    If you need to distinguish a specific error, please do not rely on parsing\n    the error message, because it is considered unstable: catch instead the\n    specific exception type you want to handle, and use the additional fields\n    each subtype exposes (for example: BadPrecisionLengthError.bounds_length)\n    \"\"\"\n</code></pre> <p>Raised when the parameters_precision array has a different length than the bounds'.</p> <p>Attributes:</p> Name Type Description <code>precisions_length</code> <code>int</code> <p>number of elements of the precision subarray</p> <code>bounds_length</code> <code>int</code> <p>number of elements of the two bounds subbarrays</p> Source code in <code>black_it/search_space.py</code> <pre><code>class BadPrecisionLengthError(SearchSpaceError):\n    \"\"\"Raised when the parameters_precision array has a different length than the bounds'.\n\n    Attributes:\n        precisions_length (int): number of elements of the precision subarray\n        bounds_length (int): number of elements of the two bounds subbarrays\n    \"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        precisions_length: int,\n        bounds_length: int,\n    ) -&gt; None:\n        super().__init__(\n            f\"parameters_precision array has {precisions_length} elements. Its \"\n            f\"length, instead, has to be {bounds_length}, the same as the bounds'.\",\n        )\n        self.precisions_length = precisions_length\n        self.bounds_length = bounds_length\n</code></pre> <p>Raised when bounds are not a two-dimensional array.</p> <p>Attributes:</p> Name Type Description <code>count_bounds_subarrays</code> <code>int</code> <p>wrong number of subarrays the parameter_bounds array was made of. It will be different than 2.</p> Source code in <code>black_it/search_space.py</code> <pre><code>class BoundsNotOfSizeTwoError(SearchSpaceError):\n    \"\"\"Raised when bounds are not a two-dimensional array.\n\n    Attributes:\n        count_bounds_subarrays (int): wrong number of subarrays the\n            parameter_bounds array was made of. It will be different than 2.\n    \"\"\"\n\n    def __init__(self, count_bounds_subarrays: int) -&gt; None:  # noqa: D107\n        super().__init__(\n            f\"parameters_bounds must be a two dimensional array. This one has size {count_bounds_subarrays}.\",\n        )\n        self.count_bounds_subarrays = count_bounds_subarrays\n</code></pre> <p>Raised when the lower and upper bounds do not have the same number of elements.</p> <p>Attributes:</p> Name Type Description <code>lower_bounds_length</code> <code>int</code> <p>number of elements in the subarray 0. It will be different than upper_bounds_length</p> <code>upper_bounds_length</code> <code>int</code> <p>number of elements in the subarray 1. It will be different than lower_bounds_length</p> Source code in <code>black_it/search_space.py</code> <pre><code>class BoundsOfDifferentLengthError(SearchSpaceError):\n    \"\"\"Raised when the lower and upper bounds do not have the same number of elements.\n\n    Attributes:\n        lower_bounds_length (int): number of elements in the subarray 0. It will\n            be different than upper_bounds_length\n        upper_bounds_length (int): number of elements in the subarray 1. It will\n            be different than lower_bounds_length\n    \"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        lower_bounds_length: int,\n        upper_bounds_length: int,\n    ) -&gt; None:\n        super().__init__(\n            f\"parameters_bounds subarrays must be of the same length. Lower \"\n            f\"bounds length: {lower_bounds_length}, upper bounds length: \"\n            f\"{upper_bounds_length}.\",\n        )\n        self.lower_bounds_length = lower_bounds_length\n        self.upper_bounds_length = upper_bounds_length\n</code></pre> <p>Raised when the lower bound of a parameter is greater than its upper bound.</p> <p>Attributes:</p> Name Type Description <code>param_index</code> <code>int</code> <p>0-based index of the parameter presenting the error</p> <code>lower_bound</code> <code>float</code> <p>lower bound. It will be higher than upper bound</p> <code>upper_bound</code> <code>float</code> <p>upper bound. It will be lower than lower bound</p> Source code in <code>black_it/search_space.py</code> <pre><code>class LowerBoundGreaterThanUpperBoundError(SearchSpaceError):\n    \"\"\"Raised when the lower bound of a parameter is greater than its upper bound.\n\n    Attributes:\n        param_index (int): 0-based index of the parameter presenting the error\n        lower_bound (float): lower bound. It will be higher than upper bound\n        upper_bound (float): upper bound. It will be lower than lower bound\n    \"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        param_index: int,\n        lower_bound: float,\n        upper_bound: float,\n    ) -&gt; None:\n        super().__init__(\n            f\"Parameter {param_index}'s lower bound ({lower_bound}) must be \"\n            f\"lower than its upper bound ({upper_bound}).\",\n        )\n        self.param_index = param_index\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n</code></pre> <p>Raised when a parameter precision is set to 0.</p> <p>Attributes:</p> Name Type Description <code>param_index</code> <code>int</code> <p>0-based index of the parameter presenting the error</p> Source code in <code>black_it/search_space.py</code> <pre><code>class PrecisionZeroError(SearchSpaceError):\n    \"\"\"Raised when a parameter precision is set to 0.\n\n    Attributes:\n        param_index (int): 0-based index of the parameter presenting the error\n    \"\"\"\n\n    def __init__(self, param_index: int) -&gt; None:  # noqa: D107\n        super().__init__(f\"Parameter {param_index}'s precision cannot be zero.\")\n        self.param_index = param_index\n</code></pre> <p>Raised when the lower and the upper bound of a parameter have the same value.</p> <p>Attributes:</p> Name Type Description <code>param_index</code> <code>int</code> <p>0-based index of the parameter presenting the error</p> <code>bound_value</code> <code>float</code> <p>common value of the parameter bound</p> Source code in <code>black_it/search_space.py</code> <pre><code>class SameLowerAndUpperBoundError(SearchSpaceError):\n    \"\"\"Raised when the lower and the upper bound of a parameter have the same value.\n\n    Attributes:\n        param_index (int): 0-based index of the parameter presenting the error\n        bound_value (float): common value of the parameter bound\n    \"\"\"\n\n    def __init__(self, param_index: int, bound_value: float) -&gt; None:  # noqa: D107\n        super().__init__(\n            f\"Parameter {param_index}'s lower and upper bounds have the same \"\n            f\"value ({bound_value}). This calibrator cannot handle that. \"\n            f\"Please redefine externally your model in order to hardcode \"\n            f\"parameter {param_index} to {bound_value}.\",\n        )\n        self.param_index = param_index\n        self.bound_value = bound_value\n</code></pre>"},{"location":"search_space/#black_it.search_space.SearchSpace.dims","title":"<code>dims: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Return the number of model parameters configured for calibration.</p>"},{"location":"search_space/#black_it.search_space.SearchSpace.param_grid","title":"<code>param_grid: list[NDArray[np.float64]]</code>  <code>property</code> <code>readonly</code>","text":"<p>Discretized parameter space containing all possible candidates for calibration.</p>"},{"location":"search_space/#black_it.search_space.SearchSpace.parameters_bounds","title":"<code>parameters_bounds: NDArray[np.float64]</code>  <code>property</code> <code>readonly</code>","text":"<p>Two dimensional array containing lower and upper bounds for each parameter.</p>"},{"location":"search_space/#black_it.search_space.SearchSpace.parameters_precision","title":"<code>parameters_precision: NDArray[np.float64]</code>  <code>property</code> <code>readonly</code>","text":"<p>One dimensional array containing the precisions for each parameter.</p>"},{"location":"search_space/#black_it.search_space.SearchSpace.space_size","title":"<code>space_size: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Cardinality of the potential parameter space to be searched.</p>"},{"location":"search_space/#black_it.search_space.SearchSpace.__init__","title":"<code>__init__(self, parameters_bounds, parameters_precision, verbose)</code>  <code>special</code>","text":"<p>Initialize the SearchSpace object.</p> <p>The values of parameters_bounds and parameters_precision parameters have to satisfy the following constraints, otherwise an exception (subclass of SearchSpaceError) is raised:</p> <ul> <li>parameters_bounds must be a two-elements array/list (or   BoundsNotOfSizeTwoError is raised)</li> <li>the two sub-arrays/lists of parameter_bounds must have the same length (or   BoundsOfDifferentLengthError is raised)</li> <li>parameters_precision array/list must have the same number of elements than   the two sub-arrays/lists in parameters_bounds (or BadPrecisionLengthError is   raised)</li> <li>lower bounds and upper bounds cannot have the same value (or   SameLowerAndUpperBoundError is raised)</li> <li>every lower bound must be lower than the corresponding upper bound (or   LowerBoundGreaterThanUpperBoundError)</li> <li>0 is an invalid value for a precision (or PrecisionZeroError is   raised)</li> <li>any given parameter precision has to be strictly lower than the   allowed parameter span (or PrecisionGreaterThanBoundsRangeError is   raised)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>parameters_bounds</code> <code>NDArray[np.float64] | list[list[float]]</code> <p>lower and upper bounds of the parameters.</p> required <code>parameters_precision</code> <code>NDArray[np.float64] | list[float]</code> <p>resolution of the grid of parameters.</p> required <code>verbose</code> <code>bool</code> <p>whether to print or not the information on the search space.</p> required Source code in <code>black_it/search_space.py</code> <pre><code>def __init__(\n    self,\n    parameters_bounds: NDArray[np.float64] | list[list[float]],\n    parameters_precision: NDArray[np.float64] | list[float],\n    verbose: bool,\n) -&gt; None:\n    \"\"\"Initialize the SearchSpace object.\n\n    The values of parameters_bounds and parameters_precision parameters have\n    to satisfy the following constraints, otherwise an exception (subclass\n    of SearchSpaceError) is raised:\n\n    - parameters_bounds must be a two-elements array/list (or\n      BoundsNotOfSizeTwoError is raised)\n    - the two sub-arrays/lists of parameter_bounds must have the same length (or\n      BoundsOfDifferentLengthError is raised)\n    - parameters_precision array/list must have the same number of elements than\n      the two sub-arrays/lists in parameters_bounds (or BadPrecisionLengthError is\n      raised)\n    - lower bounds and upper bounds cannot have the same value (or\n      SameLowerAndUpperBoundError is raised)\n    - every lower bound must be lower than the corresponding upper bound (or\n      LowerBoundGreaterThanUpperBoundError)\n    - 0 is an invalid value for a precision (or PrecisionZeroError is\n      raised)\n    - any given parameter precision has to be strictly lower than the\n      allowed parameter span (or PrecisionGreaterThanBoundsRangeError is\n      raised)\n\n    Args:\n        parameters_bounds: lower and upper bounds of the parameters.\n        parameters_precision: resolution of the grid of parameters.\n        verbose: whether to print or not the information on the search space.\n    \"\"\"\n    SearchSpace._check_bounds(parameters_bounds, parameters_precision)\n\n    # The bounds we were given are well formed. Save them.\n    self._parameters_bounds = np.array(parameters_bounds)\n    self._parameters_precision = np.array(parameters_precision)\n\n    # Initialize search grid\n    self._param_grid: list[NDArray[np.float64]] = []\n    self._space_size = 1\n    for i in range(self.dims):\n        new_col: NDArray[np.float64] = np.arange(\n            parameters_bounds[0][i],\n            parameters_bounds[1][i] + 0.0000001,\n            parameters_precision[i],\n            dtype=np.float64,\n        )\n        self._param_grid.append(new_col)\n        self._space_size *= len(new_col)\n\n    if verbose:\n        print(\"\\n***\")\n        print(f\"Number of free params:       {self.dims}.\")\n        print(f\"Explorable param space size: {self.space_size}.\")\n        print(\"***\\n\")\n</code></pre>"},{"location":"simulator_interface/","title":"How to interface Black-it with your simulator","text":"<p>As already mentioned, the tool was designed with the ability to be customizable in each of its components.  Being able to use a custom model is even more vital to the usability of the tool, since one may want to plug  their own model in the tool and calibrate it.</p> <p>A possibility is to write a custom Python function and use it as explained here.  But one may have already written their own model simulator somewhere else and may reasonably expect to use it  with the calibrator.</p> <p>To do this, essentially a Python wrapper of the simulator must be written.  The interface is flexible and language agnostic, and it allows calibrating models built using standard Python ABM frameworks  such as Mesa, as well as custom models written directly in low level languages.</p> <p>To illustrate this, we will take a look  at the SIR model tutorial which makes use of a C++ simulator  run on a Docker container. Here the wrapper is defined as follows:</p> <pre><code>def SIR_docker(theta, N, rndSeed):\n    sim_params = {\n        \"agents\": 1000,\n        \"epochs\": N - 1,\n        \"beta\": theta[0],\n        \"gamma\": theta[1],\n        \"infectious-t0\": 10,\n        \"lattice-order\": 20,\n        \"rewire-probability\": 0.2,\n    }\n\n    res = simlib.execute_simulator(\"bancaditalia/abmsimulator\", sim_params)\n    ret = np.array([(x[\"susceptible\"], x[\"infectious\"], x[\"recovered\"]) for x in res])\n\n    return ret\n</code></pre> <p>To function correctly, the wrapper must have the interface shown in the example:</p> <ul> <li> <p>it must accept in input three parameters <code>(theta, N, rndSeed)</code>, which are respectively the parameter vector to be optimized, the length of the time series and a random seed (this last one is not used in the example)</p> </li> <li> <p>it must output a N \\times D array where N is the same as before and D is the vector dimension of the time series (in the SIR example it is D=3)</p> </li> </ul>"},{"location":"slack_stock_price_shock/","title":"Simple shock model","text":"<p>Here we will fit a very simple \"shock model\" to the stock price of Slack Technologies from the 27th of august 2020 to the 31st of march 2021. </p> <p>We will try to capture the shock induces by the spearing of rumors on the 25th of november 2020 that Salesforce was interested in buying Slack. More info can be found here https://www.cnbc.com/2020/11/25/slack-shares-jump-following-report-of-possible-salesforce-acquisition.html.</p> <pre><code># import standard libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>slack_shock_data = np.atleast_2d(np.loadtxt(\"data/slack_shock.txt\")).T\nplt.plot(slack_shock_data)\nplt.xlabel(\"days from 27-8-2020\")\nplt.ylabel(\"Slack stock price\")\n</code></pre> <pre>\n<code>Text(0, 0.5, 'Slack stock price')</code>\n</pre> <pre><code># a simple model of a simple step function with three parameters\ndef step_func(theta, N, seed=None):\n    # [tau, d1, d2]\n\n    e = np.zeros(N)\n    tau = int(theta[0])  # day of the sudden change in evaluation\n    d1, d2 = theta[1], theta[2]  # initial and final stock prices\n\n    e[0:tau] = d1\n    e[tau:] = d2\n\n    return np.atleast_2d(e).T\n</code></pre> <pre><code>plt.plot(step_func([20, 0, 1], 100))\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D at 0x7f05b2c2f1c0&gt;]</code>\n</pre> <pre><code>bounds = [[0, 20, 20], [150, 40, 40]]\nprecisions = [1, 0.25, 0.25]\n</code></pre> <pre><code>bounds\n</code></pre> <pre>\n<code>[[0, 20, 20], [150, 40, 40]]</code>\n</pre> <pre><code>from black_it.samplers.halton import HaltonSampler\nfrom black_it.samplers.random_forest import RandomForestSampler\nfrom black_it.samplers.best_batch import BestBatchSampler\n</code></pre> <pre><code>hs = HaltonSampler(batch_size=16)\nrf = RandomForestSampler(batch_size=16)\nbb = BestBatchSampler(batch_size=16)\n\nsamplers = [hs, rf, bb]\n</code></pre> <pre><code># use a quadratic loss\nfrom black_it.loss_functions.minkowski import MinkowskiLoss\n\nloss = MinkowskiLoss()\n</code></pre> <pre><code>from black_it.calibrator import Calibrator\n</code></pre> <pre><code>cal = Calibrator(\n    samplers=samplers,\n    loss_function=loss,\n    model=step_func,\n    parameters_bounds=bounds,\n    parameters_precision=precisions,\n    ensemble_size=1,\n    convergence_precision=None,\n    verbose=True,\n    saving_folder=None,\n    real_data=slack_shock_data,\n    random_state=1,\n)\n</code></pre> <pre>\n<code>\n***\nNumber of free params:       3.\nExplorable param space size: 990711.\n***\n\nSelecting 8 processes for the parallel evaluation of the model\n</code>\n</pre> <pre><code>params, losses = cal.calibrate(10)\n</code></pre> <pre>\n<code>\nBATCH NUMBER:   1\nPARAMS SAMPLED: 0\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.3s\n----&gt;   min loss new params: 48.81\n----&gt;   avg loss new params: 93.88\n----&gt; avg loss exist params: 93.88\n----&gt;         curr min loss: 48.81198552405123\n====&gt;    total elapsed time: 0.3s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 46.24\n----&gt;   avg loss new params: 66.12\n----&gt; avg loss exist params: 80.0\n----&gt;         curr min loss: 46.23645638238496\n====&gt;    total elapsed time: 2.1s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 41.37\n----&gt;   avg loss new params: 59.29\n----&gt; avg loss exist params: 73.1\n----&gt;         curr min loss: 41.374024300280325\n====&gt;    total elapsed time: 0.0s\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 48\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 59.83\n----&gt;   avg loss new params: 100.14\n----&gt; avg loss exist params: 79.86\n----&gt;         curr min loss: 41.374024300280325\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 26.62\n----&gt;   avg loss new params: 63.31\n----&gt; avg loss exist params: 76.55\n----&gt;         curr min loss: 26.615971460763685\n====&gt;    total elapsed time: 2.0s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 20.26\n----&gt;   avg loss new params: 45.86\n----&gt; avg loss exist params: 71.43\n----&gt;         curr min loss: 20.25857835091562\n====&gt;    total elapsed time: 0.0s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 96\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 64.43\n----&gt;   avg loss new params: 97.26\n----&gt; avg loss exist params: 75.12\n----&gt;         curr min loss: 20.25857835091562\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 21.18\n----&gt;   avg loss new params: 43.8\n----&gt; avg loss exist params: 71.21\n----&gt;         curr min loss: 20.25857835091562\n====&gt;    total elapsed time: 2.0s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 19.47\n----&gt;   avg loss new params: 30.07\n----&gt; avg loss exist params: 66.64\n----&gt;         curr min loss: 19.468179801927736\n====&gt;    total elapsed time: 0.0s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 144\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 25.85\n----&gt;   avg loss new params: 89.51\n----&gt; avg loss exist params: 68.92\n----&gt;         curr min loss: 19.468179801927736\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 21.5\n----&gt;   avg loss new params: 30.01\n----&gt; avg loss exist params: 65.39\n----&gt;         curr min loss: 19.468179801927736\n====&gt;    total elapsed time: 2.0s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 23.76\n----&gt;   avg loss new params: 29.4\n----&gt; avg loss exist params: 62.39\n----&gt;         curr min loss: 19.468179801927736\n====&gt;    total elapsed time: 0.0s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 192\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 48.76\n----&gt;   avg loss new params: 95.72\n----&gt; avg loss exist params: 64.95\n----&gt;         curr min loss: 19.468179801927736\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 19.4\n----&gt;   avg loss new params: 30.32\n----&gt; avg loss exist params: 62.48\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 2.0s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 20.73\n----&gt;   avg loss new params: 30.09\n----&gt; avg loss exist params: 60.32\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 0.0s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 240\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 44.43\n----&gt;   avg loss new params: 93.83\n----&gt; avg loss exist params: 62.41\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 20.85\n----&gt;   avg loss new params: 26.51\n----&gt; avg loss exist params: 60.3\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 2.0s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 22.49\n----&gt;   avg loss new params: 27.91\n----&gt; avg loss exist params: 58.5\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 0.0s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 288\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 57.21\n----&gt;   avg loss new params: 96.94\n----&gt; avg loss exist params: 60.52\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 21.6\n----&gt;   avg loss new params: 29.33\n----&gt; avg loss exist params: 58.96\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 2.0s\n\nMETHOD: BestBatchSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 21.7\n----&gt;   avg loss new params: 29.8\n----&gt; avg loss exist params: 57.58\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 0.0s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 336\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 62.8\n----&gt;   avg loss new params: 97.94\n----&gt; avg loss exist params: 59.41\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 24.04\n----&gt;   avg loss new params: 31.56\n----&gt; avg loss exist params: 58.2\n----&gt;         curr min loss: 19.40128801910293\n====&gt;    total elapsed time: 2.1s\n\nMETHOD: BestBatchSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 18.78\n----&gt;   avg loss new params: 27.02\n----&gt; avg loss exist params: 56.9\n----&gt;         curr min loss: 18.777912631604902\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 384\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 49.95\n----&gt;   avg loss new params: 90.04\n----&gt; avg loss exist params: 58.23\n----&gt;         curr min loss: 18.777912631604902\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 20.55\n----&gt;   avg loss new params: 30.55\n----&gt; avg loss exist params: 57.16\n----&gt;         curr min loss: 18.777912631604902\n====&gt;    total elapsed time: 2.1s\n\nMETHOD: BestBatchSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 20.85\n----&gt;   avg loss new params: 29.97\n----&gt; avg loss exist params: 56.15\n----&gt;         curr min loss: 18.777912631604902\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 432\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 33.91\n----&gt;   avg loss new params: 89.94\n----&gt; avg loss exist params: 57.36\n----&gt;         curr min loss: 18.777912631604902\n====&gt;    total elapsed time: 0.0s\n\nMETHOD: RandomForestSampler\n</code>\n</pre> <pre>\n<code>Warning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 23.68\n----&gt;   avg loss new params: 35.37\n----&gt; avg loss exist params: 56.6\n----&gt;         curr min loss: 18.777912631604902\n====&gt;    total elapsed time: 2.1s\n\nMETHOD: BestBatchSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.0s\n----&gt;   min loss new params: 18.78\n----&gt;   avg loss new params: 27.49\n----&gt; avg loss exist params: 55.63\n----&gt;         curr min loss: 18.777912631604902\n====&gt;    total elapsed time: 0.1s\n</code>\n</pre> <pre><code># best parameters obtained so far\nparams[0]\n</code></pre> <pre>\n<code>array([64., 24., 35.])</code>\n</pre> <pre><code># index of mimumum loss\nidxmin = np.argmin(cal.losses_samp)\nparam_min = cal.params_samp[idxmin]\n</code></pre> <pre><code># convergence\nlosses_per_batch = [cal.losses_samp[cal.batch_num_samp == i] for i in range(int(max(cal.batch_num_samp)) + 1)]\nmins_per_batch = np.array([np.min(l) for l in losses_per_batch])\ncummin_per_batch = [np.min(mins_per_batch[: i + 1]) for i in range(mins_per_batch.shape[0])]\n</code></pre> <pre><code>plt.figure()\nplt.plot(mins_per_batch, \"-o\", label=\"batch minimum\")\nplt.plot(cummin_per_batch, \"-o\", label=\"cumulative minimum\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"batch number\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x7f05d0a0eaf0&gt;</code>\n</pre> <pre><code># agreement between real and simulated time series\n\nplt.plot(slack_shock_data[:, 0], \"-\", label=\"real series\")\nplt.ylabel(\n    \"Slack stock price\",\n)\nplt.xlabel(\"days from 27-8-2020\")\n\n\nplt.plot(cal.series_samp[idxmin, 0, :, 0].T, ls=\"--\", label=\"fitted model\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x7f05b2c8dd30&gt;</code>\n</pre>"},{"location":"slack_stock_price_shock/#fitting-a-simple-shock-model-to-the-stock-price-of-slack-technologies","title":"Fitting a simple shock model to the stock price of \"Slack Technologies\"","text":""},{"location":"slack_stock_price_shock/#model","title":"Model","text":""},{"location":"slack_stock_price_shock/#parameter-bounds-and-precisions","title":"parameter bounds and precisions","text":""},{"location":"slack_stock_price_shock/#samplers","title":"samplers","text":""},{"location":"slack_stock_price_shock/#loss","title":"loss","text":""},{"location":"slack_stock_price_shock/#calibrator","title":"calibrator","text":""},{"location":"slack_stock_price_shock/#calibration","title":"Calibration","text":""},{"location":"slack_stock_price_shock/#plots","title":"plots","text":""},{"location":"tests_on_toy_model/","title":"Toy model","text":"<p>In this tutorial we will use black-it to find the mean and variance of a normal distribution by fitting the 'model' to a dataset.  Obviously the parameters of a normal distribution can be obtained more efficiently (such as a by maximum likelihood), yet this example can be useful to understand how black-it works in practice.</p> <pre><code># import standard libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</code></pre> <p>We stard by loading the dataset that we want to fit with our model.</p> <pre><code># load a dataset\n\ntrue_params = [1, 1]  # in general these are not known!\nreal_data = np.atleast_2d(np.loadtxt(f\"data/gaussian_mean{true_params[0]}_var{true_params[1]}.txt\")).T\n</code></pre> <pre><code>plt.plot(real_data[:, 0])\nplt.xlabel(\"time step\")\nplt.ylabel(\"value\")\n</code></pre> <pre>\n<code>Text(0, 0.5, 'value')</code>\n</pre> <p>Then, to set up a calibration one needs to define the following components first:</p> <ol> <li>a model to be calibrated </li> <li>a loss function to measure the distance between the real time series and the simulated time series</li> <li>a set of samplers that iteratively suggest a set of parameter values to explore</li> <li>the parameter space that should be explored</li> </ol> <pre><code># a normal distribution with unknown mean and variance\nfrom models.simple_models import NormalMV\n</code></pre> <pre><code># when called with a set of parameter values, the model provides a simulated time series\nNormalMV([1, 1], N=10, seed=0)\n</code></pre> <pre>\n<code>array([[2.76405235],\n       [1.40015721],\n       [1.97873798],\n       [3.2408932 ],\n       [2.86755799],\n       [0.02272212],\n       [1.95008842],\n       [0.84864279],\n       [0.89678115],\n       [1.4105985 ]])</code>\n</pre> <pre><code># loss function based on the Method Of Moments\nfrom black_it.loss_functions.msm import MethodOfMomentsLoss\n\nloss = MethodOfMomentsLoss()\n</code></pre> <pre><code># import some samplers\nfrom black_it.samplers.best_batch import BestBatchSampler\nfrom black_it.samplers.halton import HaltonSampler\nfrom black_it.samplers.r_sequence import RSequenceSampler\nfrom black_it.samplers.random_forest import RandomForestSampler\nfrom black_it.samplers.random_uniform import RandomUniformSampler\n\n# initialize the samplers with their specific 'batch_size', i.e. the number of points\n# explored every time they are called\nbatch_size = 4\nrandom_sampler = RandomUniformSampler(batch_size=batch_size)\nrseq_sampler = RSequenceSampler(batch_size=batch_size)\nhalton_sampler = HaltonSampler(batch_size=batch_size)\nbest_batch_sampler = BestBatchSampler(batch_size=batch_size)\nrandom_forest_sampler = RandomForestSampler(batch_size=batch_size)\n\nsamplers = [\n    random_sampler,\n    rseq_sampler,\n    halton_sampler,\n    random_forest_sampler,\n    best_batch_sampler,\n]\n</code></pre> <pre><code># the full space of parameters is defined by the lower and upper bounds\n# and by the precision of each parameter\nbounds = [[0.00, 0.01], [2.00, 2.00]]\n\nprecisions = [0.0001, 0.0001]\n</code></pre> <pre><code>from black_it.calibrator import Calibrator\n\n# initialize a Calibrator object\ncal = Calibrator(\n    samplers=samplers,\n    real_data=real_data,\n    model=NormalMV,\n    parameters_bounds=bounds,\n    parameters_precision=precisions,\n    ensemble_size=5,\n    loss_function=loss,\n    saving_folder=None,\n)\n</code></pre> <pre>\n<code>\n***\nNumber of free params:       2.\nExplorable param space size: 398039901.\n***\n\nSelecting 4 processes for the parallel evaluation of the model\n</code>\n</pre> <pre><code>params, losses = cal.calibrate(10)\n</code></pre> <pre>\n<code>\nBATCH NUMBER:   1\nPARAMS SAMPLED: 0\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 1.7s\n----&gt;   min loss new params: 0.17\n----&gt;   avg loss new params: 1.41\n----&gt; avg loss exist params: 1.41\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 1.7s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 1.6\n----&gt;   avg loss new params: 1.97\n----&gt; avg loss exist params: 1.69\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.77\n----&gt;   avg loss new params: 1.46\n----&gt; avg loss exist params: 1.61\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.2s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.64\n----&gt;   avg loss new params: 0.94\n----&gt; avg loss exist params: 1.45\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 1.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.2s\n----&gt;   min loss new params: 0.54\n----&gt;   avg loss new params: 0.82\n----&gt; avg loss exist params: 1.32\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.2s\n\nBATCH NUMBER:   2\nPARAMS SAMPLED: 20\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.65\n----&gt;   avg loss new params: 1.76\n----&gt; avg loss exist params: 1.39\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.68\n----&gt;   avg loss new params: 1.84\n----&gt; avg loss exist params: 1.46\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.2s\n----&gt;   min loss new params: 0.86\n----&gt;   avg loss new params: 1.71\n----&gt; avg loss exist params: 1.49\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.2s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.35\n----&gt;   avg loss new params: 0.57\n----&gt; avg loss exist params: 1.39\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 1.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.42\n----&gt;   avg loss new params: 0.53\n----&gt; avg loss exist params: 1.3\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   3\nPARAMS SAMPLED: 40\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.96\n----&gt;   avg loss new params: 2.14\n----&gt; avg loss exist params: 1.38\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.52\n----&gt;   avg loss new params: 1.24\n----&gt; avg loss exist params: 1.37\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.67\n----&gt;   avg loss new params: 1.89\n----&gt; avg loss exist params: 1.41\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.34\n----&gt;   avg loss new params: 0.57\n----&gt; avg loss exist params: 1.35\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 1.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.29\n----&gt;   avg loss new params: 0.55\n----&gt; avg loss exist params: 1.29\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   4\nPARAMS SAMPLED: 60\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 1.46\n----&gt;   avg loss new params: 2.06\n----&gt; avg loss exist params: 1.34\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.52\n----&gt;   avg loss new params: 1.77\n----&gt; avg loss exist params: 1.37\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.92\n----&gt;   avg loss new params: 1.45\n----&gt; avg loss exist params: 1.37\n----&gt;         curr min loss: 0.16632065926583173\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.1\n----&gt;   avg loss new params: 0.37\n----&gt; avg loss exist params: 1.32\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 1.6s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.23\n----&gt;   avg loss new params: 0.29\n----&gt; avg loss exist params: 1.27\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   5\nPARAMS SAMPLED: 80\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.63\n----&gt;   avg loss new params: 1.1\n----&gt; avg loss exist params: 1.26\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 1.17\n----&gt;   avg loss new params: 1.67\n----&gt; avg loss exist params: 1.28\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.7\n----&gt;   avg loss new params: 1.99\n----&gt; avg loss exist params: 1.31\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.25\n----&gt;   avg loss new params: 0.41\n----&gt; avg loss exist params: 1.27\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 1.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.34\n----&gt;   avg loss new params: 0.54\n----&gt; avg loss exist params: 1.24\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   6\nPARAMS SAMPLED: 100\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.2s\n----&gt;   min loss new params: 1.06\n----&gt;   avg loss new params: 1.68\n----&gt; avg loss exist params: 1.26\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.2s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.53\n----&gt;   avg loss new params: 1.91\n----&gt; avg loss exist params: 1.28\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.38\n----&gt;   avg loss new params: 1.69\n----&gt; avg loss exist params: 1.3\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.3\n----&gt;   avg loss new params: 0.65\n----&gt; avg loss exist params: 1.28\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 1.6s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.2\n----&gt;   avg loss new params: 0.53\n----&gt; avg loss exist params: 1.25\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   7\nPARAMS SAMPLED: 120\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.7\n----&gt;   avg loss new params: 1.3\n----&gt; avg loss exist params: 1.25\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.36\n----&gt;   avg loss new params: 1.58\n----&gt; avg loss exist params: 1.26\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.51\n----&gt;   avg loss new params: 2.12\n----&gt; avg loss exist params: 1.29\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RandomForestSampler\n</code>\n</pre> <pre>\n<code>----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.43\n----&gt;   avg loss new params: 0.54\n----&gt; avg loss exist params: 1.27\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 1.9s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.29\n----&gt;   avg loss new params: 0.48\n----&gt; avg loss exist params: 1.24\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   8\nPARAMS SAMPLED: 140\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 1.54\n----&gt;   avg loss new params: 2.54\n----&gt; avg loss exist params: 1.28\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 1.05\n----&gt;   avg loss new params: 1.86\n----&gt; avg loss exist params: 1.3\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 1.0\n----&gt;   avg loss new params: 1.73\n----&gt; avg loss exist params: 1.31\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RandomForestSampler\nWarning: Repeated samples still found after 5 duplication passes. This is probably due to a small search space.\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.54\n----&gt;   avg loss new params: 0.64\n----&gt; avg loss exist params: 1.29\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 1.7s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.21\n----&gt;   avg loss new params: 0.32\n----&gt; avg loss exist params: 1.27\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   9\nPARAMS SAMPLED: 160\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.81\n----&gt;   avg loss new params: 2.2\n----&gt; avg loss exist params: 1.29\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.88\n----&gt;   avg loss new params: 2.12\n----&gt; avg loss exist params: 1.31\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.63\n----&gt;   avg loss new params: 2.04\n----&gt; avg loss exist params: 1.33\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.31\n----&gt;   avg loss new params: 0.6\n----&gt; avg loss exist params: 1.31\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 1.4s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.2\n----&gt;   avg loss new params: 0.29\n----&gt; avg loss exist params: 1.29\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nBATCH NUMBER:   10\nPARAMS SAMPLED: 180\n\nMETHOD: RandomUniformSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.67\n----&gt;   avg loss new params: 1.12\n----&gt; avg loss exist params: 1.28\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: RSequenceSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.58\n----&gt;   avg loss new params: 1.31\n----&gt; avg loss exist params: 1.28\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n\nMETHOD: HaltonSampler\n----&gt; sim exec elapsed time: 0.2s\n----&gt;   min loss new params: 0.52\n----&gt;   avg loss new params: 1.26\n----&gt; avg loss exist params: 1.28\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.2s\n\nMETHOD: RandomForestSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.35\n----&gt;   avg loss new params: 0.44\n----&gt; avg loss exist params: 1.27\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 1.6s\n\nMETHOD: BestBatchSampler\n----&gt; sim exec elapsed time: 0.1s\n----&gt;   min loss new params: 0.31\n----&gt;   avg loss new params: 0.53\n----&gt; avg loss exist params: 1.25\n----&gt;         curr min loss: 0.09902649522714026\n====&gt;    total elapsed time: 0.1s\n</code>\n</pre> <pre><code># best parameters obtained so far\nparams[0]\n</code></pre> <pre>\n<code>array([1.0002, 0.9791])</code>\n</pre> <pre><code># index of mimumum loss\nidxmin = np.argmin(cal.losses_samp)\nparam_min = cal.params_samp[idxmin]\n</code></pre> <pre><code># scatter plot of losses\nplt.figure(figsize=(6, 5))\n\nplt.scatter(cal.params_samp[:, 0], cal.params_samp[:, 1], c=cal.losses_samp, edgecolor=\"k\")\n\nplt.scatter(param_min[0], param_min[1], marker=\"x\", s=500, color=\"y\")\nplt.scatter(true_params[0], true_params[1], marker=\"x\", s=500, color=\"red\")\n\n\nplt.xlabel(\"mean\")\nplt.ylabel(\"variance\")\nplt.colorbar()\n</code></pre> <pre>\n<code>&lt;matplotlib.colorbar.Colorbar at 0x7fa43a70f890&gt;</code>\n</pre> <pre><code>from black_it.utils.time_series import get_mom_ts\n\nreal_moments = get_mom_ts(cal.real_data)\nsimulated_moments = get_mom_ts(cal.series_samp[idxmin, 0, :, :])\n\n# agreement of moments\nplt.figure()\nplt.plot(real_moments, \"-o\", label=\"real series\")\nplt.plot(simulated_moments, \"-o\", label=\"simulated series\")\nplt.xlabel(\"moment index\")\nplt.xticks(np.arange(18))\nplt.ylabel(\"moments\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x7fa43a589510&gt;</code>\n</pre> <pre><code># convergence\n\nlosses_per_batch = [cal.losses_samp[cal.batch_num_samp == i] for i in range(int(max(cal.batch_num_samp)) + 1)]\nmins_per_batch = np.array([np.min(l) for l in losses_per_batch])\ncummin_per_batch = [np.min(mins_per_batch[: i + 1]) for i in range(mins_per_batch.shape[0])]\n</code></pre> <pre><code>plt.figure()\nplt.plot(mins_per_batch, \"-o\", label=\"batch minimum\")\nplt.plot(cummin_per_batch, \"-o\", label=\"cumulative minimum\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"batch number\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x7fa4334355d0&gt;</code>\n</pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tests_on_toy_model/#finding-the-parameters-of-a-normal-distribution","title":"Finding the parameters of a normal distribution","text":""},{"location":"tests_on_toy_model/#initialize-a-calibrator-object","title":"Initialize a calibrator object","text":""},{"location":"tests_on_toy_model/#1-model-simulator","title":"1 Model simulator","text":""},{"location":"tests_on_toy_model/#2-loss-function","title":"2 Loss function","text":""},{"location":"tests_on_toy_model/#3-samplers","title":"3 Samplers","text":""},{"location":"tests_on_toy_model/#4-parameter-space-bounds-and-precision","title":"4) Parameter space (bounds and precision)","text":""},{"location":"tests_on_toy_model/#finally-initialise-a-calibrator-object","title":"Finally, initialise a Calibrator object","text":""},{"location":"tests_on_toy_model/#calibration","title":"Calibration","text":""},{"location":"tests_on_toy_model/#plots","title":"Plots","text":""}]}